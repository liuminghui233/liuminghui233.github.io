<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
    

    

    



    <meta charset="utf-8">
    
    
    
    
    <title>deeplearning.ai学习笔记（七）优化深度神经网络之超参数调试、批标准化与深度学习框架 | 一只程序喵 | 刘明辉的个人博客</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <meta name="theme-color" content="#3F51B5">
    
    
    <meta name="keywords" content="学习笔记,deeplearning.ai">
    <meta name="description" content="超参数调试不同超参数的重要性排序第一梯队：  学习率α：最重要  第二梯队：  动量衰减参数β：一般设置为0.9 各隐藏层神经元个数#hidden units 小批量大小mini-batch size  第三梯队：  神经网络层数#layers 学习率衰减率decay_rate  第四梯队：  Adam优化算法超参数β1、β2、ε：一般设置为0.9、0.99、10^-8">
<meta name="keywords" content="学习笔记,deeplearning.ai">
<meta property="og:type" content="article">
<meta property="og:title" content="deeplearning.ai学习笔记（七）优化深度神经网络之超参数调试、批标准化与深度学习框架">
<meta property="og:url" content="http://yoursite.com/2019/08/20/deeplearning-ai学习笔记（七）优化深度神经网络之超参数调试、Batch标准化与深度学习框架/index.html">
<meta property="og:site_name" content="一只程序喵">
<meta property="og:description" content="超参数调试不同超参数的重要性排序第一梯队：  学习率α：最重要  第二梯队：  动量衰减参数β：一般设置为0.9 各隐藏层神经元个数#hidden units 小批量大小mini-batch size  第三梯队：  神经网络层数#layers 学习率衰减率decay_rate  第四梯队：  Adam优化算法超参数β1、β2、ε：一般设置为0.9、0.99、10^-8">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://s2.ax1x.com/2019/08/22/m0dXGj.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/08/22/m0dOiQ.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/08/22/m0dHZ8.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/08/22/m0dqIg.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/08/22/m0djRs.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/08/22/m0dbdS.png">
<meta property="og:updated_time" content="2019-08-22T12:18:24.240Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="deeplearning.ai学习笔记（七）优化深度神经网络之超参数调试、批标准化与深度学习框架">
<meta name="twitter:description" content="超参数调试不同超参数的重要性排序第一梯队：  学习率α：最重要  第二梯队：  动量衰减参数β：一般设置为0.9 各隐藏层神经元个数#hidden units 小批量大小mini-batch size  第三梯队：  神经网络层数#layers 学习率衰减率decay_rate  第四梯队：  Adam优化算法超参数β1、β2、ε：一般设置为0.9、0.99、10^-8">
<meta name="twitter:image" content="https://s2.ax1x.com/2019/08/22/m0dXGj.png">
    
        <link rel="alternate" type="application/atom+xml" title="一只程序喵" href="/atom.xml">
    
    <link rel="shortcut icon" href="/favicon.ico">
    <link rel="stylesheet" href="//unpkg.com/hexo-theme-material-indigo@latest/css/style.css">
    <script>window.lazyScripts=[]</script>

    <!-- custom head -->
    

</head>

<body>
    <div id="loading" class="active"></div>

    <aside id="menu" class="hide" >
  <div class="inner flex-row-vertical">
    <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menu-off">
        <i class="icon icon-lg icon-close"></i>
    </a>
    <div class="brand-wrap" style="background-image:url(/img/brand.jpg)">
      <div class="brand">
        <a href="/" class="avatar waves-effect waves-circle waves-light">
          <img src="/img/avatar.jpg">
        </a>
        <hgroup class="introduce">
          <h5 class="nickname">刘明辉</h5>
          <a href="mailto:549208791@qq.com" title="549208791@qq.com" class="mail">549208791@qq.com</a>
        </hgroup>
      </div>
    </div>
    <div class="scroll-wrap flex-col">
      <ul class="nav">
        
            <li class="waves-block waves-effect">
              <a href="/"  >
                <i class="icon icon-lg icon-home"></i>
                主页
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/archives"  >
                <i class="icon icon-lg icon-archives"></i>
                Archives
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/tags"  >
                <i class="icon icon-lg icon-tags"></i>
                Tags
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/categories"  >
                <i class="icon icon-lg icon-th-list"></i>
                Categories
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="https://github.com/liuminghui233" target="_blank" >
                <i class="icon icon-lg icon-github"></i>
                Github
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/about"  >
                <i class="icon icon-lg icon-link"></i>
                About
              </a>
            </li>
        
      </ul>
    </div>
  </div>
</aside>

    <main id="main">
        <header class="top-header" id="header">
    <div class="flex-row">
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light on" id="menu-toggle">
          <i class="icon icon-lg icon-navicon"></i>
        </a>
        <div class="flex-col header-title ellipsis">deeplearning.ai学习笔记（七）优化深度神经网络之超参数调试、批标准化与深度学习框架</div>
        
        <div class="search-wrap" id="search-wrap">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back">
                <i class="icon icon-lg icon-chevron-left"></i>
            </a>
            <input type="text" id="key" class="search-input" autocomplete="off" placeholder="検索">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="search">
                <i class="icon icon-lg icon-search"></i>
            </a>
        </div>
        
        
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menuShare">
            <i class="icon icon-lg icon-share-alt"></i>
        </a>
        
    </div>
</header>
<header class="content-header post-header">

    <div class="container fade-scale">
        <h1 class="title">deeplearning.ai学习笔记（七）优化深度神经网络之超参数调试、批标准化与深度学习框架</h1>
        <h5 class="subtitle">
            
                <time datetime="2019-08-20T07:00:21.000Z" itemprop="datePublished" class="page-time">
  2019-08-20
</time>


            
        </h5>
    </div>

    


</header>


<div class="container body-wrap">
    
    <aside class="post-widget">
        <nav class="post-toc-wrap post-toc-shrink" id="post-toc">
            <h4>TOC</h4>
            <ol class="post-toc"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#超参数调试"><span class="post-toc-number">1.</span> <span class="post-toc-text">超参数调试</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#不同超参数的重要性排序"><span class="post-toc-number">1.1.</span> <span class="post-toc-text">不同超参数的重要性排序</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#超参数调试方法"><span class="post-toc-number">1.2.</span> <span class="post-toc-text">超参数调试方法</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#超参数调试技巧"><span class="post-toc-number">1.3.</span> <span class="post-toc-text">超参数调试技巧</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#批标准化（Batch-Normalization）"><span class="post-toc-number">2.</span> <span class="post-toc-text">批标准化（Batch Normalization）</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#批标准化算法（BN算法）"><span class="post-toc-number">2.1.</span> <span class="post-toc-text">批标准化算法（BN算法）</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#批标准化有效的原因（Why-does-Batch-Norm-work？）"><span class="post-toc-number">2.2.</span> <span class="post-toc-text">批标准化有效的原因（Why does Batch Norm work？）</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#测试时（单个样本）的批标准化"><span class="post-toc-number">2.3.</span> <span class="post-toc-text">测试时（单个样本）的批标准化</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#多分类问题（Multi-class-classification）"><span class="post-toc-number">3.</span> <span class="post-toc-text">多分类问题（Multi-class classification）</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#激活函数"><span class="post-toc-number">3.1.</span> <span class="post-toc-text">激活函数</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#损失函数和代价函数"><span class="post-toc-number">3.2.</span> <span class="post-toc-text">损失函数和代价函数</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#梯度下降"><span class="post-toc-number">3.3.</span> <span class="post-toc-text">梯度下降</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#深度学习框架"><span class="post-toc-number">4.</span> <span class="post-toc-text">深度学习框架</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#深度学习框架的选取原则"><span class="post-toc-number">4.1.</span> <span class="post-toc-text">深度学习框架的选取原则</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#TensorFlow基础"><span class="post-toc-number">4.2.</span> <span class="post-toc-text">TensorFlow基础</span></a></li></ol></li></ol>
        </nav>
    </aside>


<article id="post-deeplearning-ai学习笔记（七）优化深度神经网络之超参数调试、Batch标准化与深度学习框架"
  class="post-article article-type-post fade" itemprop="blogPost">

    <div class="post-card">
        <h1 class="post-card-title">deeplearning.ai学习笔记（七）优化深度神经网络之超参数调试、批标准化与深度学习框架</h1>
        <div class="post-meta">
            <time class="post-time" title="2019-08-20 15:00:21" datetime="2019-08-20T07:00:21.000Z"  itemprop="datePublished">2019-08-20</time>

            


            
<span id="busuanzi_container_page_pv" title="文章总阅读量" style='display:none'>
    <i class="icon icon-eye icon-pr"></i><span id="busuanzi_value_page_pv"></span>
</span>


        </div>
        <div class="post-content" id="post-content" itemprop="postContent">
            <h2 id="超参数调试"><a href="#超参数调试" class="headerlink" title="超参数调试"></a>超参数调试</h2><h3 id="不同超参数的重要性排序"><a href="#不同超参数的重要性排序" class="headerlink" title="不同超参数的重要性排序"></a>不同超参数的重要性排序</h3><p><strong>第一梯队：</strong></p>
<ul>
<li>学习率α：最重要</li>
</ul>
<p><strong>第二梯队：</strong></p>
<ul>
<li>动量衰减参数β：一般设置为0.9</li>
<li>各隐藏层神经元个数#hidden units</li>
<li>小批量大小mini-batch size</li>
</ul>
<p><strong>第三梯队：</strong></p>
<ul>
<li>神经网络层数#layers</li>
<li>学习率衰减率decay_rate</li>
</ul>
<p><strong>第四梯队：</strong></p>
<ul>
<li><p>Adam优化算法超参数β1、β2、ε：一般设置为0.9、0.99、10^-8</p>
<a id="more"></a>
</li>
</ul>
<h3 id="超参数调试方法"><a href="#超参数调试方法" class="headerlink" title="超参数调试方法"></a>超参数调试方法</h3><p><strong>均匀取点 vs 随机取点：</strong></p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://s2.ax1x.com/2019/08/22/m0dXGj.png" alt="均匀取点 vs 随机取点" title>
                </div>
                <div class="image-caption">均匀取点 vs 随机取点</div>
            </figure>
<p>由于事先很难知道超参数的重要程度，因此<strong>选择随机取点</strong>来选择更多的值进行更多实验。为了得到更精确的最优参数，我们应该继续<strong>对模型表现较好的区域进行由粗到细的采样</strong>。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://s2.ax1x.com/2019/08/22/m0dOiQ.png" alt="由粗到细采样" title>
                </div>
                <div class="image-caption">由粗到细采样</div>
            </figure>
<h3 id="超参数调试技巧"><a href="#超参数调试技巧" class="headerlink" title="超参数调试技巧"></a>超参数调试技巧</h3><ol>
<li><p>选择适当的尺度</p>
<ul>
<li><p>均匀随机采样：对于超参数#layers和#hidden units等，取值为正整数，超参数每次变化的尺度都是一致的。</p>
</li>
<li><p>非均匀随机采样：例如学习率<em>α</em>，待调范围是[0.0001, 1]。在实际应用中，最佳的<em>α</em>值可能主要分布在[0.0001, 0.1]之间，而[0.1, 1]范围内<em>α</em>值效果并不好。如果使用均匀随机采样，那么有90%的采样点分布在[0.1, 1]之间，只有10%分布在[0.0001, 0.1]之间。通常的做法是<strong>将线性尺度转换为对数尺度</strong>，然后再<strong>在对数尺度下进行均匀采样</strong>。</p>
</li>
</ul>
<p><img src="https://s2.ax1x.com/2019/08/22/m0dHZ8.png" alt="线性尺度转换为对数尺度"></p>
<p>具体做法：如果线性区间为[a, b]，令m=log(a)，n=log(b)，则对应的log区间为[m,n]。对log区间的[m,n]进行随机均匀采样，然后得到的采样值r，最后反推到线性区间10^r。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">m = np.log10(a)</span><br><span class="line">n = np.log10(b)</span><br><span class="line">r = np.random.rand()</span><br><span class="line">r = m + (n-m)*r</span><br><span class="line">r = np.power(<span class="number">10</span>,r)</span><br></pre></td></tr></table></figure>
</li>
<li><p>深度学习不同的应用领域出现相互交融的现象，某个应用领域的超参数设定有可能通用于另一领域，应该更多地阅读其他研究领域的 paper，跨领域地寻找灵感。</p>
</li>
<li><p>考虑到数据的变化或者服务器的变更等因素，建议每隔几个月重新测试或评估超参数，来获得实时的最佳模型。</p>
</li>
<li><p>根据所拥有的计算资源决定训练模型的方式：</p>
<ul>
<li><p>Panda（熊猫方式）：受计算能力所限，只能对一个模型进行训练，调试不同的超参数，使得这个模型有最佳的表现；</p>
</li>
<li><p>Caviar（鱼子酱方式）：对多个模型同时进行训练，每个模型上调试不同的超参数，根据表现情况，选择最佳的模型。</p>
<p><img src="https://s2.ax1x.com/2019/08/22/m0dqIg.png" alt="Pandas vs. Caviar"></p>
</li>
</ul>
</li>
</ol>
<h2 id="批标准化（Batch-Normalization）"><a href="#批标准化（Batch-Normalization）" class="headerlink" title="批标准化（Batch Normalization）"></a>批标准化（Batch Normalization）</h2><p>批标准化 (Batch Normalization) 是对于神经网络中间隐藏层的输出进行标准化。批标准化可以使神经网络更加鲁棒，对于超参数的选择不再那么敏感，从而可以更容易地训练非常深的网络。</p>
<h3 id="批标准化算法（BN算法）"><a href="#批标准化算法（BN算法）" class="headerlink" title="批标准化算法（BN算法）"></a>批标准化算法（BN算法）</h3><p>第<em>l</em>层隐藏层的输入就是第<em>l</em>−1层隐藏层的输出<em>A</em>[<em>l</em>−1]。对<em>A</em>[<em>l</em>−1]进行标准化处理，从原理上来说可以提高<em>W</em>[<em>l</em>]和<em>b</em>[<em>l</em>]的训练速度和准确度实际应用中，<strong>一般是对<em>Z</em>[<em>l</em>−1]进行标准化处理</strong>而不是<em>A</em>[<em>l</em>−1]。</p>
<script type="math/tex; mode=display">
\mu=\frac{1}{m}\sum_{i}z^{(i)}\\
\sigma^2=\frac{1}{m}\sum_i(z_i-\mu)^2\\
z_{norm}^{(i)}=\frac{z_i-\mu}{\sqrt{\sigma^2+\epsilon}}</script><p>其中，m 是单个 mini-batch 所包含的样本个数，ϵ 是为了防止分母为零，通常取10^−8。</p>
<p>这样，我们使得所有的输入z(i)均值为 0，方差为 1。但我们不想让隐藏层单元总是含有均值 0 和方差 1，也许<strong>隐藏层单元有了不同的分布会更有意义</strong>。而且，各隐藏层的输入均值在靠近 0 的区域，即处于激活函数的线性区域，<strong>不利于训练非线性神经网络</strong>。因此，设置 γ 和 β ，可以让 z~(i)的均值和方差可以为任意值。</p>
<script type="math/tex; mode=display">
\widetilde{z}^{(i)}=\gamma z_{norm}^{(i)}+\beta</script><p>其中，γ 和 β都是超参数，神经网络通过学习得到。</p>
<p><strong>应用于整个神经网络：</strong></p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://s2.ax1x.com/2019/08/22/m0djRs.png" alt="BN应用于神经网络" title>
                </div>
                <div class="image-caption">BN应用于神经网络</div>
            </figure>
<p>因为标准化处理中包含减去均值的一步，因此 b 实际上没有起到作用，其数值效果交由 β 来实现。因此可以省略 b 或者暂时设置为 0。</p>
<h3 id="批标准化有效的原因（Why-does-Batch-Norm-work？）"><a href="#批标准化有效的原因（Why-does-Batch-Norm-work？）" class="headerlink" title="批标准化有效的原因（Why does Batch Norm work？）"></a>批标准化有效的原因（Why does Batch Norm work？）</h3><ol>
<li><p><strong>通过对隐藏层各神经元的输入做类似的标准化处理，提高神经网络训练速度；</strong></p>
</li>
<li><p><strong>可以使前面层的权重变化对后面层造成的影响减小，整体网络更加健壮；</strong></p>
<ul>
<li><p>如果实际应用样本和训练样本的数据分布不同（如橘猫图片和黑猫图片），称发生了“<strong>Covariate Shift</strong>”。这种情况下，一般要对模型进行重新训练。<strong>BN减小了 Covariate Shift 所带来的影响</strong>，让模型变得更加健壮。</p>
</li>
<li><p>即使输入的值改变了，由于 BN的作用，使得均值和方差保持不变，限制了在前层的参数更新对数值分布的影响程度，因此后层的学习变得更容易一些。BN减少了各层 W 和 b 之间的耦合性，让各层更加独立，实现自我训练学习的效果。</p>
</li>
</ul>
</li>
<li><p><strong>起到微弱的正则化效果。</strong></p>
<ul>
<li><p>在每个 mini-batch 而非整个数据集上计算均值和方差，只由这一小部分数据估计得出的均值和方差会有一些噪声，因此最终计算出的z~(i)也有一定噪声。类似于 dropout，这种噪声会使得神经元不会再特别依赖于任何一个输入特征。</p>
</li>
<li><p>BN只有微弱的正则化效果，可以和 dropout 一起使用以获得更强大的正则化效果。但<strong>不要将BN作为正则化的手段</strong>，而是当作加速学习的方式。</p>
</li>
</ul>
</li>
</ol>
<h3 id="测试时（单个样本）的批标准化"><a href="#测试时（单个样本）的批标准化" class="headerlink" title="测试时（单个样本）的批标准化"></a>测试时（单个样本）的批标准化</h3><p>对于第 l 层隐藏层，考虑所有 mini-batch 在该隐藏层下的μ[l]和σ2[l]，然后用<strong>指数加权平均</strong>的方式来预测得到当前单个样本的μ[l]和σ2[l]。</p>
<h2 id="多分类问题（Multi-class-classification）"><a href="#多分类问题（Multi-class-classification）" class="headerlink" title="多分类问题（Multi-class classification）"></a>多分类问题（Multi-class classification）</h2><p>对于多分类问题，用C表示种类个数，神经网络中输出层就有C个神经元，即<em>n</em>[<em>L</em>]=<em>C</em>。其中，每个神经元的输出依次对应属于该类的概率，即<em>P</em>(<em>y</em>=<em>c</em>∣<em>x</em>)。为了处理多分类问题，一般使用<strong>Softmax回归模型</strong>。</p>
<h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><script type="math/tex; mode=display">
\begin{align}
& 对于输出层:\\
& z^{[L]}=W^{[L]}a^{[L-1]}+b^{[L]}\\
&a_i^{[L]}=\frac{e^{z_i^{[L]}}}{\sum_{i=1}^{C}e^{z_i^{[L]}}}~~~~(有\sum_{i=1}^{C}a_i^{[L]}=1)\\
&\hat{y}=a^{[L]}~~~其维度为(C,1)
\end{align}</script><p>例：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://s2.ax1x.com/2019/08/22/m0dbdS.png" alt="计算实例" title>
                </div>
                <div class="image-caption">计算实例</div>
            </figure>
<h3 id="损失函数和代价函数"><a href="#损失函数和代价函数" class="headerlink" title="损失函数和代价函数"></a>损失函数和代价函数</h3><p>定义损失函数为：</p>
<script type="math/tex; mode=display">
\begin{align}
&L(\hat{y},y)=-\sum_{j=1}^{C}y_jlog\hat{y}_j\\
&当i为样本真实类别时，对于j\neq i,y_j=0\\
&L(\hat{y},y)=-y_ilog\hat{y}_i=-log\hat{y}_i\\
\end{align}</script><p>m个训练样本的代价函数为：</p>
<script type="math/tex; mode=display">
J=\frac{1}{m}\sum_{i=1}^{m}L(\hat{y},y)</script><h3 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h3><script type="math/tex; mode=display">
dZ^{[L]}=A^{[L]}-Y</script><p>反向传播过程的其他步骤与逻辑回归一致。</p>
<h2 id="深度学习框架"><a href="#深度学习框架" class="headerlink" title="深度学习框架"></a>深度学习框架</h2><h3 id="深度学习框架的选取原则"><a href="#深度学习框架的选取原则" class="headerlink" title="深度学习框架的选取原则"></a>深度学习框架的选取原则</h3><ul>
<li>易于编程：包括开发和迭代、部署产品；</li>
<li>运行速度：特别是训练大型数据集时；</li>
<li>是否真正开源：不仅需要开源，而且需要良好的管理，能够持续开放所有功能。</li>
</ul>
<h3 id="TensorFlow基础"><a href="#TensorFlow基础" class="headerlink" title="TensorFlow基础"></a>TensorFlow基础</h3><p>一个简单的程序框架：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">cofficients = np.array([[<span class="number">1.</span>],[<span class="number">-10.</span>],[<span class="number">25.</span>]])</span><br><span class="line"></span><br><span class="line">w = tf.Variable(<span class="number">0</span>,dtype=tf.float32)</span><br><span class="line">x = tf.placeholder(tf.float32,[<span class="number">3</span>,<span class="number">1</span>])</span><br><span class="line"><span class="comment"># Tensorflow 重载了加减乘除符号</span></span><br><span class="line">cost = x[<span class="number">0</span>][<span class="number">0</span>]*w**<span class="number">2</span> + x[<span class="number">1</span>][<span class="number">0</span>]*w + x[<span class="number">2</span>][<span class="number">0</span>]</span><br><span class="line"><span class="comment"># 改变下面这行代码，可以换用更好的优化算法</span></span><br><span class="line">train = tf.train.GradientDescentOptimizer(<span class="number">0.01</span>).minimize(cost)</span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">session = tf.Session()</span><br><span class="line">session.run(init)</span><br><span class="line"><span class="comment">#上面一段代码可以替换为以下：</span></span><br><span class="line"><span class="comment">#with tf.Session() as session:</span></span><br><span class="line"><span class="comment">#    session.run(init)</span></span><br><span class="line"><span class="comment">#    print(session.run(w))</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    session.run(train, feed_dict=(x:coefficients))</span><br><span class="line">print(session.run(w))</span><br></pre></td></tr></table></figure>
<p>运行结果为4.99999，更改 cofficients 的值可以得到不同的结果 w。</p>

        </div>

        <blockquote class="post-copyright">
    
    <div class="content">
        
<span class="post-time">
    最終更新：<time datetime="2019-08-22T12:18:24.240Z" itemprop="dateUpdated">2019-08-22 20:18:24</time>
</span><br>


        
        版权声明:本文为博主原创文章,未经博主允许不得转载。<a href="/2019/08/20/deeplearning-ai学习笔记（七）优化深度神经网络之超参数调试、Batch标准化与深度学习框架/" target="_blank" rel="external">http://yoursite.com/2019/08/20/deeplearning-ai学习笔记（七）优化深度神经网络之超参数调试、Batch标准化与深度学习框架/</a>
        
    </div>
    
    <footer>
        <a href="http://yoursite.com">
            <img src="/img/avatar.jpg" alt="刘明辉">
            刘明辉
        </a>
    </footer>
</blockquote>

        
<div class="page-reward">
    <a id="rewardBtn" href="javascript:;" class="page-reward-btn waves-effect waves-circle waves-light">赏</a>
</div>



        <div class="post-footer">
            
	<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/deeplearning-ai/">deeplearning.ai</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/学习笔记/">学习笔记</a></li></ul>


            
<div class="page-share-wrap">
    

<div class="page-share" id="pageShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://yoursite.com/2019/08/20/deeplearning-ai学习笔记（七）优化深度神经网络之超参数调试、Batch标准化与深度学习框架/&title=《deeplearning.ai学习笔记（七）优化深度神经网络之超参数调试、批标准化与深度学习框架》 — 一只程序喵&pic=http://yoursite.com/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://yoursite.com/2019/08/20/deeplearning-ai学习笔记（七）优化深度神经网络之超参数调试、Batch标准化与深度学习框架/&title=《deeplearning.ai学习笔记（七）优化深度神经网络之超参数调试、批标准化与深度学习框架》 — 一只程序喵&source=超参数调试不同超参数的重要性排序第一梯队：

学习率α：最重要

第二梯队：

动量衰减参数β：一般设置为0.9
各隐藏层神经元个数#hidden uni..." data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://yoursite.com/2019/08/20/deeplearning-ai学习笔记（七）优化深度神经网络之超参数调试、Batch标准化与深度学习框架/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《deeplearning.ai学习笔记（七）优化深度神经网络之超参数调试、批标准化与深度学习框架》 — 一只程序喵&url=http://yoursite.com/2019/08/20/deeplearning-ai学习笔记（七）优化深度神经网络之超参数调试、Batch标准化与深度学习框架/&via=http://yoursite.com" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://yoursite.com/2019/08/20/deeplearning-ai学习笔记（七）优化深度神经网络之超参数调试、Batch标准化与深度学习框架/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>



    <a href="javascript:;" id="shareFab" class="page-share-fab waves-effect waves-circle">
        <i class="icon icon-share-alt icon-lg"></i>
    </a>
</div>



        </div>
    </div>

    
<nav class="post-nav flex-row flex-justify-between">
  
    <div class="waves-block waves-effect prev">
      <a href="/2019/08/22/deeplearning-ai学习笔记（八）构建机器学习项目之机器学习策略（上）/" id="post-prev" class="post-nav-link">
        <div class="tips"><i class="icon icon-angle-left icon-lg icon-pr"></i> Prev</div>
        <h4 class="title">deeplearning.ai学习笔记（八）构建机器学习项目之机器学习策略（上）</h4>
      </a>
    </div>
  

  
    <div class="waves-block waves-effect next">
      <a href="/2019/08/18/deeplearning-ai学习笔记（六）优化深度神经网络之优化算法（Optimization-algorithms）/" id="post-next" class="post-nav-link">
        <div class="tips">Next <i class="icon icon-angle-right icon-lg icon-pl"></i></div>
        <h4 class="title">deeplearning.ai学习笔记（六）优化深度神经网络之优化算法（Optimization algorithms）</h4>
      </a>
    </div>
  
</nav>



    




















</article>

<div id="reward" class="page-modal reward-lay">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <h3 class="reward-title">
        <i class="icon icon-quote-left"></i>
        谢谢打赏ο(=•ω＜=)ρ⌒☆
        <i class="icon icon-quote-right"></i>
    </h3>
    <div class="reward-content">
        
        <div class="reward-code">
            <img id="rewardCode" src="/img/wechat.jpg" alt="打赏二维码">
        </div>
        
        <label class="reward-toggle">
            <input id="rewardToggle" type="checkbox" class="reward-toggle-check"
                data-wechat="/img/wechat.jpg" data-alipay="/img/alipay.jpg">
            <div class="reward-toggle-ctrol">
                <span class="reward-toggle-item wechat">微信</span>
                <span class="reward-toggle-label"></span>
                <span class="reward-toggle-item alipay">支付宝</span>
            </div>
        </label>
        
    </div>
</div>



</div>

        <footer class="footer">
    <div class="top">
        
<p>
    <span id="busuanzi_container_site_uv" style='display:none'>
        站点总访客数：<span id="busuanzi_value_site_uv"></span>
    </span>
    <span id="busuanzi_container_site_pv" style='display:none'>
        站点总访问量：<span id="busuanzi_value_site_pv"></span>
    </span>
</p>


        <p>
            
                <span><a href="/atom.xml" target="_blank" class="rss" title="rss"><i class="icon icon-lg icon-rss"></i></a></span>
            
            <span>このブログの内容物は<a rel="license" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja">クリエイティブ・コモンズ 表示 - 非営利 - 継承 4.0 国際ライセンスの下に提供されています</a></span>
        </p>
    </div>
    <div class="bottom">
        <p><span>刘明辉 &copy; 2015 - 2019</span>
            <span>
                
                Power by <a href="http://hexo.io/" target="_blank">Hexo</a> Theme <a href="https://github.com/yscoder/hexo-theme-indigo" target="_blank">indigo</a>
            </span>
        </p>
    </div>
</footer>

    </main>
    <div class="mask" id="mask"></div>
<a href="javascript:;" id="gotop" class="waves-effect waves-circle waves-light"><span class="icon icon-lg icon-chevron-up"></span></a>



<div class="global-share" id="globalShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://yoursite.com/2019/08/20/deeplearning-ai学习笔记（七）优化深度神经网络之超参数调试、Batch标准化与深度学习框架/&title=《deeplearning.ai学习笔记（七）优化深度神经网络之超参数调试、批标准化与深度学习框架》 — 一只程序喵&pic=http://yoursite.com/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://yoursite.com/2019/08/20/deeplearning-ai学习笔记（七）优化深度神经网络之超参数调试、Batch标准化与深度学习框架/&title=《deeplearning.ai学习笔记（七）优化深度神经网络之超参数调试、批标准化与深度学习框架》 — 一只程序喵&source=超参数调试不同超参数的重要性排序第一梯队：

学习率α：最重要

第二梯队：

动量衰减参数β：一般设置为0.9
各隐藏层神经元个数#hidden uni..." data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://yoursite.com/2019/08/20/deeplearning-ai学习笔记（七）优化深度神经网络之超参数调试、Batch标准化与深度学习框架/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《deeplearning.ai学习笔记（七）优化深度神经网络之超参数调试、批标准化与深度学习框架》 — 一只程序喵&url=http://yoursite.com/2019/08/20/deeplearning-ai学习笔记（七）优化深度神经网络之超参数调试、Batch标准化与深度学习框架/&via=http://yoursite.com" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://yoursite.com/2019/08/20/deeplearning-ai学习笔记（七）优化深度神经网络之超参数调试、Batch标准化与深度学习框架/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>


<div class="page-modal wx-share" id="wxShare">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <p>扫一扫，分享到微信</p>
    <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAASYAAAEmCAAAAADqr2IGAAAEk0lEQVR42u3a227kNhAFwP3/n94AQR4SZD1zTjcNDOXS08AeS2TJAPv261d8/f77+v/n19/890+S6/X9k29u7n/gwoQJEyZMH8nU3jpZ6Fcbe73h/ca+emKywq+ejgkTJkyYnsGU3y45/vPQIYd+TXB2hZgwYcKE6Wcy5Y/PU9bXBJuwIHnZmDBhwoQJU55SJt9sC6yzje2TakyYMGHC9Dym5HZtqTe5T/L9vLeYp9DfWAvHhAkTJkwfxrRPJu/9fPjChAkTJkwfxvS7vPIm5SwlbtPmPBWf7fefv8WECRMmTNcy7RuTbTo62+Ss5ZmP20b/BJgwYcKE6UKms2XQdhynfcqplmpemP5P3IQJEyZMmC5n2oy/bI7nWVDSprWbEAQTJkyYMN3LNDwg47LsLKFNUtMcMQkLiqQdEyZMmDBdxbRpEJ4Fag/yTWKclHcxYcKECdMzmPLbtYdusslZ+jorSc8CIEyYMGHC9DymPB2dHbT5stpAIX95+eDOmzoBJkyYMGG6likvyM6WO2tktkM/+b6GAQEmTJgwYfp4prbnmZeA26Q3H9nZJNV50LNqDWPChAkTpo9hygdfNgtqE90Za/KCZwNAmDBhwoTpSUx5OtqmtZsjOWle5i/72MgOJkyYMGH6eKa2nJqP7+SL+L4i77F2JiZMmDBhupap3VJ73OZH9ezwngUoszQeEyZMmDDdyJQcnLOxmLzd2MLNwpFZKIAJEyZMmJ7HVFSCy3JtPh60ScKHjckkYsKECRMmTJczJRtre6Sz8ms+7tOWntt7/iEgwIQJEyZMVzG1ozbtAT+rOm9eyb69GoURmDBhwoTpQqa2wHq2BJwc0huyY4EOJkyYMGG6imlT3k3CgrwBeWrD7atK/lEwYcKECdPtTO2VpLV5MpmEGu1T8oGh6J8DEyZMmDBdy9QO37RJ8j713QwGbcKXYnwHEyZMmDB9MFO70DwUaFPitrDbhhGb4i8mTJgwYbqXqf7S6AE5ff6qzqbWRbEbEyZMmDBdyNQOweQH8IZ4MzA0e1VvVosJEyZMmB7H1D4sX0qb+s4GfWa//ZIbEyZMmDA9jikpjJ4NHfZF51kLFhMmTJgw/QSmtio8S03bcZ9NoNCOAb15qZgwYcKE6VqmttWXJLRJ6rspuSZPb4d1olQZEyZMmDBdxdSWZZNmZIu7Hx6alZXzz5gwYcKE6V6mttgaFUOPRiubICN5nbNyNiZMmDBhuoVpltYmDcXvGM1pi7nHRnwwYcKECdMjmPKlz1qYM7giilkM6Lx5LiZMmDBhupZpBpQvOtlGe2znQzabEOHNOjFhwoQJ0yVMs+N8dojOWp7tnfO0tphswoQJEyZM1zK1B3lect0XkTfF2dnQz4ELEyZMmDB9JFNelt1wnG1qtgFEOzL7h99iwoQJE6bLmdra8J51VnjdBx9tKxQTJkyYMP0cprPtz9lwz4z7QNqMCRMmTJh+GFO7yXZ0NQ8X2lfYtksxYcKECdMzmDYbSAZ99n87S5vzIOPNSjBhwoQJ07VMbdL4HQ3LPM1O1vMdwQcmTJgwYbqQ6S9qSPHzmq6QZAAAAABJRU5ErkJggg==" alt="微信分享二维码">
</div>




    <script src="//cdn.bootcss.com/node-waves/0.7.4/waves.min.js"></script>
<script>
var BLOG = { ROOT: '/', SHARE: true, REWARD: true };


</script>

<script src="//unpkg.com/hexo-theme-material-indigo@latest/js/main.min.js"></script>


<div class="search-panel" id="search-panel">
    <ul class="search-result" id="search-result"></ul>
</div>
<template id="search-tpl">
<li class="item">
    <a href="{path}" class="waves-block waves-effect">
        <div class="title ellipsis" title="{title}">{title}</div>
        <div class="flex-row flex-middle">
            <div class="tags ellipsis">
                {tags}
            </div>
            <time class="flex-col time">{date}</time>
        </div>
    </a>
</li>
</template>

<script src="//unpkg.com/hexo-theme-material-indigo@latest/js/search.min.js" async></script>






<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>



<script>
(function() {
    var OriginTitile = document.title, titleTime;
    document.addEventListener('visibilitychange', function() {
        if (document.hidden) {
            document.title = '一只程序喵 | 刘明辉的个人博客';
            clearTimeout(titleTime);
        } else {
            document.title = '一只程序喵 | 刘明辉的个人博客';
            titleTime = setTimeout(function() {
                document.title = OriginTitile;
            },2000);
        }
    });
})();
</script>



<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

</body>
</html>
