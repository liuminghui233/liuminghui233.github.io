<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
    

    

    



    <meta charset="utf-8">
    
    
    
    
    <title>deeplearning.ai学习笔记（十四）循环神经网络（Recurrent Neural Network） | 一只程序喵 | 刘明辉的个人博客</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <meta name="theme-color" content="#3F51B5">
    
    
    <meta name="keywords" content="学习笔记,deeplearning.ai">
    <meta name="description" content="序列模型（Sequence Model）概念前后相互关联的数据，如自然语言、音频等。 主要应用：                                                                                              序列模型的主要应用              符号表示对于一个序列数据x，用x^{⟨t⟩}来表示这个数据中的第t个元">
<meta name="keywords" content="学习笔记,deeplearning.ai">
<meta property="og:type" content="article">
<meta property="og:title" content="deeplearning.ai学习笔记（十四）循环神经网络（Recurrent Neural Network）">
<meta property="og:url" content="http://yoursite.com/2019/08/31/deeplearning-ai学习笔记（十四）循环神经网络/index.html">
<meta property="og:site_name" content="一只程序喵">
<meta property="og:description" content="序列模型（Sequence Model）概念前后相互关联的数据，如自然语言、音频等。 主要应用：                                                                                              序列模型的主要应用              符号表示对于一个序列数据x，用x^{⟨t⟩}来表示这个数据中的第t个元">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://s2.ax1x.com/2019/08/31/mzHMHx.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/08/31/mzHKD1.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/08/31/mzHA4U.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/08/31/mzHmv9.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/08/31/mzHZ34.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/08/31/mzHegJ.jpg">
<meta property="og:image" content="https://s2.ax1x.com/2019/08/31/mzHkNT.jpg">
<meta property="og:image" content="https://s2.ax1x.com/2019/08/31/mzHPH0.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/08/31/mzHSjs.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/08/31/mzHFEV.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/08/31/mzHCBq.jpg">
<meta property="og:image" content="https://s2.ax1x.com/2019/08/31/mz7zcj.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/08/31/mzH9un.png">
<meta property="og:updated_time" content="2019-09-02T18:17:51.473Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="deeplearning.ai学习笔记（十四）循环神经网络（Recurrent Neural Network）">
<meta name="twitter:description" content="序列模型（Sequence Model）概念前后相互关联的数据，如自然语言、音频等。 主要应用：                                                                                              序列模型的主要应用              符号表示对于一个序列数据x，用x^{⟨t⟩}来表示这个数据中的第t个元">
<meta name="twitter:image" content="https://s2.ax1x.com/2019/08/31/mzHMHx.png">
    
        <link rel="alternate" type="application/atom+xml" title="一只程序喵" href="/atom.xml">
    
    <link rel="shortcut icon" href="/favicon.ico">
    <link rel="stylesheet" href="//unpkg.com/hexo-theme-material-indigo@latest/css/style.css">
    <script>window.lazyScripts=[]</script>

    <!-- custom head -->
    

</head>

<body>
    <div id="loading" class="active"></div>

    <aside id="menu" class="hide" >
  <div class="inner flex-row-vertical">
    <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menu-off">
        <i class="icon icon-lg icon-close"></i>
    </a>
    <div class="brand-wrap" style="background-image:url(/img/brand.jpg)">
      <div class="brand">
        <a href="/" class="avatar waves-effect waves-circle waves-light">
          <img src="/img/avatar.jpg">
        </a>
        <hgroup class="introduce">
          <h5 class="nickname">刘明辉</h5>
          <a href="mailto:549208791@qq.com" title="549208791@qq.com" class="mail">549208791@qq.com</a>
        </hgroup>
      </div>
    </div>
    <div class="scroll-wrap flex-col">
      <ul class="nav">
        
            <li class="waves-block waves-effect">
              <a href="/"  >
                <i class="icon icon-lg icon-home"></i>
                主页
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/archives"  >
                <i class="icon icon-lg icon-archives"></i>
                Archives
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/tags"  >
                <i class="icon icon-lg icon-tags"></i>
                Tags
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/categories"  >
                <i class="icon icon-lg icon-th-list"></i>
                Categories
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="https://github.com/liuminghui233" target="_blank" >
                <i class="icon icon-lg icon-github"></i>
                Github
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/about"  >
                <i class="icon icon-lg icon-link"></i>
                About
              </a>
            </li>
        
      </ul>
    </div>
  </div>
</aside>

    <main id="main">
        <header class="top-header" id="header">
    <div class="flex-row">
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light on" id="menu-toggle">
          <i class="icon icon-lg icon-navicon"></i>
        </a>
        <div class="flex-col header-title ellipsis">deeplearning.ai学习笔记（十四）循环神经网络（Recurrent Neural Network）</div>
        
        <div class="search-wrap" id="search-wrap">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back">
                <i class="icon icon-lg icon-chevron-left"></i>
            </a>
            <input type="text" id="key" class="search-input" autocomplete="off" placeholder="Search">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="search">
                <i class="icon icon-lg icon-search"></i>
            </a>
        </div>
        
        
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menuShare">
            <i class="icon icon-lg icon-share-alt"></i>
        </a>
        
    </div>
</header>
<header class="content-header post-header">

    <div class="container fade-scale">
        <h1 class="title">deeplearning.ai学习笔记（十四）循环神经网络（Recurrent Neural Network）</h1>
        <h5 class="subtitle">
            
                <time datetime="2019-08-31T05:18:21.000Z" itemprop="datePublished" class="page-time">
  2019-08-31
</time>


            
        </h5>
    </div>

    


</header>


<div class="container body-wrap">
    
    <aside class="post-widget">
        <nav class="post-toc-wrap post-toc-shrink" id="post-toc">
            <h4>TOC</h4>
            <ol class="post-toc"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#序列模型（Sequence-Model）"><span class="post-toc-number">1.</span> <span class="post-toc-text">序列模型（Sequence Model）</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#概念"><span class="post-toc-number">1.1.</span> <span class="post-toc-text">概念</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#符号表示"><span class="post-toc-number">1.2.</span> <span class="post-toc-text">符号表示</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#词向量：one-hot向量"><span class="post-toc-number">1.3.</span> <span class="post-toc-text">词向量：one-hot向量</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#标准神经网络在序列模型上存在的问题"><span class="post-toc-number">1.4.</span> <span class="post-toc-text">标准神经网络在序列模型上存在的问题</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#基本循环神经网络"><span class="post-toc-number">2.</span> <span class="post-toc-text">基本循环神经网络</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#基本结构"><span class="post-toc-number">2.1.</span> <span class="post-toc-text">基本结构</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#基本RNN单元"><span class="post-toc-number">2.2.</span> <span class="post-toc-text">基本RNN单元</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#前向传播"><span class="post-toc-number">2.2.1.</span> <span class="post-toc-text">前向传播</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#代价函数"><span class="post-toc-number">2.2.2.</span> <span class="post-toc-text">代价函数</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#反向传播"><span class="post-toc-number">2.2.3.</span> <span class="post-toc-text">反向传播</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#循环神经网络的架构"><span class="post-toc-number">2.3.</span> <span class="post-toc-text">循环神经网络的架构</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#实例：语言模型（Language-Model）"><span class="post-toc-number">2.4.</span> <span class="post-toc-text">实例：语言模型（Language Model）</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#概念-1"><span class="post-toc-number">2.4.1.</span> <span class="post-toc-text">概念</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#建立与训练"><span class="post-toc-number">2.4.2.</span> <span class="post-toc-text">建立与训练</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#采样（Sampling）"><span class="post-toc-number">2.4.3.</span> <span class="post-toc-text">采样（Sampling）</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#RNN的梯度消失与梯度爆炸"><span class="post-toc-number">2.5.</span> <span class="post-toc-text">RNN的梯度消失与梯度爆炸</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#高级循环神经网络"><span class="post-toc-number">3.</span> <span class="post-toc-text">高级循环神经网络</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#GRU（Gated-Recurrent-Units-门控循环单元）"><span class="post-toc-number">3.1.</span> <span class="post-toc-text">GRU（Gated Recurrent Units, 门控循环单元）</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#LSTM（Long-Short-Term-Memory，长短期记忆）"><span class="post-toc-number">3.2.</span> <span class="post-toc-text">LSTM（Long Short Term Memory，长短期记忆）</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#双向循环神经网络（Bidirectional-RNN，BRNN）"><span class="post-toc-number">3.3.</span> <span class="post-toc-text">双向循环神经网络（Bidirectional RNN，BRNN）</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#深度循环神经网络（Deep-RNN-DRNN"><span class="post-toc-number">3.4.</span> <span class="post-toc-text">深度循环神经网络（Deep RNN, DRNN)</span></a></li></ol></li></ol>
        </nav>
    </aside>


<article id="post-deeplearning-ai学习笔记（十四）循环神经网络"
  class="post-article article-type-post fade" itemprop="blogPost">

    <div class="post-card">
        <h1 class="post-card-title">deeplearning.ai学习笔记（十四）循环神经网络（Recurrent Neural Network）</h1>
        <div class="post-meta">
            <time class="post-time" title="2019-08-31 13:18:21" datetime="2019-08-31T05:18:21.000Z"  itemprop="datePublished">2019-08-31</time>

            


            
<span id="busuanzi_container_page_pv" title="文章总阅读量" style='display:none'>
    <i class="icon icon-eye icon-pr"></i><span id="busuanzi_value_page_pv"></span>
</span>


        </div>
        <div class="post-content" id="post-content" itemprop="postContent">
            <h2 id="序列模型（Sequence-Model）"><a href="#序列模型（Sequence-Model）" class="headerlink" title="序列模型（Sequence Model）"></a>序列模型（Sequence Model）</h2><h3 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h3><p><strong>前后相互关联的数据</strong>，如自然语言、音频等。</p>
<p><strong>主要应用：</strong></p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://s2.ax1x.com/2019/08/31/mzHMHx.png" alt="序列模型的主要应用" title>
                </div>
                <div class="image-caption">序列模型的主要应用</div>
            </figure>
<h3 id="符号表示"><a href="#符号表示" class="headerlink" title="符号表示"></a>符号表示</h3><p>对于一个序列数据<script type="math/tex">x</script>，用<script type="math/tex">x^{⟨t⟩}</script>来表示这个数据中的第t个元素，用来表<script type="math/tex">y^{⟨t⟩}</script>示第t个标签，用<script type="math/tex">T_x</script>和<script type="math/tex">T_y</script>来表示输入和输出的长度。对于一段音频，元素可能是其中的几帧；对于一句话，元素可能是一到多个单词。<a id="more"></a></p>
<p>第i个序列数据的第t个元素用符号<script type="math/tex">x^{(i)⟨t⟩}</script>，第t个标签即为<script type="math/tex">y^{(i)⟨t⟩}</script>，对应有<script type="math/tex">T^{(i)}_x</script>和<script type="math/tex">T^{(i)}_y</script>。</p>
<h3 id="词向量：one-hot向量"><a href="#词向量：one-hot向量" class="headerlink" title="词向量：one-hot向量"></a>词向量：one-hot向量</h3><p>要表示一个词语：</p>
<ul>
<li><p>先建立一个<strong>词汇表（Vocabulary）</strong>/<strong>字典（Dictionary）</strong>，将需要表示的所有词语变为一个列向量，可以根据字母顺序排列；（例如一个包含10000个词的词汇表，可看成是10000 x 1的向量）</p>
<p><img src="https://s2.ax1x.com/2019/08/31/mzHKD1.png" alt="词汇表"></p>
</li>
<li><p>根据单词在向量中的位置，用 <strong>one-hot 向量（one-hot vector）</strong>来表示该单词的标签：将每个单词编码成一个<script type="math/tex">R^{|V|×1}</script>向量，其中<script type="math/tex">|V|</script>是词汇表中单词的数量。<strong>一个单词在词汇表中的索引在该向量对应的元素为1，其余元素均为0。</strong></p>
</li>
</ul>
<h3 id="标准神经网络在序列模型上存在的问题"><a href="#标准神经网络在序列模型上存在的问题" class="headerlink" title="标准神经网络在序列模型上存在的问题"></a>标准神经网络在序列模型上存在的问题</h3><ul>
<li>对于不同的示例，输入和输出可能有不同的长度，因此输入层和输出层的神经元数量无法固定。</li>
<li>从输入文本的不同位置学到的同一特征无法共享。</li>
<li>模型中的参数太多，计算量太大。</li>
</ul>
<p>为了解决这些问题，引入<strong>循环神经网络（Recurrent Neural Network，RNN）</strong>。</p>
<h2 id="基本循环神经网络"><a href="#基本循环神经网络" class="headerlink" title="基本循环神经网络"></a>基本循环神经网络</h2><h3 id="基本结构"><a href="#基本结构" class="headerlink" title="基本结构"></a>基本结构</h3><p>例：人名识别</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://s2.ax1x.com/2019/08/31/mzHA4U.png" alt="基本RNN模型（Tx=Ty）" title>
                </div>
                <div class="image-caption">基本RNN模型（Tx=Ty）</div>
            </figure>
<p>当元素<script type="math/tex">x^{⟨t⟩}</script>输入对应时间步（Time Step）的隐藏层的同时，该隐藏层也会接收来自上一时间步的隐藏层的激活值<script type="math/tex">a^{⟨t−1⟩}</script>，其中<script type="math/tex">a^{⟨0⟩}</script>一般直接初始化为零向量。一个时间步输出一个对应的预测结果<script type="math/tex">y^{⟨t⟩}</script>。</p>
<p>循环神经网络从左向右扫描数据，<strong>每个时间步的参数是共享的</strong>，输入、激活、输出的参数对应为 Wax、Waa、Wya。</p>
<h3 id="基本RNN单元"><a href="#基本RNN单元" class="headerlink" title="基本RNN单元"></a>基本RNN单元</h3><h4 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h4><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://s2.ax1x.com/2019/08/31/mzHmv9.png" alt="基本RNN单元前向传播" title>
                </div>
                <div class="image-caption">基本RNN单元前向传播</div>
            </figure>
<p>前向传播的公式如下：</p>
<script type="math/tex; mode=display">
a^{<0>}=\stackrel{\rightarrow}{0} \\
a^{<t>}=g_1(W_{aa}a^{<t-1>}+W_{ax}x^{<t>}+b_a)\\
\hat{y}=g_2(W_{ya}a^{<t>}+b_y)</script><p>激活函数g1通常选择tanh，有时也用ReLU；g2可选 sigmoid 或 softmax，取决于需要的输出类型。</p>
<p>为了进一步简化公式以方便运算，可以将Waa、Wax水平并列为一个矩阵Wa，同时<script type="math/tex">a^{⟨t−1⟩}</script>和<script type="math/tex">x^{⟨t⟩}</script>堆叠成一个矩阵。则有：</p>
<script type="math/tex; mode=display">
W_a=[W_{aa}|W_{ax}]\\
a^{<t>}=g_1(W_{a}[a^{<t-1>},x^{<t>}]+b_a)\\
\hat{y}=g_2(W_{y}a^{<t>}+b_y)</script><h4 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h4><p>对于人名识别，可用交叉熵做为损失函数：</p>
<script type="math/tex; mode=display">
L^{<t>}(\hat{y}^{<t>},y^{<t>})=-y^{<t>}log\hat{y}^{<t>}-(1-y^{<t>})log(1-\hat{y}^{<t>})\\
J=L(\hat{y},y)=\sum_{t=1}^{T_x}L^{<t>}(\hat{y}^{<t>},y^{<t>})</script><h4 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h4><p>循环神经网络的反向传播被称为<strong>基于时间的反向传播（Backpropagation through time）</strong>，因为从右向左计算的过程就像是时间倒流。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://s2.ax1x.com/2019/08/31/mzHZ34.png" alt="基本RNN单元反向传播" title>
                </div>
                <div class="image-caption">基本RNN单元反向传播</div>
            </figure>
<h3 id="循环神经网络的架构"><a href="#循环神经网络的架构" class="headerlink" title="循环神经网络的架构"></a>循环神经网络的架构</h3><p>根据Tx与Ty的关系，RNN模型包含以下几个类型：</p>
<ul>
<li><strong>Many to one</strong>：<script type="math/tex">T_x \gt 1,T_y=1</script>，如情绪判断</li>
<li><strong>One to many</strong>：<script type="math/tex">T_x=1,T_y \gt 1</script>，如音乐生成</li>
<li><strong>One to one</strong>：<script type="math/tex">T_x=1,T_y=1</script></li>
<li><strong>Many to many</strong>：<script type="math/tex">T_x=T_y</script>，如人名检测</li>
<li><strong>Many to many</strong>：<script type="math/tex">T_x\neq T_y</script>，如机器翻译</li>
</ul>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://s2.ax1x.com/2019/08/31/mzHegJ.jpg" alt="循环神经网络的架构" title>
                </div>
                <div class="image-caption">循环神经网络的架构</div>
            </figure>
<h3 id="实例：语言模型（Language-Model）"><a href="#实例：语言模型（Language-Model）" class="headerlink" title="实例：语言模型（Language Model）"></a>实例：语言模型（Language Model）</h3><h4 id="概念-1"><a href="#概念-1" class="headerlink" title="概念"></a>概念</h4><p><strong>语言模型</strong>是根据语言客观事实而进行的语言抽象数学建模，能够<strong>估计某个序列中各元素出现的可能性</strong>。例如，在一个语音识别系统中，语言模型能够<strong>计算两个读音相近的句子为正确结果的概率</strong>，以此为依据作出准确判断。</p>
<h4 id="建立与训练"><a href="#建立与训练" class="headerlink" title="建立与训练"></a>建立与训练</h4><p><strong>建立训练集：</strong>大型<strong>语料库（Corpus）</strong>，指数量众多的句子组成的文本。</p>
<ol>
<li><strong>标记化（Tokenize）</strong>，即建立字典；</li>
<li>将语料库中的每个词表示为对应的 one-hot 向量。</li>
</ol>
<p>注：</p>
<ul>
<li>增加一个额外的标记<strong>EOS（End of Sentence）</strong>来表示一个句子的结尾。</li>
<li>标点符号可以忽略，也可以加入字典后用one-hot向量表示。</li>
<li>对于语料库中部分特殊的、不包含在字典中的词汇，例如人名、地名，可在词典中加入一个 <strong>UNK（Unique Token）</strong>标记来表示。</li>
</ul>
<p><strong>将标志化后的训练集用于训练 RNN：</strong></p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://s2.ax1x.com/2019/08/31/mzHkNT.jpg" alt="语言模型训练过程" title>
                </div>
                <div class="image-caption">语言模型训练过程</div>
            </figure>
<ul>
<li>在第一个时间步中，输入的<script type="math/tex">a^{⟨0⟩}</script>和<script type="math/tex">x^{⟨1⟩}</script>都是零向量，<script type="math/tex">y^{⟨1⟩}</script>是通过softmax预测出的字典中每个词作为第一个词出现的概率；</li>
<li>在第二个时间步中，输入的<script type="math/tex">x^{⟨2⟩}</script>是训练样本的标签中的第一个单词<script type="math/tex">y^{⟨1⟩}</script>（即“cats”）和上一层的激活项<script type="math/tex">a^{⟨1⟩}</script>，输出的<script type="math/tex">y^{⟨2⟩}</script>表示的是通过softmax预测出的单词“cats”后面出现字典中的其他每个词的条件概率；</li>
<li>以此类推，最后就可以得到整个句子出现的概率。</li>
</ul>
<p><strong>定义损失函数与成本函数为：</strong></p>
<script type="math/tex; mode=display">
L(\hat{y}^{<t>},y^{<t>})=-\sum_{t} y_i^{<t>}log\hat{y}^{<t>}\\
J=\sum_t L^{<t>}(\hat{y}^{<t>},y^{<t>})</script><h4 id="采样（Sampling）"><a href="#采样（Sampling）" class="headerlink" title="采样（Sampling）"></a>采样（Sampling）</h4><p>在训练好一个语言模型后，可以通过<strong>采样（Sample）</strong>新的序列来了解这个模型中都学习到了一些什么。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://s2.ax1x.com/2019/08/31/mzHPH0.png" alt="采样" title>
                </div>
                <div class="image-caption">采样</div>
            </figure>
<p>在第一个时间步输入<script type="math/tex">a^{⟨0⟩}</script> 、<script type="math/tex">x^{⟨1⟩}</script>为零向量，输出预测出的字典中每个词作为第一个词出现的概率，根据 softmax 的分布进行随机采样（<code>np.random.choice</code>），将采样得到的<script type="math/tex">y^{⟨1⟩}</script>作为第二个时间步的输入<script type="math/tex">x^{⟨2⟩}</script>。以此类推，直到采样到EOS，最后模型会自动生成一些句子，从这些句子中可以发现模型通过语料库学习到的知识。</p>
<h3 id="RNN的梯度消失与梯度爆炸"><a href="#RNN的梯度消失与梯度爆炸" class="headerlink" title="RNN的梯度消失与梯度爆炸"></a>RNN的梯度消失与梯度爆炸</h3><ul>
<li><p><strong>RNN的梯度消失：</strong>由于梯度消失，在反向传播时网络很难调整靠前的参数，因此<strong>基本RNN不擅长捕获长距离的依赖关系</strong>。</p>
<p>如：对于下面两个句子，后面的动词单复数形式由前面的名词的单复数形式决定。</p>
<script type="math/tex; mode=display">
The~cat,which~already~ate~a~bunch~of~food,~was~full.\\
The~cats,which~already~ate~a~bunch~of~food,~were~full.</script></li>
<li><p><strong>RNN的梯度爆炸：</strong>因为参数会急剧膨胀到数值溢出（可能显示为 NaN），梯度爆炸比较容易发现。可以采用<strong>梯度修剪（Gradient Clipping）</strong>来解决，即观察梯度向量，如果它大于某个阈值，则缩放梯度向量以保证其不会太大。</p>
</li>
</ul>
<p>相比之下，<strong>梯度消失问题更难解决</strong>。<strong>GRU 和 LSTM 都可以作为缓解梯度消失问题的方案</strong>。</p>
<h2 id="高级循环神经网络"><a href="#高级循环神经网络" class="headerlink" title="高级循环神经网络"></a>高级循环神经网络</h2><h3 id="GRU（Gated-Recurrent-Units-门控循环单元）"><a href="#GRU（Gated-Recurrent-Units-门控循环单元）" class="headerlink" title="GRU（Gated Recurrent Units, 门控循环单元）"></a>GRU（Gated Recurrent Units, 门控循环单元）</h3><p><strong>GRU</strong>改善了 RNN 的隐藏层，使其可以更好地捕捉深层连接，并改善了梯度消失问题。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://s2.ax1x.com/2019/08/31/mzHSjs.png" alt="GRU单元" title>
                </div>
                <div class="image-caption">GRU单元</div>
            </figure>
<p>GRU 单元有一个新的变量称为<script type="math/tex">c</script>，代表<strong>记忆细胞（Memory Cell）</strong>，其作用是提供记忆的能力，例如记忆前文主语是单数还是复数等信息。在时间<script type="math/tex">t</script>，记忆细胞的值<script type="math/tex">c^{⟨t⟩}</script>等于输出的激活值<script type="math/tex">a^{⟨t⟩}</script>，<script type="math/tex">\tilde{c}^{⟨t⟩}</script>代表下一个<script type="math/tex">c</script>的候选值。<script type="math/tex">Γ_u</script>代表<strong>更新门（Update Gate）</strong>，用于决定什么时候更新记忆细胞的值。<strong>当<script type="math/tex">\Gamma_u=1</script>时，代表更新；当<script type="math/tex">\Gamma_u=0</script>时，代表记忆，保留之前的模块输出。</strong></p>
<p>相应表达式为：</p>
<script type="math/tex; mode=display">
\tilde{c}^{⟨t⟩}=tanh(W_c[c^{<t-1>},x^{<t>}]+b_c)\\
\Gamma _u = \sigma (W_u[c^{<t-1>},x^{<t>}]+b_u)\\
c^{<t>}=\Gamma _u * \tilde{c}^{⟨t⟩} + (1-\Gamma _u ) *c^{<t-1>}\\
a^{<t>}=c^{<t-1>}</script><p>当使用 sigmoid 作为激活函数来得到<script type="math/tex">\Gamma_u</script>时，<script type="math/tex">\Gamma_u</script>的值在 0 到 1 的范围内，且大多数时间非常接近于 0 或 1。 因为<script type="math/tex">\Gamma_u</script>可以很接近 0，因此<script type="math/tex">c^{⟨t⟩}</script>几乎就等于<script type="math/tex">c^{⟨t-1⟩}</script>。在经过很长的序列后，<script type="math/tex">c</script>的值依然被维持，从而实现“记忆”的功能。  </p>
<p>以上实际上是简化过的GRU单元，完整的GRU单元添加了一个新的<strong>相关门（Relevance Gate）</strong><script type="math/tex">\Gamma_r</script>，表示<script type="math/tex">\tilde{c}^{⟨t⟩}</script>和<script type="math/tex">c^{⟨t⟩}</script>的相关性。表达式为：</p>
<script type="math/tex; mode=display">
\tilde{c}^{⟨t⟩}=tanh(W_c[\Gamma_r*c^{<t-1>},x^{<t>}]+b_c)\\
\Gamma _u = \sigma (W_u[c^{<t-1>},x^{<t>}]+b_u)\\
\Gamma_r = \sigma (W_r[c^{<t-1>},x^{<t>}]+b_r)\\
c^{<t>}=\Gamma _u * \tilde{c}^{⟨t⟩} + (1-\Gamma _u ) *c^{<t-1>}\\
a^{<t>}=c^{<t-1>}</script><p>注：公式中的“*”表示元素相乘，而非矩阵相乘。</p>
<p>相关论文：</p>
<ul>
<li><a href="https://arxiv.org/pdf/1409.1259.pdf" target="_blank" rel="noopener">Cho et al., 2014. On the properties of neural machine translation: Encoder-decoder approaches</a></li>
<li><a href="https://arxiv.org/pdf/1412.3555.pdf" target="_blank" rel="noopener">Chung et al., 2014. Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling</a></li>
</ul>
<h3 id="LSTM（Long-Short-Term-Memory，长短期记忆）"><a href="#LSTM（Long-Short-Term-Memory，长短期记忆）" class="headerlink" title="LSTM（Long Short Term Memory，长短期记忆）"></a>LSTM（Long Short Term Memory，长短期记忆）</h3><p><strong>LSTM</strong>网络比 GRU 更加灵活和强大，它额外引入了<strong>遗忘门（Forget Gate）</strong><script type="math/tex">Γ_f</script>和<strong>输出门（Output Gate）</strong> <script type="math/tex">Γ_o</script>。其结构图和公式如下：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://s2.ax1x.com/2019/08/31/mzHFEV.png" alt="LSTM单元" title>
                </div>
                <div class="image-caption">LSTM单元</div>
            </figure>
<p>将多个 LSTM 单元按时间次序连接起来，就得到一个 LSTM 网络。</p>
<p>以上是简化版的 LSTM。在更为常用的版本中，几个门值不仅取决于<script type="math/tex">a^{⟨t−1⟩}</script>和<script type="math/tex">x^{⟨t⟩}</script>，有时也可以偷窥上一个记忆细胞输入的值<script type="math/tex">c^{⟨t−1⟩}</script>，这被称为<strong>窥视孔连接（Peephole Connection)</strong>。此时<script type="math/tex">c^{⟨t−1⟩}</script>和门值是一对一的。</p>
<p>相关论文：<a href="https://www.researchgate.net/publication/13853244_Long_Short-term_Memory" target="_blank" rel="noopener">Hochreiter &amp; Schmidhuber 1997. Long short-term memory</a></p>
<h3 id="双向循环神经网络（Bidirectional-RNN，BRNN）"><a href="#双向循环神经网络（Bidirectional-RNN，BRNN）" class="headerlink" title="双向循环神经网络（Bidirectional RNN，BRNN）"></a>双向循环神经网络（Bidirectional RNN，BRNN）</h3><p>单向的循环神经网络在某一时刻的预测结果只能使用之前输入的序列信息。<strong>双向循环神经网络</strong>可以在序列的任意位置使用之前和之后的数据。其工作原理是增加一个反向循环层，结构如下图所示：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://s2.ax1x.com/2019/08/31/mzHCBq.jpg" alt="BRNN" title>
                </div>
                <div class="image-caption">BRNN</div>
            </figure>
<script type="math/tex; mode=display">
y^{<t>}=g(W_y[\stackrel{\rightarrow}{a}^{<t>},\stackrel{\leftarrow}{a}^{<t>}]+b_y)</script><p>这个改进的方法不仅能用于基本的 RNN，也可以用于 GRU 或 LSTM。</p>
<p><strong>缺点：</strong>需要完整的序列数据，才能预测任意位置的结果。例如构建语音识别系统，需要等待用户说完并获取整个语音表达，才能处理这段语音并进一步做语音识别。</p>
<h3 id="深度循环神经网络（Deep-RNN-DRNN"><a href="#深度循环神经网络（Deep-RNN-DRNN" class="headerlink" title="深度循环神经网络（Deep RNN, DRNN)"></a>深度循环神经网络（Deep RNN, DRNN)</h3><p>循环神经网络的每个时间步上也可以包含多个隐藏层，形成<strong>深度循环神经网络</strong>。结构如下图所示：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://s2.ax1x.com/2019/08/31/mz7zcj.png" alt="DRNN" title>
                </div>
                <div class="image-caption">DRNN</div>
            </figure>
<script type="math/tex; mode=display">
a^{[l]<t>}=g(W_a^{[l]}[a^{[l]<t-1>},a^{[l-1]<t>}]+b_a^{[l]})</script><p>注：<strong>DRNN一般层数较少</strong>，3层RNN已经较复杂了。</p>
<p>另外一种Deep RNNs结构是每个输出层上还有一些垂直单元，如下图所示：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://s2.ax1x.com/2019/08/31/mzH9un.png" alt="含垂直单元的DRNN" title>
                </div>
                <div class="image-caption">含垂直单元的DRNN</div>
            </figure>
        </div>

        <blockquote class="post-copyright">
    
    <div class="content">
        
<span class="post-time">
    Last updated: <time datetime="2019-09-02T18:17:51.473Z" itemprop="dateUpdated">2019-09-03 02:17:51</time>
</span><br>


        
        版权声明:本文为博主原创文章,未经博主允许不得转载。<a href="/2019/08/31/deeplearning-ai学习笔记（十四）循环神经网络/" target="_blank" rel="external">http://yoursite.com/2019/08/31/deeplearning-ai学习笔记（十四）循环神经网络/</a>
        
    </div>
    
    <footer>
        <a href="http://yoursite.com">
            <img src="/img/avatar.jpg" alt="刘明辉">
            刘明辉
        </a>
    </footer>
</blockquote>

        
<div class="page-reward">
    <a id="rewardBtn" href="javascript:;" class="page-reward-btn waves-effect waves-circle waves-light">赏</a>
</div>



        <div class="post-footer">
            
	<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/deeplearning-ai/">deeplearning.ai</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/学习笔记/">学习笔记</a></li></ul>


            
<div class="page-share-wrap">
    

<div class="page-share" id="pageShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://yoursite.com/2019/08/31/deeplearning-ai学习笔记（十四）循环神经网络/&title=《deeplearning.ai学习笔记（十四）循环神经网络（Recurrent Neural Network）》 — 一只程序喵&pic=http://yoursite.com/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://yoursite.com/2019/08/31/deeplearning-ai学习笔记（十四）循环神经网络/&title=《deeplearning.ai学习笔记（十四）循环神经网络（Recurrent Neural Network）》 — 一只程序喵&source=序列模型（Sequence Model）概念前后相互关联的数据，如自然语言、音频等。
主要应用：

                
          ..." data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://yoursite.com/2019/08/31/deeplearning-ai学习笔记（十四）循环神经网络/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《deeplearning.ai学习笔记（十四）循环神经网络（Recurrent Neural Network）》 — 一只程序喵&url=http://yoursite.com/2019/08/31/deeplearning-ai学习笔记（十四）循环神经网络/&via=http://yoursite.com" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://yoursite.com/2019/08/31/deeplearning-ai学习笔记（十四）循环神经网络/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>



    <a href="javascript:;" id="shareFab" class="page-share-fab waves-effect waves-circle">
        <i class="icon icon-share-alt icon-lg"></i>
    </a>
</div>



        </div>
    </div>

    
<nav class="post-nav flex-row flex-justify-between">
  
    <div class="waves-block waves-effect prev">
      <a href="/2019/09/01/deeplearning-ai学习笔记（十五）自然语言处理与词嵌入/" id="post-prev" class="post-nav-link">
        <div class="tips"><i class="icon icon-angle-left icon-lg icon-pr"></i> Prev</div>
        <h4 class="title">deeplearning.ai学习笔记（十五）自然语言处理与词嵌入</h4>
      </a>
    </div>
  

  
    <div class="waves-block waves-effect next">
      <a href="/2019/08/29/deeplearning-ai学习笔记（十三）卷积神经网络应用之人脸识别-神经风格迁移/" id="post-next" class="post-nav-link">
        <div class="tips">Next <i class="icon icon-angle-right icon-lg icon-pl"></i></div>
        <h4 class="title">deeplearning.ai学习笔记（十三）卷积神经网络应用之人脸识别&amp;神经风格迁移</h4>
      </a>
    </div>
  
</nav>



    




















</article>

<div id="reward" class="page-modal reward-lay">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <h3 class="reward-title">
        <i class="icon icon-quote-left"></i>
        谢谢打赏ο(=•ω＜=)ρ⌒☆
        <i class="icon icon-quote-right"></i>
    </h3>
    <div class="reward-content">
        
        <div class="reward-code">
            <img id="rewardCode" src="/img/wechat.jpg" alt="打赏二维码">
        </div>
        
        <label class="reward-toggle">
            <input id="rewardToggle" type="checkbox" class="reward-toggle-check"
                data-wechat="/img/wechat.jpg" data-alipay="/img/alipay.jpg">
            <div class="reward-toggle-ctrol">
                <span class="reward-toggle-item wechat">微信</span>
                <span class="reward-toggle-label"></span>
                <span class="reward-toggle-item alipay">支付宝</span>
            </div>
        </label>
        
    </div>
</div>



</div>

        <footer class="footer">
    <div class="top">
        
<p>
    <span id="busuanzi_container_site_uv" style='display:none'>
        站点总访客数：<span id="busuanzi_value_site_uv"></span>
    </span>
    <span id="busuanzi_container_site_pv" style='display:none'>
        站点总访问量：<span id="busuanzi_value_site_pv"></span>
    </span>
</p>


        <p>
            
                <span><a href="/atom.xml" target="_blank" class="rss" title="rss"><i class="icon icon-lg icon-rss"></i></a></span>
            
            <span>This blog is licensed under a <a rel="license" href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>.</span>
        </p>
    </div>
    <div class="bottom">
        <p><span>刘明辉 &copy; 2015 - 2019</span>
            <span>
                
                Power by <a href="http://hexo.io/" target="_blank">Hexo</a> Theme <a href="https://github.com/yscoder/hexo-theme-indigo" target="_blank">indigo</a>
            </span>
        </p>
    </div>
</footer>

    </main>
    <div class="mask" id="mask"></div>
<a href="javascript:;" id="gotop" class="waves-effect waves-circle waves-light"><span class="icon icon-lg icon-chevron-up"></span></a>



<div class="global-share" id="globalShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://yoursite.com/2019/08/31/deeplearning-ai学习笔记（十四）循环神经网络/&title=《deeplearning.ai学习笔记（十四）循环神经网络（Recurrent Neural Network）》 — 一只程序喵&pic=http://yoursite.com/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://yoursite.com/2019/08/31/deeplearning-ai学习笔记（十四）循环神经网络/&title=《deeplearning.ai学习笔记（十四）循环神经网络（Recurrent Neural Network）》 — 一只程序喵&source=序列模型（Sequence Model）概念前后相互关联的数据，如自然语言、音频等。
主要应用：

                
          ..." data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://yoursite.com/2019/08/31/deeplearning-ai学习笔记（十四）循环神经网络/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《deeplearning.ai学习笔记（十四）循环神经网络（Recurrent Neural Network）》 — 一只程序喵&url=http://yoursite.com/2019/08/31/deeplearning-ai学习笔记（十四）循环神经网络/&via=http://yoursite.com" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://yoursite.com/2019/08/31/deeplearning-ai学习笔记（十四）循环神经网络/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>


<div class="page-modal wx-share" id="wxShare">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <p>扫一扫，分享到微信</p>
    <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAPYAAAD2CAAAAADAeSUUAAADHUlEQVR42u3aQW6DUAwE0N7/0nTTRTaQsU2k4D5WFYHkPyJ1Yvv//MTH8XK8njm78q5rklfPrrnhwMbGxn4I+7g8JtecUZPzZ69en0nWiY2Njb2VncfMNWySojngem3Rw8XGxsb+x+zkQeRBUg2k5Dw2NjY2dh4MeQmRRF1UQozbWNjY2Nhb2dWf+/kH5GeSkJs3v7CxsbH3sXvtnu/8+yPzbWxsbOwvZh/Foxp7SThNGkm9AxsbG3sTOw+AfLg751WLlub6sbGxsVewez/6e4GXRFq+MWjyJWFjY2NvYk+aONUtO9VA6o0Q8ijFxsbG3sTujWAny8qXnj/W6ntiY2Njb2Ln//p7TfzJhs58Dc3QwsbGxn44+xORUy1R8gZT8lAK4wFsbGzsRezegLb66ihjiyPeQr2FjY2NvYJd/bk/eRx5aPXWEzXCsLGxsVezrz+4t9y7Wk759dFmHWxsbOxF7LxBnyzlrrHupHn05l5sbGzsRezq9POucW/efuqVN4V4xsbGxn44u9fEqY5mq4Pk6r3VZhM2Njb2PvZkSJCzq+2h5KFUS5pmVmNjY2M/hF1dYhJg+Vae5K68BVbesoONjY39WHb10mpDZ17eTMbMb4zY2NjYi9j5UqpFQrXdU11DL3SxsbGx97HzQWx+zaRR1Rs2NOswbGxs7Iezq437/OgVD3m5MroLGxsbeyl73iSaL/0T5cpp1YWNjY29gl0tJyZj4OTVyZadcsGDjY2NvYJdDZLeZp3qsCFvMFWbWX/nsbGxsRexJ236Xmkxv3L+BWBjY2NvZVebO0m50ms2JefnAwxsbGzsrexJy7537yTY8gAb9dKwsbGxH8iuPoi8MJiE2V1biLCxsbE3sY/iMdm+09vuMwmq05VgY2NjL2JPci9p90yokweUbznCxsbG3sHuhdZdjyMvh6ojhDfviY2Njb2O3YucSfj1WlqTSS42NjY2djJwLQfJoPipFiTY2NjY2HlI5Phe6fKR+TY2Njb2w9lJPPRaRZMgvGvQO+qlYWNjY389u9egmc+T54OE/B2aAYmNjY397exf8dc1AKyR97IAAAAASUVORK5CYII=" alt="微信分享二维码">
</div>




    <script src="//cdn.bootcss.com/node-waves/0.7.4/waves.min.js"></script>
<script>
var BLOG = { ROOT: '/', SHARE: true, REWARD: true };


</script>

<script src="//unpkg.com/hexo-theme-material-indigo@latest/js/main.min.js"></script>


<div class="search-panel" id="search-panel">
    <ul class="search-result" id="search-result"></ul>
</div>
<template id="search-tpl">
<li class="item">
    <a href="{path}" class="waves-block waves-effect">
        <div class="title ellipsis" title="{title}">{title}</div>
        <div class="flex-row flex-middle">
            <div class="tags ellipsis">
                {tags}
            </div>
            <time class="flex-col time">{date}</time>
        </div>
    </a>
</li>
</template>

<script src="//unpkg.com/hexo-theme-material-indigo@latest/js/search.min.js" async></script>






<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>



<script>
(function() {
    var OriginTitile = document.title, titleTime;
    document.addEventListener('visibilitychange', function() {
        if (document.hidden) {
            document.title = '一只程序喵 | 刘明辉的个人博客';
            clearTimeout(titleTime);
        } else {
            document.title = '一只程序喵 | 刘明辉的个人博客';
            titleTime = setTimeout(function() {
                document.title = OriginTitile;
            },2000);
        }
    });
})();
</script>



<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

</body>
</html>
