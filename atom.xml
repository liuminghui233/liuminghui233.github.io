<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>一只程序喵</title>
  
  <subtitle>刘明辉的个人博客</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-08-19T14:36:11.721Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>刘明辉</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>deeplearning.ai学习笔记（六）优化深度神经网络之优化算法（Optimization algorithms）</title>
    <link href="http://yoursite.com/2019/08/18/deeplearning-ai%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%85%AD%EF%BC%89%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B9%8B%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%EF%BC%88Optimization-algorithms%EF%BC%89/"/>
    <id>http://yoursite.com/2019/08/18/deeplearning-ai学习笔记（六）优化深度神经网络之优化算法（Optimization-algorithms）/</id>
    <published>2019-08-18T08:47:56.000Z</published>
    <updated>2019-08-19T14:36:11.721Z</updated>
    
    <content type="html"><![CDATA[<p>本节课学习深度神经网络中的一些优化算法，通过使用这些技巧和方法来<strong>提高神经网络的训练速度和精度</strong>。</p><h2 id="小批量梯度下降算法（mini-batch-gradient-descent）"><a href="#小批量梯度下降算法（mini-batch-gradient-descent）" class="headerlink" title="小批量梯度下降算法（mini-batch gradient descent）"></a>小批量梯度下降算法（<strong>mini-batch</strong> gradient descent）</h2><h3 id="算法思想"><a href="#算法思想" class="headerlink" title="算法思想"></a>算法思想</h3><p>如果样本数量m很大，如达到百万数量级，由于受到矩阵运算速度的限制，训练速度往往会很慢。因此，<strong>把m个训练样本分成若干个子集（mini-batches），然后每次在单一子集上进行神经网络训练</strong>。</p><p>例：假设总的训练样本个数m=5000000，其维度为(n_x,m)。将其分成5000个子集，每个mini-batch含有1000个样本。我们将每个mini-batch记为X^{t}，其维度为(n_x,1000)。相应的每个mini-batch的输出记为Y^{t}，其维度为(1,1000)，且t=1,2,⋯,5000。</p><h3 id="算法过程"><a href="#算法过程" class="headerlink" title="算法过程"></a>算法过程</h3><p>先将总的训练样本分成T个子集（mini-batches），然后对每个mini-batch进行神经网络训练，包括Forward Propagation，Compute Cost Function，Backward Propagation，循环至T个mini-batch都训练完毕。</p><script type="math/tex; mode=display">\begin{align}& for~~t=1,\cdots,T\\&~~~~Forward~Propagation\\&~~~~Compute~Cost~Function\\&~~~~Backward~Propagation\\&~~~~W^{\{ t \}}:=W^{\{ t \}} - \alpha \centerdot dW^{\{ t \}}\\&~~~~b^{\{ t \}}:=b^{\{ t \}} - \alpha \centerdot db^{\{ t \}}\\\end{align}</script><p>经过T次循环之后，所有m个训练样本都进行了梯度下降计算。这个过程称为一个epoch，一个epoch会进行T次梯度下降算法。</p><p>注：对于Mini-Batches Gradient Descent，可以进行多次epoch训练；每次epoch最好将总体训练数据重新打乱、重新分成T组mini-batches。</p><p><strong>cost曲线：</strong></p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://s2.ax1x.com/2019/08/19/m8lcVJ.png" alt="cost曲线比较" title>                </div>                <div class="image-caption">cost曲线比较</div>            </figure><p>出现细微振荡的原因是不同的mini-batch之间是有差异的，但整体的趋势是下降的，最终也能得到较低的cost值。</p><h3 id="超参数-mini-batch-size-选取"><a href="#超参数-mini-batch-size-选取" class="headerlink" title="超参数 mini-batch size 选取"></a>超参数 <code>mini-batch size</code> 选取</h3><p><strong>考虑两种极端情况：</strong></p><ul><li>如果<strong>mini-batch size=m</strong>，即为Batch gradient descent，只包含一个子集为(X^{1},Y^{1})=(X,Y)。会比较平稳地接近全局最小值，但是因为使用了所有m个样本，每次前进的速度有些慢。</li><li>如果<strong>mini-batch size=1</strong>，即为随机梯度下降（Stachastic gradient descent），每个样本就是一个子集(X^{i},Y^{i})=(x^(i),y^(i))，共有m个子集。每次前进速度很快，但是路线曲折，有较大的振荡，最终会在最小值附近来回波动，难以真正达到最小值处。而且在数值处理上就不能使用向量化的方法来提高运算速度。</li></ul><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://s2.ax1x.com/2019/08/19/m8lf8x.png" alt="两种极端情况下梯度下降图示" title>                </div>                <div class="image-caption">两种极端情况下梯度下降图示</div>            </figure><p><strong>正确选取原则：</strong></p><ul><li>一般来说，如果总体样本数量m不太大时（如<em>m</em>≤2000），建议直接使用Batch gradient descent。</li><li>如果总体样本数量m很大时，建议将样本分成许多mini-batches。mini-batch size不能设置得太大，也不能太小。推荐选取2的幂做为mini-batch size的值（计算机存储数据一般是2的幂，这样设置可以提高运算速度），<strong>常用的有64、128、256、512</strong>。</li></ul><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://s2.ax1x.com/2019/08/19/m8l4xK.png" alt="适中mini-batch size时的梯度下降" title>                </div>                <div class="image-caption">适中mini-batch size时的梯度下降</div>            </figure><h2 id="动量梯度下降算法（Gradient-descent-with-momentum）"><a href="#动量梯度下降算法（Gradient-descent-with-momentum）" class="headerlink" title="动量梯度下降算法（Gradient descent with momentum）"></a>动量梯度下降算法（Gradient descent with momentum）</h2><h3 id="数学基础：指数加权平均（exponentially-weighted-averages）"><a href="#数学基础：指数加权平均（exponentially-weighted-averages）" class="headerlink" title="数学基础：指数加权平均（exponentially weighted averages）"></a>数学基础：指数加权平均（exponentially weighted averages）</h3><h4 id="实例：伦敦市半年内气温整体变化趋势"><a href="#实例：伦敦市半年内气温整体变化趋势" class="headerlink" title="实例：伦敦市半年内气温整体变化趋势"></a>实例：伦敦市半年内气温整体变化趋势</h4><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://s2.ax1x.com/2019/08/19/m8lIKO.png" alt="伦敦市半年内气温散点图" title>                </div>                <div class="image-caption">伦敦市半年内气温散点图</div>            </figure><p>通过移动平均（moving average）的方法来对每天气温进行平滑处理：</p><p>设<em>V</em>0=0，做为第0天的气温值。第一天至第t天气温可按如下计算：</p><script type="math/tex; mode=display">\begin{align}&V_1=0.9V_0+0.1 \theta_1 \\&V_2=0.9V_1+0.1 \theta_2 = 0.9(0.9V_0+0.1 \theta_1)+0.1 \theta_2 = 0.9^2V_0+0.9 \centerdot 0.1\theta_1+0.1\theta_2\\&V_3=0.9V_2+0.1 \theta_3=0.9(0.9^2V_0+0.9 \centerdot 0.1\theta_1+0.1\theta_2)+0.1 \theta_3 \\&~~~~= 0.9^3V_0+0.9^2 \centerdot 0.1\theta_1+0.9 \centerdot 0.1\theta_2+0.1\theta_3 \\&~~~~~~~~~~~~~\vdots \\&V_t=0.9V_{t-1}+0.1 \theta_t=0.9^tV_0+0.9^{t-1}\centerdot 0.1\theta_1 + 0.9^{t-2}\centerdot 0.1\theta_2 + \cdots + 0.9\centerdot 0.1\theta_{t-1} + 0.1\theta_t\end{align}</script><p>这种滑动平均算法称为指数加权平均。</p><h4 id="指数加权平均的一般形式"><a href="#指数加权平均的一般形式" class="headerlink" title="指数加权平均的一般形式"></a>指数加权平均的一般形式</h4><script type="math/tex; mode=display">V_t=\beta V_{t-1} +(1-\beta)\theta_t</script><p><em>β</em>值决定了指数加权平均的天数。</p><script type="math/tex; mode=display">\begin{align}& 当\beta\rightarrow0,N=\frac{1}{1-\beta}\rightarrow \infty时:\\&~~~~~~ \beta^{\frac{1}{1-\beta}}=(1-\frac{1}{N})^N=\frac{1}{e}\end{align}</script><p>一般认为衰减到1/e就可以忽略不计了，故指数加权平均的天数的计算公式为：</p><script type="math/tex; mode=display">N=\frac{1}{1-\beta}</script><p><em>β</em>值越大，则指数加权平均的天数越多，平均后的趋势线就越平缓，同时向右平移。下图绿色曲线和黄色曲线分别表示了<em>β</em>=0.98和<em>β</em>=0.5时，指数加权平均的结果。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://s2.ax1x.com/2019/08/19/m8lh26.png" alt="不同β值指数加权平均的结果" title>                </div>                <div class="image-caption">不同β值指数加权平均的结果</div>            </figure><h4 id="偏差修正（bias-correction）"><a href="#偏差修正（bias-correction）" class="headerlink" title="偏差修正（bias correction）"></a>偏差修正（bias correction）</h4><p>当<em>β</em>=0.98时，指数加权平均结果如绿色曲线所示。但是实际上，真实曲线如紫色曲线所示。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://s2.ax1x.com/2019/08/19/m8lTqe.png" alt="真实情况下的指数加权平均结果" title>                </div>                <div class="image-caption">真实情况下的指数加权平均结果</div>            </figure><p>由于开始时设置<em>V</em>0=0，所以初始值会相对小一些，直到后面受前面的影响渐渐变小，趋于正常。</p><p><strong>偏差修正：</strong></p><script type="math/tex; mode=display">V_t:=\frac{V_t}{1-\beta^t}</script><ul><li>刚开始t比较小，(1−<em>β</em>^t)&lt;1，这样就将<em>V</em>t修正得更大一些；</li><li>随着t增大，(1−<em>β</em>^t)≈1，<em>V</em>t基本不变。</li></ul><h3 id="动量梯度下降算法思想"><a href="#动量梯度下降算法思想" class="headerlink" title="动量梯度下降算法思想"></a>动量梯度下降算法思想</h3><p>在每次训练时，<strong>对梯度进行指数加权平均</strong>，然后用得到的梯度值更新权重W和常数项b。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://s2.ax1x.com/2019/08/19/m8lorD.png" alt="动量梯度下降算法" title>                </div>                <div class="image-caption">动量梯度下降算法</div>            </figure><ul><li>原始的梯度下降算法如上图蓝色折线所示。在梯度下降过程中，对于W、b之间数值范围差别较大的情况，梯度下降的振荡较大。此时每一点处的梯度只与当前方向有关，产生类似折线的效果，前进缓慢。</li><li>如果对梯度进行指数加权平均，这样使当前梯度不仅与当前方向有关，还与之前的方向有关，这样处理让梯度前进方向更加平滑，减少振荡，能够更快地到达最小值处。</li></ul><h3 id="动量梯度下降算法过程"><a href="#动量梯度下降算法过程" class="headerlink" title="动量梯度下降算法过程"></a>动量梯度下降算法过程</h3><script type="math/tex; mode=display">\begin{align}& 初始化:V_{dW}=0,V_{db}=0,\beta=0.9 \\& On~iteration~t:\\&~~~~~Compute~dW,db~on~the~current~mini-binch\\&~~~~~V_{dW}=\beta V_{dW}+(1-\beta)dW\\&~~~~~V_{db}=\beta V_{db}+(1-\beta)db\\&~~~~~W:=W-\alpha V_{dW}\\&~~~~~b:=b-\alpha V_{db}\\\end{align}</script><h2 id="均方根传递优化算法（RMSprop-Root-Mean-Square-prop）"><a href="#均方根传递优化算法（RMSprop-Root-Mean-Square-prop）" class="headerlink" title="均方根传递优化算法（RMSprop, Root Mean Square prop）"></a>均方根传递优化算法（<strong>RMSprop</strong>, Root Mean Square prop）</h2><h3 id="算法过程-1"><a href="#算法过程-1" class="headerlink" title="算法过程"></a>算法过程</h3><p>按如下方式更新参数：</p><script type="math/tex; mode=display">\begin{align}&S_W=\beta S_{dW}+(1-\beta)dW^2\\&S_b=\beta S_{db}+(1-\beta)db^2\\&W:=W-\alpha \frac{dW}{\sqrt{S_w}},b:=b-\alpha \frac{db}{\sqrt{S_b}}\\\end{align}</script><p>为了避免RMSprop算法中分母为零，通常可以在分母增加一个极小的常数<em>ε</em>：</p><script type="math/tex; mode=display">W:=W-\alpha \frac{dW}{\sqrt{S_w}+\epsilon},b:=b-\alpha \frac{db}{\sqrt{S_b}+\epsilon}</script><p>其中，<em>ε</em>一般取10^−8。</p><h3 id="算法原理"><a href="#算法原理" class="headerlink" title="算法原理"></a>算法原理</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://s2.ax1x.com/2019/08/19/m8lHVH.png" alt="RMSprop算法原理" title>                </div>                <div class="image-caption">RMSprop算法原理</div>            </figure><p>从图中可以看出，梯度下降（蓝色折线）在垂直方向（b）上振荡较大，在水平方向（W）上振荡较小，表示在b方向上梯度较大，即<em>db</em>较大，而在W方向上梯度较小，即<em>dW</em>较小。因此，上述表达式中<em>Sb</em>较大，而<em>SW</em>较小。在更新W和b的表达式中，使得W变化得多一些，b变化得少一些。即加快了W方向的速度，减小了b方向的速度，减小振荡，实现快速梯度下降算法，其梯度下降过程如绿色折线所示。</p><h2 id="自适应矩估计优化算法-Adam-Adaptive-Moment-Estimation"><a href="#自适应矩估计优化算法-Adam-Adaptive-Moment-Estimation" class="headerlink" title="自适应矩估计优化算法(Adam, Adaptive Moment Estimation)"></a>自适应矩估计优化算法(<strong>Adam</strong>, Adaptive Moment Estimation)</h2><p><code>Adam优化算法</code>本质上是<strong>将动量算法和RMSprop结合</strong>起来。</p><h3 id="算法过程-2"><a href="#算法过程-2" class="headerlink" title="算法过程"></a>算法过程</h3><script type="math/tex; mode=display">\begin{align}& 初始化:V_{dW}=0,S_{dW}=0,V_{db}=0,S_{db}=0\\& On~iteration~t:\\&~~~~~Compute~dW,db~on~the~current~mini-binch\\&~~~~~V_{dW}=\beta_1 V_{dW}+(1-\beta_1)dW,V_{db}=\beta_1 V_{db}+(1-\beta_1)db\\&~~~~~S_{dW}=\beta_2 S_{dW}+(1-\beta_2)dW^2,S_{db}=\beta_2 S_{db}+(1-\beta_2)db^2\\&~~~~~V_{dW}^{corrected}=\frac{V_{dW}}{1-\beta_1^t},V_{db}^{corrected}=\frac{V_{db}}{1-\beta_1^t}\\&~~~~~S_{dW}^{corrected}=\frac{S_{dW}}{1-\beta_2^t},S_{db}^{corrected}=\frac{S_{db}}{1-\beta_2^t}\\&~~~~~W:=W-\alpha \frac{V_{dW}^{corrected}}{\sqrt{S_{dW}^{corrected}}+\epsilon},b:=b-\alpha \frac{V_{db}^{corrected}}{\sqrt{S_{db}^{corrected}}+\epsilon}\\\end{align}</script><h3 id="超参数选取"><a href="#超参数选取" class="headerlink" title="超参数选取"></a>超参数选取</h3><p><em>β</em>1通常设置为0.9，<em>β</em>2通常设置为0.999，<em>ε</em>通常设置为10^-8。(一般不会调整)</p><h2 id="学习率衰减（Learning-rate-decay）"><a href="#学习率衰减（Learning-rate-decay）" class="headerlink" title="学习率衰减（Learning rate decay）"></a>学习率衰减（Learning rate decay）</h2><h3 id="算法思想-1"><a href="#算法思想-1" class="headerlink" title="算法思想"></a>算法思想</h3><p><strong>随着迭代次数增加，学习因子<em>α</em>逐渐减小。</strong></p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://s2.ax1x.com/2019/08/19/m8lbad.png" alt="学习率衰减时的梯度下降" title>                </div>                <div class="image-caption">学习率衰减时的梯度下降</div>            </figure><ul><li>蓝色折线表示使用恒定的学习因子<em>α</em>，由于每次训练<em>α</em>相同，步进长度不变，在接近最优值处的振荡也大，在最优值附近较大范围内振荡，与最优值距离就比较远。</li><li>绿色折线表示使用衰减的<em>α</em>，随着训练次数增加，<em>α</em>逐渐减小，步进长度减小，使得能够在最优值处较小范围内微弱振荡，不断逼近最优值。</li></ul><h3 id="学习率衰减的方式"><a href="#学习率衰减的方式" class="headerlink" title="学习率衰减的方式"></a>学习率衰减的方式</h3><script type="math/tex; mode=display">\begin{align}& 1.~\alpha = \frac{1}{1+decay\_rate*epoch\_num}\alpha_0\\& （其中衰减率deacy\_rate为可调超参数、epoch\_num为迭代次数）\\&2.~\alpha = 0.95^{epoch\_num}\centerdot \alpha_0\\&3.~\alpha = \frac{k}{\sqrt{epoch\_num}} \centerdot \alpha_0~~or~~\frac{k}{\sqrt{mini-bach\_number}} \centerdot \alpha_0\\&（其中k为可调超参数）\\&4.~设置\alpha为关于t的离散值，随着t增加，\alpha呈阶梯式减小。\end{align}</script><h2 id="局部最优-Local-Optima-问题-amp-停滞区-Plateaus-问题"><a href="#局部最优-Local-Optima-问题-amp-停滞区-Plateaus-问题" class="headerlink" title="局部最优(Local Optima)问题 &amp; 停滞区(Plateaus)问题"></a>局部最优(Local Optima)问题 &amp; 停滞区(Plateaus)问题</h2><p>在使用梯度下降算法不断减小cost function时，可能会得到<strong>局部最优解（local optima）</strong>而不是<strong>全局最优解（global optima）</strong>。</p><p>在神经网络中，局部最优不能理解为如左图形碗状的凹槽，因为大部分梯度为零的点并不是这些凹槽处，而是形如右边所示的马鞍状，称为鞍点（saddle point）。特别是在神经网络中参数很多的情况下，所有参数梯度为零的点很可能是鞍点。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://s2.ax1x.com/2019/08/19/m8lOPI.png" alt="局部最优点 VS 鞍点" title>                </div>                <div class="image-caption">局部最优点 VS 鞍点</div>            </figure><p>类似马鞍状的停滞区（plateaus）会降低神经网络学习速度。停滞区是梯度接近于零的平缓区域，在停滞区上梯度很小，前进缓慢，到达鞍点需要很长时间。到达鞍点后，由于随机扰动，梯度一般能够沿着图中绿色箭头，离开鞍点，继续前进，只是在停滞区上花费了太多时间。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://s2.ax1x.com/2019/08/19/m8lqIA.png" alt="停滞区的梯度下降" title>                </div>                <div class="image-caption">停滞区的梯度下降</div>            </figure><p><strong>总结：</strong></p><ul><li>只要选择合理强大的神经网络，一般不太可能陷入局部最优处；</li><li>停滞区可能会使梯度下降变慢，降低学习速度。动量梯度下降、RMSprop、Adam算法都能有效解决停滞区下降过慢的问题。</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本节课学习深度神经网络中的一些优化算法，通过使用这些技巧和方法来&lt;strong&gt;提高神经网络的训练速度和精度&lt;/strong&gt;。&lt;/p&gt;
&lt;h2 id=&quot;小批量梯度下降算法（mini-batch-gradient-descent）&quot;&gt;&lt;a href=&quot;#小批量梯度下降算法（
      
    
    </summary>
    
    
      <category term="学习笔记" scheme="http://yoursite.com/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
      <category term="deeplearning.ai" scheme="http://yoursite.com/tags/deeplearning-ai/"/>
    
  </entry>
  
  <entry>
    <title>deeplearning.ai学习笔记（五）优化深度神经网络之应用层面的深度学习</title>
    <link href="http://yoursite.com/2019/08/16/deeplearning-ai%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%94%EF%BC%89%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%BA%94%E7%94%A8%E5%B1%82%E9%9D%A2%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    <id>http://yoursite.com/2019/08/16/deeplearning-ai学习笔记（五）优化深度神经网络之应用层面的深度学习/</id>
    <published>2019-08-16T12:03:14.000Z</published>
    <updated>2019-08-19T14:34:38.742Z</updated>
    
    <content type="html"><![CDATA[<h2 id="数据集：训练集-开发集-测试集（Train-Dev-Test-sets）"><a href="#数据集：训练集-开发集-测试集（Train-Dev-Test-sets）" class="headerlink" title="数据集：训练集/开发集/测试集（Train/Dev/Test sets）"></a>数据集：训练集/开发集/测试集（Train/Dev/Test sets）</h2><p><strong>为实现<code>交叉验证</code>（cross validation），数据集一般会划分为三个部分：</strong></p><ul><li>训练集（Train sets）：用于训练算法模型；</li><li>开发集（Dev sets）：用于验证不同算法模型的表现情况，从中选择最好的算法模型；</li><li>测试集（Test sets）：用于测试最好算法的实际表现（算法的无偏估计）。</li></ul><p>注：Test sets的目标主要是进行无偏估计。如果不需要无偏估计，也可以没有Test sets，可以通过Train sets训练不同的算法模型，然后分别在Dev sets上进行验证，根据结果选择最好的算法模型。（如果只有Train sets和Dev sets，通常把这里的Dev sets称为Test sets）</p><p><strong>比例分配：</strong></p><ul><li>样本数量不是很大（如100、1000、10000）：Train sets和Test sets的数量比例为70%/30%；如果有Dev sets，则设置比例为60%/20%/20%。</li><li>样本数量很大（如100万）：Dev sets和Test sets大到足以完成其目标即可，对于100万的样本，往往也只需要10000个样本就够了。因此，对于大数据样本，Train/Dev/Test sets的比例可设置为98%/1%/1%或99%/0.5%/0.5%。样本数据量越大，相应的Dev/Test sets的比例可以设置的越低一些。</li></ul><p><strong>训练样本和测试样本分布不匹配问题：</strong></p><p>训练样本和验证/测试样本可能来自不同的分布。一条经验原则是<strong>尽量保证Dev sets和Test sets来自于同一分布</strong>。</p><h2 id="偏差（Bias）与方差（Variance）"><a href="#偏差（Bias）与方差（Variance）" class="headerlink" title="偏差（Bias）与方差（Variance）"></a>偏差（Bias）与方差（Variance）</h2><h3 id="偏差、方差与算法的优劣"><a href="#偏差、方差与算法的优劣" class="headerlink" title="偏差、方差与算法的优劣"></a>偏差、方差与算法的优劣</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://s2.ax1x.com/2019/08/19/m8lga9.png" alt="偏差方差的各种情况" title>                </div>                <div class="image-caption">偏差方差的各种情况</div>            </figure><ol><li>高偏差（欠拟合，underfitting）：算法模型在训练样本和测试样本上的表现相差不大，但都不太好。（如：Train set error为15%，而Dev set error为16%）</li><li>高方差（过拟合，overfitting）：算法模型在训练样本上的表现很好，但是在测试样本上的表现却不太好。这说明了该<strong>模型泛化能力不强</strong>。（如：Train set error为1%，而Dev set error为11%）</li><li>低偏差&amp;低方差：最好情况的算法。</li><li>高偏差&amp;高方差：可以理解成某段区域是欠拟合的，某段区域是过拟合的，是最差情况的算法。</li></ol><p>总结：一般来说，<strong>Train set error体现了是否出现high bias</strong>；<strong>Dev set error与Train set error的相对差值体现了是否出现high variance</strong>。</p><h3 id="机器学习中算法评价的基本原则"><a href="#机器学习中算法评价的基本原则" class="headerlink" title="机器学习中算法评价的基本原则"></a>机器学习中算法评价的基本原则</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://s2.ax1x.com/2019/08/19/m8l02V.png" alt="算法评价流程" title>                </div>                <div class="image-caption">算法评价流程</div>            </figure><h3 id="高偏差和高方差的解决策略"><a href="#高偏差和高方差的解决策略" class="headerlink" title="高偏差和高方差的解决策略"></a>高偏差和高方差的解决策略</h3><p><strong>对于高偏差：</strong></p><ul><li>增加神经网络的隐藏层个数、神经元个数</li><li>延长训练时间</li><li>选择其它更复杂的神经网络模型</li><li>……</li></ul><p><strong>对于高方差：</strong></p><ul><li>增加训练样本数据</li><li><strong>进行正则化</strong>（Regularization）</li><li>选择其他更复杂的神经网络模型</li><li>……</li></ul><h2 id="正则化（Regularization）"><a href="#正则化（Regularization）" class="headerlink" title="正则化（Regularization）"></a>正则化（Regularization）</h2><h3 id="L2正则化"><a href="#L2正则化" class="headerlink" title="L2正则化"></a>L2正则化</h3><h4 id="具体实现"><a href="#具体实现" class="headerlink" title="具体实现"></a>具体实现</h4><p><strong>对于Logistic regression：</strong></p><script type="math/tex; mode=display">J(w,b)=\frac{1}{m} \sum_{i=1}^{m}L(\hat{y}^{(i)},y^{(i)})+\frac{\lambda}{2m}||w||_2^{2} \\||w||_2^{2}=\sum_{j=1}^{n_x}w_j^{2}=w^{T}w</script><p>注：</p><ul><li>由于<em>W</em>的维度很大，而<em>b</em>只是一个常数，参数很大程度上由<em>W</em>决定，改变<em>b</em>值对整体模型影响较小，所以没有对<em>b</em>进行正则化。</li><li><em>λ</em>——<strong>正则化参数</strong>，属于超参数的一种。可以设置<em>λ</em>为不同的值，在Dev set中进行验证，选择最佳的<em>λ</em>。</li></ul><p><strong>对于深度神经网络：</strong></p><script type="math/tex; mode=display">J(w^{[1]},b^{[1]},\cdots,w^{[L]},b^{[L]})=\frac{1}{m} \sum_{i=1}^{m}L(\hat{y}^{(i)},y^{(i)})+\frac{\lambda}{2m}||w^{[l]}||^{2} \\||w||_F^{2}=\sum_{i=1}^{n^{[l]}} \sum_{j=1}^{n^{[l-1]}}(w_{ij}^{[l]})^{2} ~~~~~~~~(Frobenius范数)\\dw^{[l]}=dw_{before}^{[l]}+\frac{\lambda}{m}w^{[l]}\\w^{[l]}:=w^{[l]}-\alpha \centerdot dw^{[l]}</script><p>由于加上了正则项，<em>dw</em>[<em>l</em>]有个增量，在更新<em>w</em>[<em>l</em>]的时候，会多减去这个增量，使得<em>w</em>[<em>l</em>]比没有正则项的值要小一些。因此，L2 regularization也被称做<strong>权重衰减</strong>（weight decay）。</p><script type="math/tex; mode=display">w^{[l]}:=w^{[l]}-\alpha \centerdot dw^{[l]}=w^{[l]}-\alpha \centerdot (dw_{before}^{[l]}+\frac{\lambda}{m}w^{[l]})=(1-\alpha \frac{\lambda}{m})w^{[l]}-\alpha \centerdot dw_{before}^{[l]}</script><h4 id="直观解释（Why-regularization-reduces-overfitting-）"><a href="#直观解释（Why-regularization-reduces-overfitting-）" class="headerlink" title="直观解释（Why regularization reduces overfitting?）"></a>直观解释（Why regularization reduces overfitting?）</h4><p>假如选择了非常复杂的神经网络模型，在未使用正则化的情况下出现了过拟合。但是，如果使用L2 regularization，当<em>λ</em>很大时，<em>w</em>[<em>l</em>]近似为零，意味着该神经网络模型中的某些神经元实际的作用很小，<strong>原本过于复杂的神经网络模型就被简单化了</strong>。因此，正则化方法可以减轻过拟合程度。</p><h3 id="L1正则化"><a href="#L1正则化" class="headerlink" title="L1正则化"></a>L1正则化</h3><script type="math/tex; mode=display">J(w,b)=\frac{1}{m} \sum_{i=1}^{m}L(\hat{y}^{(i)},y^{(i)})+\frac{\lambda}{2m}||w||_1 \\||w||_1=\sum_{j=1}^{n_x}|w_j|</script><p>与L2 regularization相比，L1 regularization得到的w更加稀疏，即很多w为零值。其优点是节约存储空间，因为大部分w为0。然而，实际上L1 regularization在解决high variance方面比L2 regularization并不更具优势。而且，L1的在微分求导方面比较复杂。所以，一般<strong>L2 regularization更加常用</strong>。</p><h3 id="随机失活（Dropout）正则化"><a href="#随机失活（Dropout）正则化" class="headerlink" title="随机失活（Dropout）正则化"></a>随机失活（<strong>Dropout</strong>）正则化</h3><h4 id="方法思想"><a href="#方法思想" class="headerlink" title="方法思想"></a>方法思想</h4><p>随机失活（Dropout）是指在深度学习网络的训练过程中，<strong>对于每层的神经元，按照一定的概率将其暂时从网络中丢弃</strong>。也就是说，每次训练时，每一层都有部分神经元不工作，起到简化复杂网络模型的效果，从而避免发生过拟合。</p><p>注：使用dropout训练结束后，在测试和实际应用模型时，不需要进行dropout</p><h4 id="实现方法：反向随机失活（Inverted-dropout）"><a href="#实现方法：反向随机失活（Inverted-dropout）" class="headerlink" title="实现方法：反向随机失活（Inverted dropout）"></a>实现方法：反向随机失活（Inverted dropout）</h4><p>假设对于第<em>l</em>层神经元，设定<strong>神经元保留概率keep_prob</strong>=0.8，即该层有20%的神经元停止工作。<em>dl</em>为随机失活向量，其中80%的元素为1，20%的元素为0。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dl = np.random.rand(al.shape[<span class="number">0</span>],al.shape[<span class="number">1</span>])&lt;keep_prob  //生成随机失活向量</span><br><span class="line">al = np.multiply(al,dl) //第l层经过dropout的输出</span><br><span class="line">al /= keep_prob //进行缩放（scale up）以尽可能保持al的期望值相比之前没有大的变化，测试时就不需要再对样本数据进行类似的尺度伸缩操作</span><br></pre></td></tr></table></figure><p>对于m个样本，单次迭代训练时，随机失活隐藏层一定数量的神经元；然后，在删除后的剩下的神经元上正向和反向更新参数；接着，下一次迭代中，恢复之前失活的神经元，重新随机失活一定数量的神经元，进行正向和反向更新参数；不断重复上述过程，直至迭代训练完成。</p><h3 id="其他正则化方法"><a href="#其他正则化方法" class="headerlink" title="其他正则化方法"></a>其他正则化方法</h3><h4 id="数据增强（Data-Augmentation）"><a href="#数据增强（Data-Augmentation）" class="headerlink" title="数据增强（Data Augmentation）"></a>数据增强（Data Augmentation）</h4><p>增加训练样本数量通常成本较高，难以获得额外的训练样本。但可以对已有的训练样本进行一些处理来“制造”出更多的样本。虽然这些是基于原有样本的，但是对增大训练样本数量还是有很有帮助的，不需要增加额外成本，却能起到防止过拟合的效果。</p><p>例如：</p><ul><li>在图片识别问题中，可以对已有的图片进行水平翻转、垂直翻转、任意角度旋转、缩放或扩大等等。</li></ul><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://s2.ax1x.com/2019/08/19/m8l25R.png" alt="图片识别中的数据增强" title>                </div>                <div class="image-caption">图片识别中的数据增强</div>            </figure><ul><li>在数字识别问题中，可以将原有的数字图片进行任意旋转或者扭曲，增加一些noise。</li></ul><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://s2.ax1x.com/2019/08/19/m8lBvT.png" alt="数字识别中的数据增强" title>                </div>                <div class="image-caption">数字识别中的数据增强</div>            </figure><h4 id="提前终止（Early-Stopping）"><a href="#提前终止（Early-Stopping）" class="headerlink" title="提前终止（Early Stopping）"></a>提前终止（Early Stopping）</h4><p>个神经网络模型随着迭代训练次数增加，train set error一般是单调减小的，而dev set error 先减小，之后又增大。也就是说训练次数过多时，模型会对训练样本拟合的越来越好，但是对验证集拟合效果逐渐变差，即发生了过拟合。因此，迭代训练次数不是越多越好，可以<strong>通过train set error和dev set error随迭代次数的变化趋势，选择合适的迭代次数</strong>，即early stopping。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://s2.ax1x.com/2019/08/19/m8lw80.png" alt="随迭代次数的变化趋势" title>                </div>                <div class="image-caption">随迭代次数的变化趋势</div>            </figure><p><strong>Early Stopping的缺点：</strong></p><p>机器学习训练模型有两个目标：一是优化cost function，尽量减小J；二是防止过拟合。通过减少迭代次数来防止过拟合，会使得cost function不会足够小。即将两个目标融合在一起，同时优化，但可能没有“分而治之”的效果好。</p><p><strong>Early Stopping vs L2 regularization：</strong></p><p>与Early Stopping相比，L2 regularization可以实现“分而治之”的效果，但正则化参数<em>λ</em>的选择比较复杂。对这一点来说，early stopping比较简单。<strong>总的来说，L2 regularization更加常用一些。</strong></p><h2 id="标准化输入（Normalizing-inputs）"><a href="#标准化输入（Normalizing-inputs）" class="headerlink" title="标准化输入（Normalizing inputs）"></a>标准化输入（Normalizing inputs）</h2><h3 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h3><p>标准化输入就是对训练数据集进行<strong>归一化</strong>的操作，即将原始数据<strong>减去其均值<em>μ</em>后，再除以其方差<em>σ</em>²</strong>。</p><script type="math/tex; mode=display">\mu = \frac{1}{m} \sum_{i=1}^{m} X^{(i)}\\\sigma^{2} = \frac{1}{m} \sum_{i=1}^{m}(X^{(i)})^2\\X :=  \frac{X-\mu}{\sigma^2}</script><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://s2.ax1x.com/2019/08/19/m8lsrF.png" alt="归一化过程" title>                </div>                <div class="image-caption">归一化过程</div>            </figure><p>注：由于训练集进行了标准化处理，那么对于测试集或在实际应用时，应该使用同样的\mu<em>μ</em>和<em>σ</em>²对其进行标准化处理。</p><h3 id="进行标准化的好处"><a href="#进行标准化的好处" class="headerlink" title="进行标准化的好处"></a>进行标准化的好处</h3><p>让所有输入归一到同样的尺度上，<strong>方便进行梯度下降算法时能够更快更准确地找到全局最优解</strong>。</p><ul><li>如果不进行标准化处理，x1与x2之间分布极不平衡，训练得到的w1和w2也会在数量级上差别很大。这样导致的结果是cost function与w和b的关系可能是一个非常细长的椭圆形碗。对其进行梯度下降算法时，由于w1和w2数值差异很大，只能选择很小的学习因子<em>α</em>，来避免J发生振荡。一旦<em>α</em>较大，必然发生振荡，J不再单调下降。</li><li>如果进行了标准化操作，x1与x2分布均匀，w1和w2数值差别不大，得到的cost function与w和b的关系是类似圆形碗。对其进行梯度下降算法时，<em>α</em>可以选择相对大一些，且J一般不会发生振荡，保证了J是单调下降的。</li></ul><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://s2.ax1x.com/2019/08/19/m8lWP1.png" alt="归一化的好处图示" title>                </div>                <div class="image-caption">归一化的好处图示</div>            </figure><h2 id="梯度消失和梯度爆炸（Vanishing-and-Exploding-gradients）"><a href="#梯度消失和梯度爆炸（Vanishing-and-Exploding-gradients）" class="headerlink" title="梯度消失和梯度爆炸（Vanishing and Exploding gradients）"></a>梯度消失和梯度爆炸（Vanishing and Exploding gradients）</h2><h3 id="概念-1"><a href="#概念-1" class="headerlink" title="概念"></a>概念</h3><p>在梯度函数上出现的以指数级递增或者递减的情况分别称为<strong>梯度爆炸</strong>或者<strong>梯度消失</strong>。</p><p>假定 g(z)=z,b[l]=0，对于目标输出有：</p><script type="math/tex; mode=display">\hat{y} = (W^{[L]}W^{[L-1]}\cdots W^{[2]}W^{[1]})X</script><ul><li>对于 W[l]的值大于 1 的情况，激活函数的值将以指数级递增（数值爆炸）；</li><li>对于 W[l]的值小于 1 的情况，激活函数的值将以指数级递减（数值消失）。</li></ul><p>同样，这种情况也会引起梯度呈现同样的指数型增大或减小的变化，引起每次更新的步进长度过大或者过小，这让训练过程十分困难。</p><h3 id="改善方法：对权展w进行初始化处理"><a href="#改善方法：对权展w进行初始化处理" class="headerlink" title="改善方法：对权展w进行初始化处理"></a>改善方法：对权展w进行初始化处理</h3><p>由下式</p><script type="math/tex; mode=display">z = w_1x_1+w_2x_2+\cdots + w_nx_n+b</script><p>可知，当输入的数量 n 较大时，我们希望每个wi的值都小一些，这样它们的和得到的 z 也较小。为了<strong>得到较小的wi</strong>，可进行如下初始化：</p><ul><li>若激活函数为sigmod/tanh函数，令其方差为1/n：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w[l] = np.random.randn(n[l],n[l<span class="number">-1</span>])*np.sqrt(<span class="number">1</span>/n[l<span class="number">-1</span>])</span><br></pre></td></tr></table></figure><ul><li>若激活函数为ReLU函数，令其方差为2/n：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w[l] = np.random.randn(n[l],n[l<span class="number">-1</span>])*np.sqrt(<span class="number">1</span>/n[l<span class="number">-1</span>])</span><br></pre></td></tr></table></figure><ul><li>另外还有：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w[l] = np.random.randn(n[l],n[l<span class="number">-1</span>])*np.sqrt(<span class="number">2</span>/n[l<span class="number">-1</span>]*n[l])</span><br></pre></td></tr></table></figure><h2 id="梯度检验（Gradient-Checking）"><a href="#梯度检验（Gradient-Checking）" class="headerlink" title="梯度检验（Gradient Checking）"></a>梯度检验（Gradient Checking）</h2><p>检查验证<strong>反向传播过程中梯度下降算法是否正确</strong>。</p><h3 id="梯度的数值近似"><a href="#梯度的数值近似" class="headerlink" title="梯度的数值近似"></a>梯度的数值近似</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://s2.ax1x.com/2019/08/19/m8lyb4.png" alt="梯度近似值求解" title>                </div>                <div class="image-caption">梯度近似值求解</div>            </figure><p>函数在θ出的导数近似为：</p><script type="math/tex; mode=display">g(\theta)=\frac{f(\theta+\epsilon)-f(\theta-\epsilon)}{2\epsilon}</script><p>其中，<em>ε</em>&gt;0，且足够小。</p><h3 id="梯度检查的过程"><a href="#梯度检查的过程" class="headerlink" title="梯度检查的过程"></a>梯度检查的过程</h3><ol><li><p>将<em>W</em>[1],<em>b</em>[1],⋯,<em>W</em>[<em>L</em>],<em>b</em>[<em>L</em>]这些矩阵构造成一维向量，然后将这些一维向量组合起来构成一个更大的一维向量<em>θ</em>。这样<em>J</em>(<em>W</em>[1],<em>b</em>[1],⋯,<em>W</em>[<em>L</em>],<em>b</em>[<em>L</em>])就可以表示为<em>J</em>(<em>θ</em>)。</p></li><li><p>将反向传播过程通过梯度下降算法得到的d<em>W</em>[1],d<em>b</em>[1],⋯,d<em>W</em>[<em>L</em>],d<em>b</em>[<em>L</em>]按照一样的顺序构造成一个一维向量<em>dθ</em>。<em>dθ</em>的维度与<em>θ</em>一致。</p></li><li><p>接着利用<em>J</em>(<em>θ</em>)对每个<em>θ</em>i计算近似梯度，其值与反向传播算法得到的<em>dθ</em>i相比较，检查是否一致。例如，对于第i个元素，近似梯度为：</p><script type="math/tex; mode=display">d\theta_{approx}[i]=\frac{J(\theta_1,\theta_2,\cdots,\theta_i+\epsilon,\cdots)-J(\theta_1,\theta_2,\cdots,\theta_i-\epsilon,\cdots)}{2\epsilon}</script></li><li><p>计算<em>dθ</em>approx与<em>dθ</em>的欧氏距离来比较二者的相似度。公式如下：</p></li></ol><script type="math/tex; mode=display">\frac{||d\theta_{approx}-d\theta||_2}{||d\theta_{approx}||_2+||d\theta||_2}</script><p>一般来说：</p><ul><li>如果欧氏距离很小，例如10^−7，甚至更小，则表明反向梯度计算是正确的;</li><li>如果欧氏距离较大，例如10^-5，则表明梯度计算可能出现问题，需要再次检查是否有bugs存在;</li><li>如果欧氏距离很大，例如10^-3，甚至更大，则表明梯度下降计算过程有bugs，需要仔细检查。</li></ul><h3 id="几点注意"><a href="#几点注意" class="headerlink" title="几点注意"></a>几点注意</h3><ol><li>不要在整个训练过程中都进行梯度检查，仅仅作为debug使用。</li><li>如果梯度检查出现错误，找到对应出错的梯度，检查其推导是否出现错误。</li><li>计算近似梯度的时候不能忽略正则项。</li><li>梯度检查时关闭dropout，检查完毕后再打开dropout</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;数据集：训练集-开发集-测试集（Train-Dev-Test-sets）&quot;&gt;&lt;a href=&quot;#数据集：训练集-开发集-测试集（Train-Dev-Test-sets）&quot; class=&quot;headerlink&quot; title=&quot;数据集：训练集/开发集/测试集（Trai
      
    
    </summary>
    
    
      <category term="学习笔记" scheme="http://yoursite.com/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
      <category term="deeplearning.ai" scheme="http://yoursite.com/tags/deeplearning-ai/"/>
    
  </entry>
  
  <entry>
    <title>deeplearning.ai学习笔记（四）深度神经网络（Deep Neural Network）</title>
    <link href="http://yoursite.com/2019/08/14/deeplearning-ai%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%9B%9B%EF%BC%89%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88Deep-Neural-Network%EF%BC%89/"/>
    <id>http://yoursite.com/2019/08/14/deeplearning-ai学习笔记（四）深度神经网络（Deep-Neural-Network）/</id>
    <published>2019-08-14T14:28:38.000Z</published>
    <updated>2019-08-15T11:11:41.698Z</updated>
    
    <content type="html"><![CDATA[<p><strong>深度神经网络</strong>其实就是包含更多的隐藏层神经网络。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://s2.ax1x.com/2019/08/15/mVik1s.png" alt="越来越“深”的神经网络" title>                </div>                <div class="image-caption">越来越“深”的神经网络</div>            </figure><h2 id="深度神经网络为何如此有效？（Why-deep-representations"><a href="#深度神经网络为何如此有效？（Why-deep-representations" class="headerlink" title="深度神经网络为何如此有效？（Why deep representations?)"></a>深度神经网络为何如此有效？（Why deep representations?)</h2><p>神经网络效果显著，其强大能力主要源自神经网络足够“深”，即<strong>网络层数越多，神经网络就更加复杂和深入，学习也更加准确</strong>。</p><a id="more"></a><h3 id="由“浅”到“深”的特征提取"><a href="#由“浅”到“深”的特征提取" class="headerlink" title="由“浅”到“深”的特征提取"></a>由“浅”到“深”的特征提取</h3><p>以人脸识别为例：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://s2.ax1x.com/2019/08/15/mVieBV.png" alt="人脸识别的特征提取" title>                </div>                <div class="image-caption">人脸识别的特征提取</div>            </figure><p>经过训练，神经网络第一层所做的事就是从原始图片中提取出人脸的轮廓与边缘，即边缘检测。这样每个神经元得到的是一些边缘信息。神经网络第二层所做的事情就是将前一层的边缘进行组合，组合成人脸一些局部特征，比如眼睛、鼻子、嘴巴等。再往后面，就将这些局部特征组合起来，融合成人脸的模样。</p><h3 id="深度网络能减少神经元个数"><a href="#深度网络能减少神经元个数" class="headerlink" title="深度网络能减少神经元个数"></a>深度网络能减少神经元个数</h3><p>以计算逻辑输出为例</p><script type="math/tex; mode=display">y=x_1 \oplus x_2 \oplus \cdots \oplus x_n</script><p><strong>使用深度网络：</strong></p><p>每层将前一层的两两单元进行异或，最后到一个输出。整个深度网络的层数是<em>l<strong>o</strong>g</em>2(<em>n</em>)，神经元个数为：</p><script type="math/tex; mode=display">1+2+\cdots+2^{log_2(n)-1}=n-1</script><p><strong>不使用深度网络：</strong></p><p>仅仅使用单个隐藏层，由于包含了所有的逻辑位，需要的神经元个数达到指数级别。</p><h2 id="构建深度神经网络"><a href="#构建深度神经网络" class="headerlink" title="构建深度神经网络"></a>构建深度神经网络</h2><h3 id="前向传播和反向传播流程块图"><a href="#前向传播和反向传播流程块图" class="headerlink" title="前向传播和反向传播流程块图"></a>前向传播和反向传播流程块图</h3><p><strong>对于第<em>l</em>层来说：</strong></p><p>正向传播时：</p><script type="math/tex; mode=display">输入:a^{[l-1]} \\输出:a^{[l]} \\参数:W^{[l]},b^{[l]} \\缓存:z^{[l]}</script><p>反向传播时：</p><script type="math/tex; mode=display">输入:da^{[l]} \\输出:da^{[l-1]},dW^{[l]},db^{[l]}\\参数:W^{[l]},b^{[l]}</script><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://s2.ax1x.com/2019/08/15/mViPhQ.png" alt="第l层流程块图" title>                </div>                <div class="image-caption">第l层流程块图</div>            </figure><p><strong>对于整个神经网络：</strong></p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://s2.ax1x.com/2019/08/15/mViFpj.png" alt="神经网络流程块图" title>                </div>                <div class="image-caption">神经网络流程块图</div>            </figure><h3 id="前向传播和反向传播计算表达式"><a href="#前向传播和反向传播计算表达式" class="headerlink" title="前向传播和反向传播计算表达式"></a>前向传播和反向传播计算表达式</h3><h4 id="正向传播"><a href="#正向传播" class="headerlink" title="正向传播"></a>正向传播</h4><script type="math/tex; mode=display">\begin{cases}z^{[l]}=W^{[l]}a^{[l-1]}+b^{[l]} \\a^{[l]}=g^{[l]}(z^{[l]})\end{cases}</script><p><strong>m个训练样本：</strong></p><script type="math/tex; mode=display">\begin{cases}Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]} \\A^{[l]}=g^{[l]}(Z^{[l]})\end{cases}</script><h4 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h4><script type="math/tex; mode=display">\begin{cases}dz^{[l]}=da^{[l]}*g^{[l]'}(z^{[l]}) \\dW^{[l]}=dz^{[l]}\centerdot a^{[l-1]}\\db^{[l]}=dz^{[l]}\\da^{[l-1]}=W^{[l]T}\centerdot dz^{[l]}\\\end{cases}\\进一步推导可得递推关系:dz^{[l]}=W^{[l+1]T}\centerdot dz^{[l+1]}*g^{[l]'}(z^{[l]})</script><p><strong>m个训练样本：</strong></p><script type="math/tex; mode=display">\begin{cases}dZ^{[l]}=dA^{[l]}*g^{[l]'}(Z^{[l]}) \\dW^{[l]}=\frac{1}{m}dZ^{[l]}\centerdot A^{[l-1]T}\\db^{[l]}=\frac{1}{m}np.sum(dZ^{[l]},axis=1,keepdim=True) \\dA^{[l-1]}=W^{[l]T}\centerdot dZ^{[l]}\\\end{cases}\\进一步推导可得递推关系:dZ^{[l]}=W^{[l+1]T}\centerdot dZ^{[l+1]}*g^{[l]'}(Z^{[l]})</script><p>注：这里的“*”运算符表示矩阵对应位置相乘。</p><h2 id="参数（Parameters）-vs-超参数（Hyperparameters）"><a href="#参数（Parameters）-vs-超参数（Hyperparameters）" class="headerlink" title="参数（Parameters） vs 超参数（Hyperparameters）"></a>参数（Parameters） vs 超参数（Hyperparameters）</h2><ul><li>参数（parameters）：如<em>W</em>[l]、<em>b</em>[l]</li><li>超参数（hyperparameters）：学习率<em>α</em>，迭代次数N，神经网络层数L，各层神经元个数<em>n</em>[<em>l</em>]，激活函数<em>g</em>(<em>z</em>)等。它们决定了参数<em>W</em>[<em>l</em>]和<em>b</em>[<em>l</em>]的值，因此称为超参数。</li></ul><blockquote><p>Applied deep learning is a very empirical process.</p></blockquote><p>如何设置最优的超参数是一个比较困难的、需要经验知识的问题。通常的做法是选择超参数一定范围内的值，分别代入神经网络进行训练，测试cost function随着迭代次数增加的变化，根据结果选择cost function最小时对应的超参数值。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;深度神经网络&lt;/strong&gt;其实就是包含更多的隐藏层神经网络。&lt;/p&gt;
&lt;figure class=&quot;image-bubble&quot;&gt;
                &lt;div class=&quot;img-lightbox&quot;&gt;
                    &lt;div class=&quot;overlay&quot;&gt;&lt;/div&gt;
                    &lt;img src=&quot;https://s2.ax1x.com/2019/08/15/mVik1s.png&quot; alt=&quot;越来越“深”的神经网络&quot; title&gt;
                &lt;/div&gt;
                &lt;div class=&quot;image-caption&quot;&gt;越来越“深”的神经网络&lt;/div&gt;
            &lt;/figure&gt;
&lt;h2 id=&quot;深度神经网络为何如此有效？（Why-deep-representations&quot;&gt;&lt;a href=&quot;#深度神经网络为何如此有效？（Why-deep-representations&quot; class=&quot;headerlink&quot; title=&quot;深度神经网络为何如此有效？（Why deep representations?)&quot;&gt;&lt;/a&gt;深度神经网络为何如此有效？（Why deep representations?)&lt;/h2&gt;&lt;p&gt;神经网络效果显著，其强大能力主要源自神经网络足够“深”，即&lt;strong&gt;网络层数越多，神经网络就更加复杂和深入，学习也更加准确&lt;/strong&gt;。&lt;/p&gt;
    
    </summary>
    
    
      <category term="学习笔记" scheme="http://yoursite.com/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
      <category term="deeplearning.ai" scheme="http://yoursite.com/tags/deeplearning-ai/"/>
    
  </entry>
  
  <entry>
    <title>deeplearning.ai学习笔记（三）浅层神经网络（Shallow Neural Networks）</title>
    <link href="http://yoursite.com/2019/08/12/deeplearning-ai%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%89%EF%BC%89%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <id>http://yoursite.com/2019/08/12/deeplearning-ai学习笔记（三）浅层神经网络/</id>
    <published>2019-08-12T13:44:29.000Z</published>
    <updated>2019-08-15T08:39:39.766Z</updated>
    
    <content type="html"><![CDATA[<h2 id="神经网络的表示"><a href="#神经网络的表示" class="headerlink" title="神经网络的表示"></a>神经网络的表示</h2><p>以一个<strong>单隐层神经网络</strong>为例：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://s2.ax1x.com/2019/08/14/mkzbB4.png" alt="单隐层神经网络" title>                </div>                <div class="image-caption">单隐层神经网络</div>            </figure><p><strong>结构上</strong>，从左到右，可以分成三层：<code>输入层</code>（Input layer），<code>隐藏层</code>（Hidden layer）和<code>输出层</code>（Output layer）。输入层和输出层，对应着训练样本的输入和输出。隐藏层是抽象的非线性中间层，中间这一层节点的真实值并没有所观察，这也是其被命名为隐藏层的原因。</p><a id="more"></a><p>注：单隐藏层神经网络属于为两层神经网络，<strong>输入层不计入</strong>。</p><p><strong>记法上</strong>：</p><ol><li><p>把输入矩阵X记为<em>a</em>[0]，把隐藏层输出记为<em>a</em>[1]，输出层输出记为<em>a</em>[2]（即ŷ）。</p></li><li><p>用下标表示第几个神经元（下标从1开始）。例如<em>a</em>1[1]表示隐藏层第1个神经元，<em>a</em>2[1]表示隐藏层第2个神经元。隐藏层的输出可以写成矩阵形式：</p><script type="math/tex; mode=display">a^{[1]}=\begin{bmatrix}a_1^{[1]}\\a_2^{[1]}\\a_3^{[1]}\\a_4^{[1]}\end{bmatrix}</script></li><li><p>参数</p></li></ol><ul><li>隐藏层参数：权重<em>W</em>[1]，维度是（4,3），4对应着隐藏层神经元个数，3对应着输入层x特征向量包含元素个数。常数项<em>b</em>[1]，维度是（4,1），4同样对应着隐藏层神经元个数。</li><li>输出层参数：权重<em>W</em>[2]，维度是（1,4），1对应着输出层神经元个数，4对应着输出层神经元个数。常数项<em>b</em>[2]，维度是（1,1），因为输出只有一个神经元。</li></ul><p><strong>总结：第i层的权重<em>W</em>[<em>i</em>]维度的行等于i层神经元的个数，列等于i-1层神经元的个数；第i层常数项<em>b</em>[<em>i</em>]维度的行等于i层神经元的个数，列始终为1。</strong></p><h2 id="神经网络的输出（正向传播）"><a href="#神经网络的输出（正向传播）" class="headerlink" title="神经网络的输出（正向传播）"></a>神经网络的输出（正向传播）</h2><h3 id="单个神经元的输出"><a href="#单个神经元的输出" class="headerlink" title="单个神经元的输出"></a>单个神经元的输出</h3><p>每个神经元进行两个计算：<strong>线性加权</strong>（计算z）、<strong>非线性激活</strong>（计算a）。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://s2.ax1x.com/2019/08/14/mkzHuF.png" alt="逻辑回归单元" title>                </div>                <div class="image-caption">逻辑回归单元</div>            </figure><script type="math/tex; mode=display">z=w^{T}x+b \\a=\sigma(z)</script><h3 id="单个样本的神经网络正向传播过程"><a href="#单个样本的神经网络正向传播过程" class="headerlink" title="单个样本的神经网络正向传播过程"></a>单个样本的神经网络正向传播过程</h3><p>每个节点的计算都对应着一次逻辑运算的过程，分别由计算z和a两部分组成。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://s2.ax1x.com/2019/08/14/mkzbB4.png" alt="单隐层神经网络" title>                </div>                <div class="image-caption">单隐层神经网络</div>            </figure><h4 id="非向量化计算过程"><a href="#非向量化计算过程" class="headerlink" title="非向量化计算过程"></a>非向量化计算过程</h4><p>从输入层到隐藏层：</p><script type="math/tex; mode=display">z_1^{[1]}=w_1^{[1]T}x+b_1^{[1]},a_1^{[1]}=\sigma(z_1^{[1]}) \\z_2^{[1]}=w_2^{[1]T}x+b_2^{[1]},a_2^{[1]}=\sigma(z_2^{[1]}) \\z_3^{[1]}=w_3^{[1]T}x+b_3^{[1]},a_3^{[1]}=\sigma(z_3^{[1]}) \\z_4^{[1]}=w_4^{[1]T}x+b_4^{[1]},a_4^{[1]}=\sigma(z_4^{[1]})</script><p>从隐藏层到输出层：</p><script type="math/tex; mode=display">z_1^{[2]}=w_1^{[2]T}a^{[1]}+b_1^{[2]},a_1^{[2]}=\sigma(z_1^{[2]})</script><h4 id="向量化计算过程"><a href="#向量化计算过程" class="headerlink" title="向量化计算过程"></a>向量化计算过程</h4><script type="math/tex; mode=display">z^{[1]}=W^{[1]}x+b^{[1]},a^{[1]}=\sigma(z^{[1]}) \\z^{[2]}=W^{[2]}a^{[1]}+b^{[2]},a^{[2]}=\sigma(z^{[2]})</script><h3 id="m个训练样本的神经网络正向传播过程"><a href="#m个训练样本的神经网络正向传播过程" class="headerlink" title="m个训练样本的神经网络正向传播过程"></a>m个训练样本的神经网络正向传播过程</h3><h4 id="非向量化计算过程-1"><a href="#非向量化计算过程-1" class="headerlink" title="非向量化计算过程"></a>非向量化计算过程</h4><p>for i=1 to m:</p><script type="math/tex; mode=display">z^{[1](i)}=W^{[1]}x^{(i)}+b^{[1]},a^{[1](i)}=\sigma(z^{[1](i)}) \\z^{[2](i)}=W^{[2]}a^{[1](i)}+b^{[2]},a^{[2](i)}=\sigma(z^{[2](i)})</script><h4 id="向量化计算过程-1"><a href="#向量化计算过程-1" class="headerlink" title="向量化计算过程"></a>向量化计算过程</h4><script type="math/tex; mode=display">Z^{[1]}=W^{[1]}X+b^{[1]},A^{[1]}=\sigma(Z^{[1]}) \\Z^{[2]}=W^{[2]}A^{[1]}+b^{[2]},A^{[2]}=\sigma(Z^{[2]})</script><p>其中，<em>Z</em>[1]的维度是（4,m），4是隐藏层神经元的个数；<em>A</em>[1]的维度与<em>Z</em>[1]相同；<em>Z</em>[2]和<em>A</em>[2]的维度均为（1,m）。（行表示神经元个数，列表示样本数目m）</p><h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><h3 id="几种常见激活函数"><a href="#几种常见激活函数" class="headerlink" title="几种常见激活函数"></a>几种常见激活函数</h3><h4 id="sigmoid函数"><a href="#sigmoid函数" class="headerlink" title="sigmoid函数"></a>sigmoid函数</h4><ul><li><p>表达式及图像：</p><p><img src="https://s2.ax1x.com/2019/08/14/mkzOE9.png" alt="sigmoid函数"></p></li><li><p>导数：</p><script type="math/tex; mode=display">g^{'}(z)=a(1-a)</script></li></ul><h4 id="tanh函数"><a href="#tanh函数" class="headerlink" title="tanh函数"></a>tanh函数</h4><ul><li><p>表达式及图像：</p><p><img src="https://s2.ax1x.com/2019/08/14/mkzj41.png" alt="tanh函数"></p></li><li><p>导数：</p><script type="math/tex; mode=display">g^{'}(z)=1-a^{2}</script></li></ul><h4 id="ReLU函数（线性整流函数，Rectified-Linear-Unit）"><a href="#ReLU函数（线性整流函数，Rectified-Linear-Unit）" class="headerlink" title="ReLU函数（线性整流函数，Rectified Linear Unit）"></a>ReLU函数（线性整流函数，Rectified Linear Unit）</h4><ul><li><p>表达式及图像：</p><p><img src="https://s2.ax1x.com/2019/08/14/mkzx9x.png" alt="ReLU函数"></p></li><li><p>导数：</p><script type="math/tex; mode=display">g^{'}(z)=\begin{cases}1 & ,z>0 \\0 & ,z<0 \\Undefined & ,z=0\end{cases}</script></li></ul><h4 id="Leaky-ReLU函数"><a href="#Leaky-ReLU函数" class="headerlink" title="Leaky ReLU函数"></a>Leaky ReLU函数</h4><ul><li><p>表达式及图像：</p><p><img src="https://s2.ax1x.com/2019/08/14/mkzz36.png" alt="Leaky ReLU函数"></p></li><li><p>导数：</p><script type="math/tex; mode=display">g^{'}(z)=\begin{cases}1 & ,z>0 \\0.01 & ,z<0 \\Undefined & ,z=0\end{cases}</script></li></ul><h3 id="激活函数的比较与选择"><a href="#激活函数的比较与选择" class="headerlink" title="激活函数的比较与选择"></a>激活函数的比较与选择</h3><ul><li><strong>sigmoid VS tanh：</strong>对于隐藏层的激活函数，一般来说，tanh函数要比sigmoid函数表现更好一些。特别的，对于二分类问题的输出层，由于取值为[0,1]，所以一般会选择sigmoid作为激活函数。</li><li><strong>ReLU &amp; Leaky ReLU：</strong>sigmoid函数和tanh函数，当|z|很大的时候，激活函数的斜率（梯度）很小，梯度下降算法会运行得比较慢。对于隐藏层，选择ReLU作为激活函数能够保证z大于零时梯度始终为1，从而提高神经网络梯度下降算法运算速度。但当z小于零时，存在梯度为0的缺点（实际影响不大），而Leaky ReLU激活函数能够保证z小于零是梯度不为0。</li></ul><p><strong>实际应用中，通常会会选择使用ReLU/Leaky ReLU，保证梯度下降速度不会太小。</strong>具体选择哪个函数作为激活函数没有一个固定的准确的答案，要根据具体实际问题进行验证。</p><h3 id="为什么要用非线性函数做激活函数？"><a href="#为什么要用非线性函数做激活函数？" class="headerlink" title="为什么要用非线性函数做激活函数？"></a>为什么要用<strong>非线性函数</strong>做激活函数？</h3><p>假设所有的激活函数都是线性的，为了简化计算，我们直接令激活函数<em>g</em>(<em>z</em>)=<em>z</em>，即<em>a=z</em>。那么，浅层神经网络的各层输出为：</p><script type="math/tex; mode=display">z^{[1]}=W^{[1]}x+b^{[1]},a^{[1]}=z^{[1]}\\z^{[2]}=W^{[2]}a^{[1]}+b^{[2]},a^{[2]}=z^{[2]}</script><p>可简化为：</p><script type="math/tex; mode=display">a^{[2]}=z^{[2]}=W^{[2]}a^{[1]}+b^{[2]}=W^{[2]}(W^{[1]}x+b^{[1]})+b^{[2]}\\=(W^{[2]}W^{[1]})x+(b^{[1]}+b^{[2]})=W^{'}x+b^{'}</script><p>可见输出结果仍是输入变量x的线性组合。</p><p>这表明：</p><ul><li>包含多层隐藏层的神经网络，如果使用线性函数作为激活函数，最终的输出仍然是输入x的线性模型，神经网络就没有任何作用了。因此，<strong>隐藏层的激活函数必须要是非线性的</strong>。</li><li>如果所有的隐藏层全部使用线性激活函数，只有输出层使用非线性激活函数，那么整个神经网络的结构就类似于一个简单的逻辑回归模型，而失去了神经网络模型本身的优势和价值。</li><li>如果是预测问题而不是分类问题，<strong>输出y是连续的情况下，输出层的激活函数可以使用线性函数</strong>。如果输出y恒为正值，则也可以使用ReLU激活函数。</li></ul><h2 id="神经网络的梯度下降法"><a href="#神经网络的梯度下降法" class="headerlink" title="神经网络的梯度下降法"></a>神经网络的梯度下降法</h2><h3 id="算法过程"><a href="#算法过程" class="headerlink" title="算法过程"></a>算法过程</h3><p>与逻辑回归类似，先对参数进行初始化，然后前向传播计算输出值，再反向传播更新参数，直至到达损失函数的最优值。</p><h3 id="参数的随机初始化"><a href="#参数的随机初始化" class="headerlink" title="参数的随机初始化"></a>参数的随机初始化</h3><h4 id="神经网络中的参数权重W不能全部初始化为零"><a href="#神经网络中的参数权重W不能全部初始化为零" class="headerlink" title="神经网络中的参数权重W不能全部初始化为零"></a>神经网络中的参数权重W不能全部初始化为零</h4><p>以例来说明：一个浅层神经网络包含两个输入，隐藏层包含两个神经元。如果权重<em>W</em>[1]和<em>W</em>[2]都初始化为零，即：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://s2.ax1x.com/2019/08/14/mASPDe.png" alt="浅层神经网络实例" title>                </div>                <div class="image-caption">浅层神经网络实例</div>            </figure><script type="math/tex; mode=display">W^{[1]}=\begin{bmatrix}0 & 0\\0 & 0\end{bmatrix},W^{[2]}=\begin{bmatrix}0 & 0\end{bmatrix}</script><p>这样使得隐藏层第一个神经元的输出等于第二个神经元的输出，即<em>a</em>1[1]=<em>a</em>2[1]。经过推导得到d<em>z</em>1[1]=d<em>z</em>2[1]、d<em>W</em>1[1]=d<em>W</em>2[1]。隐藏层两个神经元对应的权重每次迭代更新都会得到完全相同的结果，<em>W</em>1[1]始终等于<em>W</em>2[1]，完全对称，产生<code>Symmetry breaking problem</code>，使得隐藏层设置多个神经元就没有任何意义了。</p><p>注：参数b可以全部初始化为零，并不会影响神经网络训练效果。</p><h4 id="随机初始化方法"><a href="#随机初始化方法" class="headerlink" title="随机初始化方法"></a>随机初始化方法</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">W_1 = np.random.randn((<span class="number">2</span>,<span class="number">2</span>))*<span class="number">0.01</span></span><br><span class="line">b_1 = np.zero((<span class="number">2</span>,<span class="number">1</span>))</span><br><span class="line">W_2 = np.random.randn((<span class="number">1</span>,<span class="number">2</span>))*<span class="number">0.01</span></span><br><span class="line">b_2 = <span class="number">0</span></span><br></pre></td></tr></table></figure><p>注：乘以0.01的目的是<strong>尽量使得权重W初始化为比较小的值</strong>。如果使用sigmoid/tanh函数作为激活函数，W比较小，得到的|z|也比较小（靠近零点），梯度大，梯度下降算法的迭代速度快。（如果激活函数是ReLU/Leaky ReLU函数，则不需要考虑这个问题）</p><h3 id="神经网络的前向传播（Forward-propagation）"><a href="#神经网络的前向传播（Forward-propagation）" class="headerlink" title="神经网络的前向传播（Forward propagation）"></a>神经网络的前向传播（Forward propagation）</h3><p>即计算神经网络的输出。</p><h3 id="神经网络的反向传播（Back-propagation）"><a href="#神经网络的反向传播（Back-propagation）" class="headerlink" title="神经网络的反向传播（Back propagation）"></a>神经网络的反向传播（Back propagation）</h3><p><strong>计算公式：</strong></p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://s2.ax1x.com/2019/08/14/mASCuD.png" alt="反向传播计算公式" title>                </div>                <div class="image-caption">反向传播计算公式</div>            </figure><p><strong>推导过程：</strong></p><p>利用神经网络的计算图：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://s2.ax1x.com/2019/08/14/mASpjO.png" alt="神经网络计算图" title>                </div>                <div class="image-caption">神经网络计算图</div>            </figure><p>根据求导的链式法则反向进行逐步推导，再利用向量化推广至整个训练集。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;神经网络的表示&quot;&gt;&lt;a href=&quot;#神经网络的表示&quot; class=&quot;headerlink&quot; title=&quot;神经网络的表示&quot;&gt;&lt;/a&gt;神经网络的表示&lt;/h2&gt;&lt;p&gt;以一个&lt;strong&gt;单隐层神经网络&lt;/strong&gt;为例：&lt;/p&gt;
&lt;figure class=&quot;image-bubble&quot;&gt;
                &lt;div class=&quot;img-lightbox&quot;&gt;
                    &lt;div class=&quot;overlay&quot;&gt;&lt;/div&gt;
                    &lt;img src=&quot;https://s2.ax1x.com/2019/08/14/mkzbB4.png&quot; alt=&quot;单隐层神经网络&quot; title&gt;
                &lt;/div&gt;
                &lt;div class=&quot;image-caption&quot;&gt;单隐层神经网络&lt;/div&gt;
            &lt;/figure&gt;
&lt;p&gt;&lt;strong&gt;结构上&lt;/strong&gt;，从左到右，可以分成三层：&lt;code&gt;输入层&lt;/code&gt;（Input layer），&lt;code&gt;隐藏层&lt;/code&gt;（Hidden layer）和&lt;code&gt;输出层&lt;/code&gt;（Output layer）。输入层和输出层，对应着训练样本的输入和输出。隐藏层是抽象的非线性中间层，中间这一层节点的真实值并没有所观察，这也是其被命名为隐藏层的原因。&lt;/p&gt;
    
    </summary>
    
    
      <category term="学习笔记" scheme="http://yoursite.com/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
      <category term="deeplearning.ai" scheme="http://yoursite.com/tags/deeplearning-ai/"/>
    
  </entry>
  
  <entry>
    <title>deeplearning.ai学习笔记（二）神经网络基础——以二值分类问题的逻辑回归模型为例</title>
    <link href="http://yoursite.com/2019/08/11/deeplearning-ai%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80%E2%80%94%E2%80%94%E4%BB%A5%E4%BA%8C%E5%80%BC%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E7%9A%84%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%E4%B8%BA%E4%BE%8B/"/>
    <id>http://yoursite.com/2019/08/11/deeplearning-ai学习笔记（二）神经网络基础——以二值分类问题的逻辑回归模型为例/</id>
    <published>2019-08-11T02:18:57.000Z</published>
    <updated>2019-08-12T13:56:39.367Z</updated>
    
    <content type="html"><![CDATA[<h2 id="二值分类（Binary-Classification-）问题"><a href="#二值分类（Binary-Classification-）问题" class="headerlink" title="二值分类（Binary Classification ）问题"></a>二值分类（Binary Classification ）问题</h2><h3 id="描述"><a href="#描述" class="headerlink" title="描述"></a>描述</h3><p>例如：<strong>猫咪检测器</strong>（Cat vs Non-Cat ）</p><p>目标是训练一个分类器，对于输入的照片，如果它是一张猫咪的照片就输出1，否则输出0。</p><h3 id="输入"><a href="#输入" class="headerlink" title="输入"></a>输入</h3><p>将一张RGB三通道彩色图像展开为一个长的列向量做为输入。</p><p>若图片尺寸为64*64，则向量的维度n(x)=64*64*3=12288。</p><a id="more"></a><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://s2.ax1x.com/2019/08/11/exSOED.md.png" alt="输入特征向量x" title>                </div>                <div class="image-caption">输入特征向量x</div>            </figure><h3 id="输出"><a href="#输出" class="headerlink" title="输出"></a>输出</h3><p>对于二值分类问题，输出结果只有两个——<strong>0或1</strong>。</p><p>输出标签的向量形式：（m维列向量）</p><script type="math/tex; mode=display">\begin{bmatrix}y^{(1)}\\y^{(2)}  \\\vdots \\y^{(i)}  \end{bmatrix}</script><h3 id="训练数据"><a href="#训练数据" class="headerlink" title="训练数据"></a>训练数据</h3><ul><li>单个样本由一对（x,y）表示，其中x是一个n(x)维的特征向量 ，y是取值为0或1的标签。</li><li>训练集包含m个训练样本（用小写m代表训练集的样本总数），(x(i),y(i)) 表示第i个样本的输入和输出。</li><li>写成矩阵形式如下：（X.shape=n_x*m）</li></ul><script type="math/tex; mode=display">\begin{bmatrix}\vdots & \vdots & \vdots & \vdots \\x^{(1)} & x^{(2)} & \cdots & x^{(i)} \\\vdots & \vdots & \vdots & \vdots \end{bmatrix}</script><h2 id="逻辑回归（Logistic-Regression-）模型"><a href="#逻辑回归（Logistic-Regression-）模型" class="headerlink" title="逻辑回归（Logistic Regression ）模型"></a><strong>逻辑回归</strong>（Logistic Regression ）模型</h2><p>逻辑回归是一种用于解决输出是0/1的监督学习问题的学习算法，它使得预测值与训练数据之间的偏差最小。</p><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p>以Cat vs No - cat为例：</p><p>把一张图片展开为特征向量x做为输入，算法将估计这张图片中包含猫咪的概率。</p><script type="math/tex; mode=display">Given~x~,~\hat{y}=P(y=1|x),where~0\leq\hat{y}\leq1</script><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://s2.ax1x.com/2019/08/11/exSWEF.md.png" alt="逻辑回归模型" title>                </div>                <div class="image-caption">逻辑回归模型</div>            </figure><p>其中各个参数的意义：</p><ul><li>x：输入特征向量（一个n_x维列向量）</li><li>y：训练集标签，y∈0,1</li><li>weights——w：权重（一个n_x维列向量）</li><li>threshold——b：偏置量（一个实数）</li></ul><blockquote><p>Sigmoid 函数：</p><script type="math/tex; mode=display">\sigma(z)=\frac{1}{1+e^{-z}}</script><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://s2.ax1x.com/2019/08/11/exSog1.md.png" alt="Sigmoid函数图像" title>                </div>                <div class="image-caption">Sigmoid函数图像</div>            </figure><p>该函数的特点：</p><ul><li>当z趋于+∞，函数值趋于1</li><li>当z趋于-∞，函数值趋于0</li><li>当z=0，函数值为0.5</li></ul><p>作用：将输出值限制在[0,1]，使其可以表示一个概率值</p></blockquote><h3 id="代价函数（Cost-function-）"><a href="#代价函数（Cost-function-）" class="headerlink" title="代价函数（Cost function ）"></a>代价函数（Cost function ）</h3><h4 id="损失函数-vs-代价函数"><a href="#损失函数-vs-代价函数" class="headerlink" title="损失函数 vs 代价函数"></a>损失函数 vs 代价函数</h4><blockquote><p>损失函数（Loss function）是定义在<strong>单个样本</strong>上的，算的是一个样本的误差。</p><p>代价函数（Cost function）是定义在<strong>整个训练集</strong>上的，是所有样本误差的平均，也就是损失函数的平均。</p></blockquote><p><strong>代价函数用来衡量参数在整个模型中的作用效果。</strong></p><h4 id="逻辑回归的损失函数"><a href="#逻辑回归的损失函数" class="headerlink" title="逻辑回归的损失函数"></a>逻辑回归的损失函数</h4><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://s2.ax1x.com/2019/08/11/exSh4J.md.png" alt="损失函数" title>                </div>                <div class="image-caption">损失函数</div>            </figure><p><strong>推导过程：</strong></p><p>我们希望算法输出ŷ 表示当给定输入特征x的时候y=1的概率。换句话说，如果y等于1，那么p(y|x)就等于ŷ ；相反地，当y=0时，p(y|x)就等于1-ŷ 。因此，如果ŷ表示当y=1的概率，那么1-ŷ就表示y=0的概率。</p><p>可以定义p(y|x)如下：</p><p>由于y只有0和1两种取值，因此上面的两个方程可以归纳为如下一个方程：</p><script type="math/tex; mode=display">p(y|x)=ŷ^y*(1-ŷ)^{1-y}</script><p>因为对数函数是一个绝对的单调递增函数，最大化log(p(y|x))会得出和最大化p(y|x)相似的结果，因此可以取对数，简化公式。</p><script type="math/tex; mode=display">log(ŷ^y*(1-ŷ)^{1-y}) =ylog(ŷ)+(1-y)log(1-ŷ)</script><p>又因为通常在训练一个学习算法的时候，我们想要让概率变大。而在逻辑回归中，我们想要最小化L(ŷ,y)这个损失函数。最小化损失函数相当于最大化概率的对数，所以需要加一个负号。</p><script type="math/tex; mode=display">L(ŷ,y)=-(ylog(ŷ)+(1-y)log(1-ŷ))</script><h4 id="逻辑回归的代价函数"><a href="#逻辑回归的代价函数" class="headerlink" title="逻辑回归的代价函数"></a>逻辑回归的代价函数</h4><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://s2.ax1x.com/2019/08/11/exSI3R.md.png" alt="代价函数" title>                </div>                <div class="image-caption">代价函数</div>            </figure><p><strong>推导过程：</strong></p><p>假设取出的训练样本相互独立，或者说服从独立同分布 (I.I.D: Independent and Identically Distributed) ，这些样本的概率就是各项概率的乘积。</p><p>给定X(i)从i=1到m时p(y(i))的乘积：</p><script type="math/tex; mode=display">p(y^{(1)})*p(y^{(2)})*\cdots*p(y^{(m)})</script><p>最大化这个式子本身和最大化它的对数效果相同，所以取对数：</p><script type="math/tex; mode=display">log(p(y^{(1)}))+log(p(y^{(2)}))+\cdots+log(p(y^{(m)}))=\sum_{i=1}^{m}log(p(y^{(i)}))=-\sum_{i=1}^{m}L(ŷ^{(i)},y^{(i)})</script><p>根据最大似然估计原理，选择能使该式最大化的参数。因为要最小化代价函数，而不是最大化似然值，所以要去掉这个负号。最后为了方便起见，使这些数值处于更好的尺度上，在前面添加一个缩放系数1/m。</p><p>综上，代价函数为：</p><script type="math/tex; mode=display">J(w,b)=\frac{1}{m}\sum_{i=1}^{m}L(ŷ^{(i)},y^{(i)})=-\frac{1}{m}\sum_{i=1}^{m}[y^{(i)}log(ŷ^{(i)})+(1-y^{(i)})log(1-ŷ^{(i)})]</script><p><strong>优化逻辑回归模型时，我们试着去找参数w和b，以此来缩小代价函数J， 逻辑回归可被视为一个非常小的神经网络 。</strong></p><h2 id="训练过程——梯度下降（Gradient-Descent）算法"><a href="#训练过程——梯度下降（Gradient-Descent）算法" class="headerlink" title="训练过程——梯度下降（Gradient Descent）算法"></a>训练过程——<strong>梯度下降</strong>（Gradient Descent）算法</h2><h3 id="算法思想"><a href="#算法思想" class="headerlink" title="算法思想"></a>算法思想</h3><p>在逻辑回归问题中，代价函数J是一个凸函数(convex function) 。为了去找到优的参数值，首先用一些初始值（0或随机）来初始化w和b，因为函数是凸函数，无论在哪里初始化，应该达到同一点或大致相同的点。</p><p>梯度下降法以初始点开始，然后朝最陡的下坡方向走一步，这是梯度下降的一次迭代。经过多次迭代收敛到全局最优值或接近全局最优值。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://s2.ax1x.com/2019/08/11/exS5C9.md.png" alt="梯度下降示意图" title>                </div>                <div class="image-caption">梯度下降示意图</div>            </figure><p>参数的迭代公式：</p><script type="math/tex; mode=display">w:=w-\alpha\frac{\partial{J(w,b)}}{\partial{w}}</script><script type="math/tex; mode=display">b:=b-\alpha\frac{\partial{J(w,b)}}{\partial{b}}</script><h3 id="数学基础：计算图（Computation-Graph）"><a href="#数学基础：计算图（Computation-Graph）" class="headerlink" title="数学基础：计算图（Computation Graph）"></a>数学基础：计算图（Computation Graph）</h3><p>以下式为例</p><script type="math/tex; mode=display">J(a,b,c)=3(a+bc)</script><p>其计算图为：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://s2.ax1x.com/2019/08/11/exSHu6.md.png" alt="计算图" title>                </div>                <div class="image-caption">计算图</div>            </figure><h4 id="计算图计算（前向传播过程）"><a href="#计算图计算（前向传播过程）" class="headerlink" title="计算图计算（前向传播过程）"></a>计算图计算（前向传播过程）</h4><p>前向传播即为计算一个样本下J的值，计算过程如下：</p><script type="math/tex; mode=display">u=b*c=3*2=6 \\v=a+u=5+6=11 \\J=3V=33</script><h4 id="计算图求导（反向传播过程）"><a href="#计算图求导（反向传播过程）" class="headerlink" title="计算图求导（反向传播过程）"></a>计算图求导（反向传播过程）</h4><p>反向传播即根据<strong>求导的链式法则</strong>，求解最终输出变量J对各个变量的导数。</p><p>计算过程如下：</p><script type="math/tex; mode=display">\frac{dJ}{dv}=\frac{d(3v)}{dv}=3 \\\frac{dJ}{da}=\frac{dJ}{dv}*\frac{dv}{da}=3*\frac{d(a+u)}{da}=3 \\\frac{dJ}{du}=\frac{dJ}{dv}*\frac{dv}{du}=3*\frac{d(a+u)}{du}=3 \\\frac{dJ}{db}=\frac{dJ}{du}*\frac{du}{db}=3*\frac{d(bc)}{db}=3c=3*2=6 \\\frac{dJ}{dc}=\frac{dJ}{du}*\frac{du}{dc}=3*\frac{d(bc)}{db}=3b=3*3=9</script><p>为简化表示，将dJ/dval简记为dval，则在本例中：</p><script type="math/tex; mode=display">da=3,db=6,dc=9</script><h3 id="逻辑回归模型的前向传播和反向传播过程（单个样本）"><a href="#逻辑回归模型的前向传播和反向传播过程（单个样本）" class="headerlink" title="逻辑回归模型的前向传播和反向传播过程（单个样本）"></a>逻辑回归模型的前向传播和反向传播过程（单个样本）</h3><h4 id="逻辑回归模型的计算图"><a href="#逻辑回归模型的计算图" class="headerlink" title="逻辑回归模型的计算图"></a>逻辑回归模型的计算图</h4><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://s2.ax1x.com/2019/08/11/exSbDK.md.png" alt="逻辑回归模型的计算图" title>                </div>                <div class="image-caption">逻辑回归模型的计算图</div>            </figure><h4 id="逻辑回归模型的前向传播"><a href="#逻辑回归模型的前向传播" class="headerlink" title="逻辑回归模型的前向传播"></a>逻辑回归模型的前向传播</h4><p>顺序计算即可。</p><h4 id="逻辑回归模型的反向传播"><a href="#逻辑回归模型的反向传播" class="headerlink" title="逻辑回归模型的反向传播"></a>逻辑回归模型的反向传播</h4><p>计算过程如下：</p><script type="math/tex; mode=display">“da”=\frac{dL}{da}=-\frac{y}{a}+\frac{1-y}{1-a} \\“dz”=\frac{dL}{dz}=\frac{dL}{da}*\frac{da}{dz}=(-\frac{y}{a}+\frac{1-y}{1-a})*\frac{d\sigma(z)}{dz} = a(1-a) \\“dw_1”=\frac{dL}{dw_1}=\frac{dL}{dz}*\frac{dz}{dw_1}=x_1*“dz” \\“dw_2”=\frac{dL}{dw_2}=\frac{dL}{dz}*\frac{dz}{dw_2}=x_2*“dz” \\“db”=\frac{dL}{db}=\frac{dL}{dz}*\frac{dz}{db}=“dz”</script><h3 id="非向量化的逻辑回归训练过程"><a href="#非向量化的逻辑回归训练过程" class="headerlink" title="非向量化的逻辑回归训练过程"></a>非向量化的逻辑回归训练过程</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://s2.ax1x.com/2019/08/11/exSqHO.md.png" alt="非向量化的逻辑回归训练过程" title>                </div>                <div class="image-caption">非向量化的逻辑回归训练过程</div>            </figure><ul><li>缺点：显式地使用了两层for循环，时间效率低</li><li>解决方法：采用向量化的方法，可以极大地提高时间效率</li></ul><h3 id="向量化（Vectorizing）的逻辑回归训练过程"><a href="#向量化（Vectorizing）的逻辑回归训练过程" class="headerlink" title="向量化（Vectorizing）的逻辑回归训练过程"></a>向量化（Vectorizing）的逻辑回归训练过程</h3><script type="math/tex; mode=display">Z = W^{T}*X+b = np.dot(W.T,X)+b //此处存在python的广播机制 \\A = σ(Z) \\dZ = A - Y \\dW = \frac{1}{m}XdZ^{T}\\db = \frac{1}{m}*np.sum(dZ)\\W := W - \alpha dW\\b := b - \alpha db</script><p>注意：此为一次迭代的过程，多次迭代使用显示循环不可避免。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;二值分类（Binary-Classification-）问题&quot;&gt;&lt;a href=&quot;#二值分类（Binary-Classification-）问题&quot; class=&quot;headerlink&quot; title=&quot;二值分类（Binary Classification ）问题&quot;&gt;&lt;/a&gt;二值分类（Binary Classification ）问题&lt;/h2&gt;&lt;h3 id=&quot;描述&quot;&gt;&lt;a href=&quot;#描述&quot; class=&quot;headerlink&quot; title=&quot;描述&quot;&gt;&lt;/a&gt;描述&lt;/h3&gt;&lt;p&gt;例如：&lt;strong&gt;猫咪检测器&lt;/strong&gt;（Cat vs Non-Cat ）&lt;/p&gt;
&lt;p&gt;目标是训练一个分类器，对于输入的照片，如果它是一张猫咪的照片就输出1，否则输出0。&lt;/p&gt;
&lt;h3 id=&quot;输入&quot;&gt;&lt;a href=&quot;#输入&quot; class=&quot;headerlink&quot; title=&quot;输入&quot;&gt;&lt;/a&gt;输入&lt;/h3&gt;&lt;p&gt;将一张RGB三通道彩色图像展开为一个长的列向量做为输入。&lt;/p&gt;
&lt;p&gt;若图片尺寸为64*64，则向量的维度n(x)=64*64*3=12288。&lt;/p&gt;
    
    </summary>
    
    
      <category term="学习笔记" scheme="http://yoursite.com/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
      <category term="deeplearning.ai" scheme="http://yoursite.com/tags/deeplearning-ai/"/>
    
  </entry>
  
  <entry>
    <title>那些既熟悉又陌生的C/C++知识点（长期更新）</title>
    <link href="http://yoursite.com/2019/08/09/%E9%82%A3%E4%BA%9B%E6%97%A2%E7%86%9F%E6%82%89%E5%8F%88%E9%99%8C%E7%94%9F%E7%9A%84C-C-%E7%9F%A5%E8%AF%86%E7%82%B9%EF%BC%88%E9%95%BF%E6%9C%9F%E6%9B%B4%E6%96%B0%EF%BC%89/"/>
    <id>http://yoursite.com/2019/08/09/那些既熟悉又陌生的C-C-知识点（长期更新）/</id>
    <published>2019-08-09T15:21:44.000Z</published>
    <updated>2019-08-09T16:04:41.156Z</updated>
    
    <content type="html"><![CDATA[<h2 id="头文件的等价写法"><a href="#头文件的等价写法" class="headerlink" title="头文件的等价写法"></a>头文件的等价写法</h2><p>在C++标准中，<strong>stdio.h</strong>更推荐使用等价写法：<strong>cstdio</strong>，也就是在前面加一个<code>c</code>，然后去掉<code>.h</code>即可。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* 以下两种写法等价 */</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;cstdio&gt;</span></span></span><br></pre></td></tr></table></figure><h2 id="整型变量类型表示数的范围"><a href="#整型变量类型表示数的范围" class="headerlink" title="整型变量类型表示数的范围"></a>整型变量类型表示数的范围</h2><ul><li>整型<code>int</code>：<strong>32位</strong>整数/<strong>绝对值在10^9范围以内</strong>的整数都可以定义为int型</li><li>长整型<code>long long</code>：<strong>64位</strong>整数/<strong>10^18以内（如10^10）</strong>的整数就要定义为long long型</li></ul><h2 id="long-long型的使用"><a href="#long-long型的使用" class="headerlink" title="long long型的使用"></a>long long型的使用</h2><p>long long型赋<strong>大于2^31-1</strong>的初值，需要在初值后面加上LL</p><p>经常<strong>利用typedef用LL来代替long long</strong>，以避免在程序中大量出现long long而降低编码的效率。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;cstdio&gt;</span></span></span><br><span class="line"><span class="keyword">typedef</span> <span class="keyword">long</span> <span class="keyword">long</span> LL; <span class="comment">//给long long起个别名LL</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    LL a = <span class="number">123456789012345L</span>L, b = <span class="number">234567890123456L</span>L;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"%lld\n"</span>, a + b);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="浮点数的存储类型"><a href="#浮点数的存储类型" class="headerlink" title="浮点数的存储类型"></a>浮点数的存储类型</h2><p>对于浮点型，<strong>不要使用float，碰到浮点型的数据都应该用double来存储</strong>。</p><ul><li>单精度<code>float</code>：有效精度只有<strong>6~7位</strong></li><li>双精度<code>double</code>：有效精度有<strong>15~16位</strong></li></ul><h2 id="double型的输入输出格式"><a href="#double型的输入输出格式" class="headerlink" title="double型的输入输出格式"></a>double型的输入输出格式</h2><ul><li>输出格式：<code>%f</code>（与float型相同）</li><li>输入格式：scanf中是<code>%lf</code></li></ul><p>注：有些系统如果把输出格式写成%lf也不会出错，但尽量还是按标准来</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;头文件的等价写法&quot;&gt;&lt;a href=&quot;#头文件的等价写法&quot; class=&quot;headerlink&quot; title=&quot;头文件的等价写法&quot;&gt;&lt;/a&gt;头文件的等价写法&lt;/h2&gt;&lt;p&gt;在C++标准中，&lt;strong&gt;stdio.h&lt;/strong&gt;更推荐使用等价写法：&lt;str
      
    
    </summary>
    
    
      <category term="C/C++" scheme="http://yoursite.com/tags/C-C/"/>
    
  </entry>
  
  <entry>
    <title>deeplearning.ai学习笔记（一）深度学习引言</title>
    <link href="http://yoursite.com/2019/08/09/deeplearning-ai%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BC%95%E8%A8%80/"/>
    <id>http://yoursite.com/2019/08/09/deeplearning-ai学习笔记（一）深度学习引言/</id>
    <published>2019-08-09T06:30:38.000Z</published>
    <updated>2019-08-09T09:05:27.062Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>AI is the new Electricity. ——吴恩达（<a href="https://www.coursera.org/instructor/andrewng" target="_blank" rel="noopener">Andrew Ng</a>）</p></blockquote><p>大约在一百年前，社会的电气化改变了每个主要行业。而如今我们见到了 AI令人惊讶的能量会产生同样巨大的转变。显然 AI的各个分支中，发展最为迅速的就是<strong>深度学习</strong>（deep learning）。而<strong>深度学习</strong>，一般指的是训练<strong>神经网络</strong>（有时是非常非常大/深的神经网络）。</p><a id="more"></a><h2 id="何为神经网络（What-is-a-neural-network-）"><a href="#何为神经网络（What-is-a-neural-network-）" class="headerlink" title="何为神经网络（What is a neural network?）"></a>何为神经网络（What is a <strong>neural network</strong>?）</h2><h3 id="引例：房价预测（Housing-Price-Prediction）"><a href="#引例：房价预测（Housing-Price-Prediction）" class="headerlink" title="引例：房价预测（Housing Price Prediction）"></a>引例：房价预测（Housing Price Prediction）</h3><p>如下图，对于单个因素（如房屋面积）的房屋预测，可以用ReLU函数进行拟合，相当于一个只有一个神经元的神经网络。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://s2.ax1x.com/2019/08/09/ebRgtx.md.png" alt="ReLU函数拟合" title>                </div>                <div class="image-caption">ReLU函数拟合</div>            </figure><p>用房子的大小 x 作为神经网络的输入，它进入到这个节点(这个小圈)中 ，然后这个小圈就输出了房价 y 。所以这个小圈，也就是一个神经网络中的一个神经元，就会执行上图中画出的这个方程。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://s2.ax1x.com/2019/08/09/ebfBw9.png" alt="单个神经元的神经网络" title>                </div>                <div class="image-caption">单个神经元的神经网络</div>            </figure><p><strong>一个很大的神经网络是由许多这样的单一神经元叠加在一起组成。</strong>比如下面这个例子，我们不仅仅根据房屋大小来预测房屋价格，我们引入其他特征量。能够容纳的家庭人口也会影响房屋价格 ，这个因素其实是取决于房屋大小以及卧室的数量这两个因素决定了这个房子是否能够容纳你的家庭 。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://s2.ax1x.com/2019/08/09/ebfvwj.md.png" alt="多因素房价预测" title>                </div>                <div class="image-caption">多因素房价预测</div>            </figure><p>所以在这个例子中：</p><ul><li>x 表示所有这四个输入 (房屋大小、卧室数量、邮政编码、富裕程度)</li><li>y表示试图去预测的价格</li><li>每一个小圆圈是ReLU 函数或者别的一些非线性函数</li></ul><p>这就是最基本的神经网络。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://s2.ax1x.com/2019/08/09/ebftzT.md.png" alt="简单神经网络" title>                </div>                <div class="image-caption">简单神经网络</div>            </figure><h2 id="深度学习：监督学习的一种（Supervised-Learning-with-Neural-Networks）"><a href="#深度学习：监督学习的一种（Supervised-Learning-with-Neural-Networks）" class="headerlink" title="深度学习：监督学习的一种（Supervised Learning with Neural Networks）"></a>深度学习：监督学习的一种（Supervised Learning with Neural Networks）</h2><h3 id="监督学习与非监督学习"><a href="#监督学习与非监督学习" class="headerlink" title="监督学习与非监督学习"></a>监督学习与非监督学习</h3><p><code>监督学习</code>：给定标记好的训练样本集，如回归问题、分类问题</p><p><code>非监督学习</code>：给定样本集，发现特征数据中的分布结构，如聚类问题</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://s2.ax1x.com/2019/08/09/ebfayF.md.png" alt="监督学习" title>                </div>                <div class="image-caption">监督学习</div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://s2.ax1x.com/2019/08/09/ebfdL4.md.png" alt="非监督学习" title>                </div>                <div class="image-caption">非监督学习</div>            </figure><h3 id="深度学习在监督学习中的应用-amp-常用神经网络"><a href="#深度学习在监督学习中的应用-amp-常用神经网络" class="headerlink" title="深度学习在监督学习中的应用 &amp; 常用神经网络"></a>深度学习在监督学习中的应用 &amp; 常用神经网络</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://s2.ax1x.com/2019/08/09/ebfDoR.md.png" alt="深度学习的应用" title>                </div>                <div class="image-caption">深度学习的应用</div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://s2.ax1x.com/2019/08/09/ebfTYt.md.png" alt="几种常见神经网络" title>                </div>                <div class="image-caption">几种常见神经网络</div>            </figure><h3 id="结构化数据与非结构化数据"><a href="#结构化数据与非结构化数据" class="headerlink" title="结构化数据与非结构化数据"></a>结构化数据与非结构化数据</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://s2.ax1x.com/2019/08/09/ebfoFI.md.png" alt="结构化数据与非结构化数据示例" title>                </div>                <div class="image-caption">结构化数据与非结构化数据示例</div>            </figure><h2 id="深度学习兴起的原因（Why-is-Deep-Learning-taking-off-）"><a href="#深度学习兴起的原因（Why-is-Deep-Learning-taking-off-）" class="headerlink" title="深度学习兴起的原因（Why is Deep Learning taking off?）"></a>深度学习兴起的原因（Why is Deep Learning taking off?）</h2><blockquote><p>Scale drives deep learning progress.——吴恩达（<a href="https://www.coursera.org/instructor/andrewng" target="_blank" rel="noopener">Andrew Ng</a>）</p></blockquote><h3 id="深度学习的性能"><a href="#深度学习的性能" class="headerlink" title="深度学习的性能"></a>深度学习的性能</h3><ul><li>x-axis is the amount of data （数据量）</li><li>y-axis (vertical axis) is the performance of the algorithm. （算法性能）</li></ul><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/JP--G3ooEeeJIwrF5BVsIg_b60d752c05bec0881d8ca08cfc2646d2_Screen-Shot-2017-08-05-at-2.30.09-PM.png?expiry=1565481600000&hmac=I-FnXoQoONjGN17_lNuk7vXj7BK4h_mP1txu2K-Y830" alt="数据量与算法性能关系" title>                </div>                <div class="image-caption">数据量与算法性能关系</div>            </figure><ul><li>当数据量比较少时，深度学习方法性能不一定优于经典机器学习方法。</li><li>通过增加数据量和神经网络规模可提升深度学习方法的性能。</li></ul><h3 id="深度学习兴起的原因"><a href="#深度学习兴起的原因" class="headerlink" title="深度学习兴起的原因"></a>深度学习兴起的原因</h3><ul><li>Data：信息化社会产生了巨大的数据量</li><li>Computation：现代计算机计算性能的极大提升（GPU与CPU）</li><li>Algorithms：算法的优化进一步提升了性能（如ReLU的使用）</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;AI is the new Electricity. ——吴恩达（&lt;a href=&quot;https://www.coursera.org/instructor/andrewng&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Andrew Ng&lt;/a&gt;）&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;大约在一百年前，社会的电气化改变了每个主要行业。而如今我们见到了 AI令人惊讶的能量会产生同样巨大的转变。显然 AI的各个分支中，发展最为迅速的就是&lt;strong&gt;深度学习&lt;/strong&gt;（deep learning）。而&lt;strong&gt;深度学习&lt;/strong&gt;，一般指的是训练&lt;strong&gt;神经网络&lt;/strong&gt;（有时是非常非常大/深的神经网络）。&lt;/p&gt;
    
    </summary>
    
    
      <category term="学习笔记" scheme="http://yoursite.com/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
      <category term="deeplearning.ai" scheme="http://yoursite.com/tags/deeplearning-ai/"/>
    
  </entry>
  
</feed>
