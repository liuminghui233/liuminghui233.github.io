{"meta":{"title":"一只程序喵","subtitle":"刘明辉的个人博客","description":null,"author":"刘明辉","url":"http://yoursite.com","root":"/"},"pages":[{"title":"","date":"2019-08-08T07:34:55.547Z","updated":"2019-08-08T07:34:55.546Z","comments":false,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"","date":"2019-08-08T07:34:30.115Z","updated":"2019-08-08T07:34:30.115Z","comments":false,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""},{"title":"custom","date":"2019-08-08T07:46:19.000Z","updated":"2019-08-08T07:46:19.813Z","comments":true,"path":"custom/index.html","permalink":"http://yoursite.com/custom/index.html","excerpt":"","text":""},{"title":"About","date":"2019-08-08T12:10:35.000Z","updated":"2019-08-14T16:22:23.398Z","comments":true,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":"教育经历2017.9-至今：厦门大学信息学院（计算机科学与技术专业） 2014.9-2017.6：山东省实验中学"}],"posts":[{"title":"deeplearning.ai学习笔记（六）优化深度神经网络之优化算法（Optimization algorithms）","slug":"deeplearning-ai学习笔记（六）优化深度神经网络之优化算法（Optimization-algorithms）","date":"2019-08-18T08:47:56.000Z","updated":"2019-08-18T15:31:12.045Z","comments":true,"path":"2019/08/18/deeplearning-ai学习笔记（六）优化深度神经网络之优化算法（Optimization-algorithms）/","link":"","permalink":"http://yoursite.com/2019/08/18/deeplearning-ai学习笔记（六）优化深度神经网络之优化算法（Optimization-algorithms）/","excerpt":"","text":"本节课学习深度神经网络中的一些优化算法，通过使用这些技巧和方法来提高神经网络的训练速度和精度。 小批量梯度下降算法（mini-batch gradient descent）算法思想如果样本数量m很大，如达到百万数量级，由于受到矩阵运算速度的限制，训练速度往往会很慢。因此，把m个训练样本分成若干个子集（mini-batches），然后每次在单一子集上进行神经网络训练。 例：假设总的训练样本个数m=5000000，其维度为(n_x,m)。将其分成5000个子集，每个mini-batch含有1000个样本。我们将每个mini-batch记为X^{t}，其维度为(n_x,1000)。相应的每个mini-batch的输出记为Y^{t}，其维度为(1,1000)，且t=1,2,⋯,5000。 算法过程先将总的训练样本分成T个子集（mini-batches），然后对每个mini-batch进行神经网络训练，包括Forward Propagation，Compute Cost Function，Backward Propagation，循环至T个mini-batch都训练完毕。 \\begin{align} & for~~t=1,\\cdots,T\\\\ &~~~~Forward~Propagation\\\\ &~~~~Compute~Cost~Function\\\\ &~~~~Backward~Propagation\\\\ &~~~~W^{\\{ t \\}}:=W^{\\{ t \\}} - \\alpha \\centerdot dW^{\\{ t \\}}\\\\ &~~~~b^{\\{ t \\}}:=b^{\\{ t \\}} - \\alpha \\centerdot db^{\\{ t \\}}\\\\ \\end{align}经过T次循环之后，所有m个训练样本都进行了梯度下降计算。这个过程称为一个epoch，一个epoch会进行T次梯度下降算法。 注：对于Mini-Batches Gradient Descent，可以进行多次epoch训练；每次epoch最好将总体训练数据重新打乱、重新分成T组mini-batches。 cost曲线： cost曲线比较 出现细微振荡的原因是不同的mini-batch之间是有差异的，但整体的趋势是下降的，最终也能得到较低的cost值。 超参数 mini-batch size 选取考虑两种极端情况： 如果mini-batch size=m，即为Batch gradient descent，只包含一个子集为(X^{1},Y^{1})=(X,Y)。会比较平稳地接近全局最小值，但是因为使用了所有m个样本，每次前进的速度有些慢。 如果mini-batch size=1，即为随机梯度下降（Stachastic gradient descent），每个样本就是一个子集(X^{i},Y^{i})=(x^(i),y^(i))，共有m个子集。每次前进速度很快，但是路线曲折，有较大的振荡，最终会在最小值附近来回波动，难以真正达到最小值处。而且在数值处理上就不能使用向量化的方法来提高运算速度。 两种极端情况下梯度下降图示 正确选取原则： 一般来说，如果总体样本数量m不太大时（如m≤2000），建议直接使用Batch gradient descent。 如果总体样本数量m很大时，建议将样本分成许多mini-batches。mini-batch size不能设置得太大，也不能太小。推荐选取2的幂做为mini-batch size的值（计算机存储数据一般是2的幂，这样设置可以提高运算速度），常用的有64、128、256、512。 1566141548914 动量梯度下降算法（Gradient descent with momentum）数学基础：指数加权平均（exponentially weighted averages）算法思想均方根传递（RMSprop, Root Mean Square prop）Adam优化算法学习率衰减（Learning rate decay）局部最优(Local Optima)问题","categories":[],"tags":[{"name":"学习笔记","slug":"学习笔记","permalink":"http://yoursite.com/tags/学习笔记/"},{"name":"deeplearning.ai","slug":"deeplearning-ai","permalink":"http://yoursite.com/tags/deeplearning-ai/"}]},{"title":"deeplearning.ai学习笔记（五）优化深度神经网络之应用层面的深度学习","slug":"deeplearning-ai学习笔记（五）优化深度神经网络之应用层面的深度学习","date":"2019-08-16T12:03:14.000Z","updated":"2019-08-17T17:21:07.833Z","comments":true,"path":"2019/08/16/deeplearning-ai学习笔记（五）优化深度神经网络之应用层面的深度学习/","link":"","permalink":"http://yoursite.com/2019/08/16/deeplearning-ai学习笔记（五）优化深度神经网络之应用层面的深度学习/","excerpt":"","text":"数据集：训练集/开发集/测试集（Train/Dev/Test sets）为实现交叉验证（cross validation），数据集一般会划分为三个部分： 训练集（Train sets）：用于训练算法模型； 开发集（Dev sets）：用于验证不同算法模型的表现情况，从中选择最好的算法模型； 测试集（Test sets）：用于测试最好算法的实际表现（算法的无偏估计）。 注：Test sets的目标主要是进行无偏估计。如果不需要无偏估计，也可以没有Test sets，可以通过Train sets训练不同的算法模型，然后分别在Dev sets上进行验证，根据结果选择最好的算法模型。（如果只有Train sets和Dev sets，通常把这里的Dev sets称为Test sets） 比例分配： 样本数量不是很大（如100、1000、10000）：Train sets和Test sets的数量比例为70%/30%；如果有Dev sets，则设置比例为60%/20%/20%。 样本数量很大（如100万）：Dev sets和Test sets大到足以完成其目标即可，对于100万的样本，往往也只需要10000个样本就够了。因此，对于大数据样本，Train/Dev/Test sets的比例可设置为98%/1%/1%或99%/0.5%/0.5%。样本数据量越大，相应的Dev/Test sets的比例可以设置的越低一些。 训练样本和测试样本分布不匹配问题： 训练样本和验证/测试样本可能来自不同的分布。一条经验原则是尽量保证Dev sets和Test sets来自于同一分布。 偏差（Bias）与方差（Variance）偏差、方差与算法的优劣 偏差方差的各种情况 高偏差（欠拟合，underfitting）：算法模型在训练样本和测试样本上的表现相差不大，但都不太好。（如：Train set error为15%，而Dev set error为16%） 高方差（过拟合，overfitting）：算法模型在训练样本上的表现很好，但是在测试样本上的表现却不太好。这说明了该模型泛化能力不强。（如：Train set error为1%，而Dev set error为11%） 低偏差&amp;低方差：最好情况的算法。 高偏差&amp;高方差：可以理解成某段区域是欠拟合的，某段区域是过拟合的，是最差情况的算法。 总结：一般来说，Train set error体现了是否出现high bias；Dev set error与Train set error的相对差值体现了是否出现high variance。 机器学习中算法评价的基本原则 算法评价流程 高偏差和高方差的解决策略对于高偏差： 增加神经网络的隐藏层个数、神经元个数 延长训练时间 选择其它更复杂的神经网络模型 …… 对于高方差： 增加训练样本数据 进行正则化（Regularization） 选择其他更复杂的神经网络模型 …… 正则化（Regularization）L2正则化具体实现对于Logistic regression： J(w,b)=\\frac{1}{m} \\sum_{i=1}^{m}L(\\hat{y}^{(i)},y^{(i)})+\\frac{\\lambda}{2m}||w||_2^{2} \\\\ ||w||_2^{2}=\\sum_{j=1}^{n_x}w_j^{2}=w^{T}w注： 由于W的维度很大，而b只是一个常数，参数很大程度上由W决定，改变b值对整体模型影响较小，所以没有对b进行正则化。 λ——正则化参数，属于超参数的一种。可以设置λ为不同的值，在Dev set中进行验证，选择最佳的λ。 对于深度神经网络： J(w^{[1]},b^{[1]},\\cdots,w^{[L]},b^{[L]})=\\frac{1}{m} \\sum_{i=1}^{m}L(\\hat{y}^{(i)},y^{(i)})+\\frac{\\lambda}{2m}||w^{[l]}||^{2} \\\\ ||w||_F^{2}=\\sum_{i=1}^{n^{[l]}} \\sum_{j=1}^{n^{[l-1]}}(w_{ij}^{[l]})^{2} ~~~~~~~~(Frobenius范数)\\\\ dw^{[l]}=dw_{before}^{[l]}+\\frac{\\lambda}{m}w^{[l]}\\\\ w^{[l]}:=w^{[l]}-\\alpha \\centerdot dw^{[l]}由于加上了正则项，dw[l]有个增量，在更新w[l]的时候，会多减去这个增量，使得w[l]比没有正则项的值要小一些。因此，L2 regularization也被称做权重衰减（weight decay）。 w^{[l]}:=w^{[l]}-\\alpha \\centerdot dw^{[l]}=w^{[l]}-\\alpha \\centerdot (dw_{before}^{[l]}+\\frac{\\lambda}{m}w^{[l]})=(1-\\alpha \\frac{\\lambda}{m})w^{[l]}-\\alpha \\centerdot dw_{before}^{[l]}直观解释（Why regularization reduces overfitting?）假如选择了非常复杂的神经网络模型，在未使用正则化的情况下出现了过拟合。但是，如果使用L2 regularization，当λ很大时，w[l]近似为零，意味着该神经网络模型中的某些神经元实际的作用很小，原本过于复杂的神经网络模型就被简单化了。因此，正则化方法可以减轻过拟合程度。 L1正则化 J(w,b)=\\frac{1}{m} \\sum_{i=1}^{m}L(\\hat{y}^{(i)},y^{(i)})+\\frac{\\lambda}{2m}||w||_1 \\\\ ||w||_1=\\sum_{j=1}^{n_x}|w_j|与L2 regularization相比，L1 regularization得到的w更加稀疏，即很多w为零值。其优点是节约存储空间，因为大部分w为0。然而，实际上L1 regularization在解决high variance方面比L2 regularization并不更具优势。而且，L1的在微分求导方面比较复杂。所以，一般L2 regularization更加常用。 随机失活（Dropout）正则化方法思想随机失活（Dropout）是指在深度学习网络的训练过程中，对于每层的神经元，按照一定的概率将其暂时从网络中丢弃。也就是说，每次训练时，每一层都有部分神经元不工作，起到简化复杂网络模型的效果，从而避免发生过拟合。 注：使用dropout训练结束后，在测试和实际应用模型时，不需要进行dropout 实现方法：反向随机失活（Inverted dropout）假设对于第l层神经元，设定神经元保留概率keep_prob=0.8，即该层有20%的神经元停止工作。dl为随机失活向量，其中80%的元素为1，20%的元素为0。 123dl = np.random.rand(al.shape[0],al.shape[1])&lt;keep_prob //生成随机失活向量al = np.multiply(al,dl) //第l层经过dropout的输出al /= keep_prob //进行缩放（scale up）以尽可能保持al的期望值相比之前没有大的变化，测试时就不需要再对样本数据进行类似的尺度伸缩操作 对于m个样本，单次迭代训练时，随机失活隐藏层一定数量的神经元；然后，在删除后的剩下的神经元上正向和反向更新参数；接着，下一次迭代中，恢复之前失活的神经元，重新随机失活一定数量的神经元，进行正向和反向更新参数；不断重复上述过程，直至迭代训练完成。 其他正则化方法数据增强（Data Augmentation）增加训练样本数量通常成本较高，难以获得额外的训练样本。但可以对已有的训练样本进行一些处理来“制造”出更多的样本。虽然这些是基于原有样本的，但是对增大训练样本数量还是有很有帮助的，不需要增加额外成本，却能起到防止过拟合的效果。 例如： 在图片识别问题中，可以对已有的图片进行水平翻转、垂直翻转、任意角度旋转、缩放或扩大等等。 图片识别中的数据增强 在数字识别问题中，可以将原有的数字图片进行任意旋转或者扭曲，增加一些noise。 数字识别中的数据增强 提前终止（Early Stopping）个神经网络模型随着迭代训练次数增加，train set error一般是单调减小的，而dev set error 先减小，之后又增大。也就是说训练次数过多时，模型会对训练样本拟合的越来越好，但是对验证集拟合效果逐渐变差，即发生了过拟合。因此，迭代训练次数不是越多越好，可以通过train set error和dev set error随迭代次数的变化趋势，选择合适的迭代次数，即early stopping。 随迭代次数的变化趋势 Early Stopping的缺点： 机器学习训练模型有两个目标：一是优化cost function，尽量减小J；二是防止过拟合。通过减少迭代次数来防止过拟合，会使得cost function不会足够小。即将两个目标融合在一起，同时优化，但可能没有“分而治之”的效果好。 Early Stopping vs L2 regularization： 与Early Stopping相比，L2 regularization可以实现“分而治之”的效果，但正则化参数λ的选择比较复杂。对这一点来说，early stopping比较简单。总的来说，L2 regularization更加常用一些。 标准化输入（Normalizing inputs）概念标准化输入就是对训练数据集进行归一化的操作，即将原始数据减去其均值μ后，再除以其方差σ²。 \\mu = \\frac{1}{m} \\sum_{i=1}^{m} X^{(i)}\\\\ \\sigma^{2} = \\frac{1}{m} \\sum_{i=1}^{m}(X^{(i)})^2\\\\ X := \\frac{X-\\mu}{\\sigma^2} 归一化过程 注：由于训练集进行了标准化处理，那么对于测试集或在实际应用时，应该使用同样的\\muμ和σ²对其进行标准化处理。 进行标准化的好处让所有输入归一到同样的尺度上，方便进行梯度下降算法时能够更快更准确地找到全局最优解。 如果不进行标准化处理，x1与x2之间分布极不平衡，训练得到的w1和w2也会在数量级上差别很大。这样导致的结果是cost function与w和b的关系可能是一个非常细长的椭圆形碗。对其进行梯度下降算法时，由于w1和w2数值差异很大，只能选择很小的学习因子α，来避免J发生振荡。一旦α较大，必然发生振荡，J不再单调下降。 如果进行了标准化操作，x1与x2分布均匀，w1和w2数值差别不大，得到的cost function与w和b的关系是类似圆形碗。对其进行梯度下降算法时，α可以选择相对大一些，且J一般不会发生振荡，保证了J是单调下降的。 归一化的好处图示 梯度消失和梯度爆炸（Vanishing and Exploding gradients）概念在梯度函数上出现的以指数级递增或者递减的情况分别称为梯度爆炸或者梯度消失。 假定 g(z)=z,b[l]=0，对于目标输出有： \\hat{y} = (W^{[L]}W^{[L-1]}\\cdots W^{[2]}W^{[1]})X 对于 W[l]的值大于 1 的情况，激活函数的值将以指数级递增（数值爆炸）； 对于 W[l]的值小于 1 的情况，激活函数的值将以指数级递减（数值消失）。 同样，这种情况也会引起梯度呈现同样的指数型增大或减小的变化，引起每次更新的步进长度过大或者过小，这让训练过程十分困难。 改善方法：对权展w进行初始化处理由下式 z = w_1x_1+w_2x_2+\\cdots + w_nx_n+b可知，当输入的数量 n 较大时，我们希望每个wi的值都小一些，这样它们的和得到的 z 也较小。为了得到较小的wi，可进行如下初始化： 若激活函数为sigmod/tanh函数，令其方差为1/n： 1w[l] = np.random.randn(n[l],n[l-1])*np.sqrt(1/n[l-1]) 若激活函数为ReLU函数，令其方差为2/n： 1w[l] = np.random.randn(n[l],n[l-1])*np.sqrt(1/n[l-1]) 另外还有： 1w[l] = np.random.randn(n[l],n[l-1])*np.sqrt(2/n[l-1]*n[l]) 梯度检验（Gradient Checking）检查验证反向传播过程中梯度下降算法是否正确。 梯度近似值求解 梯度近似值 梯度检查的过程几点注意 不要在整个训练过程中都进行梯度检查，仅仅作为debug使用。 如果梯度检查出现错误，找到对应出错的梯度，检查其推导是否出现错误。 计算近似梯度的时候不能忽略正则项。 梯度检查时关闭dropout，检查完毕后再打开dropout","categories":[],"tags":[{"name":"学习笔记","slug":"学习笔记","permalink":"http://yoursite.com/tags/学习笔记/"},{"name":"deeplearning.ai","slug":"deeplearning-ai","permalink":"http://yoursite.com/tags/deeplearning-ai/"}]},{"title":"deeplearning.ai学习笔记（四）深度神经网络（Deep Neural Network）","slug":"deeplearning-ai学习笔记（四）深度神经网络（Deep-Neural-Network）","date":"2019-08-14T14:28:38.000Z","updated":"2019-08-15T11:11:41.698Z","comments":true,"path":"2019/08/14/deeplearning-ai学习笔记（四）深度神经网络（Deep-Neural-Network）/","link":"","permalink":"http://yoursite.com/2019/08/14/deeplearning-ai学习笔记（四）深度神经网络（Deep-Neural-Network）/","excerpt":"深度神经网络其实就是包含更多的隐藏层神经网络。 越来越“深”的神经网络 深度神经网络为何如此有效？（Why deep representations?)神经网络效果显著，其强大能力主要源自神经网络足够“深”，即网络层数越多，神经网络就更加复杂和深入，学习也更加准确。","text":"深度神经网络其实就是包含更多的隐藏层神经网络。 越来越“深”的神经网络 深度神经网络为何如此有效？（Why deep representations?)神经网络效果显著，其强大能力主要源自神经网络足够“深”，即网络层数越多，神经网络就更加复杂和深入，学习也更加准确。 由“浅”到“深”的特征提取以人脸识别为例： 人脸识别的特征提取 经过训练，神经网络第一层所做的事就是从原始图片中提取出人脸的轮廓与边缘，即边缘检测。这样每个神经元得到的是一些边缘信息。神经网络第二层所做的事情就是将前一层的边缘进行组合，组合成人脸一些局部特征，比如眼睛、鼻子、嘴巴等。再往后面，就将这些局部特征组合起来，融合成人脸的模样。 深度网络能减少神经元个数以计算逻辑输出为例 y=x_1 \\oplus x_2 \\oplus \\cdots \\oplus x_n使用深度网络： 每层将前一层的两两单元进行异或，最后到一个输出。整个深度网络的层数是log2(n)，神经元个数为： 1+2+\\cdots+2^{log_2(n)-1}=n-1不使用深度网络： 仅仅使用单个隐藏层，由于包含了所有的逻辑位，需要的神经元个数达到指数级别。 构建深度神经网络前向传播和反向传播流程块图对于第l层来说： 正向传播时： 输入:a^{[l-1]} \\\\ 输出:a^{[l]} \\\\ 参数:W^{[l]},b^{[l]} \\\\ 缓存:z^{[l]}反向传播时： 输入:da^{[l]} \\\\ 输出:da^{[l-1]},dW^{[l]},db^{[l]}\\\\ 参数:W^{[l]},b^{[l]} 第l层流程块图 对于整个神经网络： 神经网络流程块图 前向传播和反向传播计算表达式正向传播 \\begin{cases} z^{[l]}=W^{[l]}a^{[l-1]}+b^{[l]} \\\\ a^{[l]}=g^{[l]}(z^{[l]}) \\end{cases}m个训练样本： \\begin{cases} Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]} \\\\ A^{[l]}=g^{[l]}(Z^{[l]}) \\end{cases}反向传播 \\begin{cases} dz^{[l]}=da^{[l]}*g^{[l]'}(z^{[l]}) \\\\ dW^{[l]}=dz^{[l]}\\centerdot a^{[l-1]}\\\\ db^{[l]}=dz^{[l]}\\\\ da^{[l-1]}=W^{[l]T}\\centerdot dz^{[l]}\\\\ \\end{cases}\\\\ 进一步推导可得递推关系:dz^{[l]}=W^{[l+1]T}\\centerdot dz^{[l+1]}*g^{[l]'}(z^{[l]})m个训练样本： \\begin{cases} dZ^{[l]}=dA^{[l]}*g^{[l]'}(Z^{[l]}) \\\\ dW^{[l]}=\\frac{1}{m}dZ^{[l]}\\centerdot A^{[l-1]T}\\\\ db^{[l]}=\\frac{1}{m}np.sum(dZ^{[l]},axis=1,keepdim=True) \\\\ dA^{[l-1]}=W^{[l]T}\\centerdot dZ^{[l]}\\\\ \\end{cases}\\\\ 进一步推导可得递推关系:dZ^{[l]}=W^{[l+1]T}\\centerdot dZ^{[l+1]}*g^{[l]'}(Z^{[l]})注：这里的“*”运算符表示矩阵对应位置相乘。 参数（Parameters） vs 超参数（Hyperparameters） 参数（parameters）：如W[l]、b[l] 超参数（hyperparameters）：学习率α，迭代次数N，神经网络层数L，各层神经元个数n[l]，激活函数g(z)等。它们决定了参数W[l]和b[l]的值，因此称为超参数。 Applied deep learning is a very empirical process. 如何设置最优的超参数是一个比较困难的、需要经验知识的问题。通常的做法是选择超参数一定范围内的值，分别代入神经网络进行训练，测试cost function随着迭代次数增加的变化，根据结果选择cost function最小时对应的超参数值。","categories":[],"tags":[{"name":"学习笔记","slug":"学习笔记","permalink":"http://yoursite.com/tags/学习笔记/"},{"name":"deeplearning.ai","slug":"deeplearning-ai","permalink":"http://yoursite.com/tags/deeplearning-ai/"}]},{"title":"deeplearning.ai学习笔记（三）浅层神经网络（Shallow Neural Networks）","slug":"deeplearning-ai学习笔记（三）浅层神经网络","date":"2019-08-12T13:44:29.000Z","updated":"2019-08-15T08:39:39.766Z","comments":true,"path":"2019/08/12/deeplearning-ai学习笔记（三）浅层神经网络/","link":"","permalink":"http://yoursite.com/2019/08/12/deeplearning-ai学习笔记（三）浅层神经网络/","excerpt":"神经网络的表示以一个单隐层神经网络为例： 单隐层神经网络 结构上，从左到右，可以分成三层：输入层（Input layer），隐藏层（Hidden layer）和输出层（Output layer）。输入层和输出层，对应着训练样本的输入和输出。隐藏层是抽象的非线性中间层，中间这一层节点的真实值并没有所观察，这也是其被命名为隐藏层的原因。","text":"神经网络的表示以一个单隐层神经网络为例： 单隐层神经网络 结构上，从左到右，可以分成三层：输入层（Input layer），隐藏层（Hidden layer）和输出层（Output layer）。输入层和输出层，对应着训练样本的输入和输出。隐藏层是抽象的非线性中间层，中间这一层节点的真实值并没有所观察，这也是其被命名为隐藏层的原因。 注：单隐藏层神经网络属于为两层神经网络，输入层不计入。 记法上： 把输入矩阵X记为a[0]，把隐藏层输出记为a[1]，输出层输出记为a[2]（即ŷ）。 用下标表示第几个神经元（下标从1开始）。例如a1[1]表示隐藏层第1个神经元，a2[1]表示隐藏层第2个神经元。隐藏层的输出可以写成矩阵形式： a^{[1]}= \\begin{bmatrix} a_1^{[1]}\\\\ a_2^{[1]}\\\\ a_3^{[1]}\\\\ a_4^{[1]} \\end{bmatrix} 参数 隐藏层参数：权重W[1]，维度是（4,3），4对应着隐藏层神经元个数，3对应着输入层x特征向量包含元素个数。常数项b[1]，维度是（4,1），4同样对应着隐藏层神经元个数。 输出层参数：权重W[2]，维度是（1,4），1对应着输出层神经元个数，4对应着输出层神经元个数。常数项b[2]，维度是（1,1），因为输出只有一个神经元。 总结：第i层的权重W[i]维度的行等于i层神经元的个数，列等于i-1层神经元的个数；第i层常数项b[i]维度的行等于i层神经元的个数，列始终为1。 神经网络的输出（正向传播）单个神经元的输出每个神经元进行两个计算：线性加权（计算z）、非线性激活（计算a）。 逻辑回归单元 z=w^{T}x+b \\\\ a=\\sigma(z)单个样本的神经网络正向传播过程每个节点的计算都对应着一次逻辑运算的过程，分别由计算z和a两部分组成。 单隐层神经网络 非向量化计算过程从输入层到隐藏层： z_1^{[1]}=w_1^{[1]T}x+b_1^{[1]},a_1^{[1]}=\\sigma(z_1^{[1]}) \\\\ z_2^{[1]}=w_2^{[1]T}x+b_2^{[1]},a_2^{[1]}=\\sigma(z_2^{[1]}) \\\\ z_3^{[1]}=w_3^{[1]T}x+b_3^{[1]},a_3^{[1]}=\\sigma(z_3^{[1]}) \\\\ z_4^{[1]}=w_4^{[1]T}x+b_4^{[1]},a_4^{[1]}=\\sigma(z_4^{[1]})从隐藏层到输出层： z_1^{[2]}=w_1^{[2]T}a^{[1]}+b_1^{[2]},a_1^{[2]}=\\sigma(z_1^{[2]})向量化计算过程 z^{[1]}=W^{[1]}x+b^{[1]},a^{[1]}=\\sigma(z^{[1]}) \\\\ z^{[2]}=W^{[2]}a^{[1]}+b^{[2]},a^{[2]}=\\sigma(z^{[2]})m个训练样本的神经网络正向传播过程非向量化计算过程for i=1 to m: z^{[1](i)}=W^{[1]}x^{(i)}+b^{[1]},a^{[1](i)}=\\sigma(z^{[1](i)}) \\\\ z^{[2](i)}=W^{[2]}a^{[1](i)}+b^{[2]},a^{[2](i)}=\\sigma(z^{[2](i)})向量化计算过程 Z^{[1]}=W^{[1]}X+b^{[1]},A^{[1]}=\\sigma(Z^{[1]}) \\\\ Z^{[2]}=W^{[2]}A^{[1]}+b^{[2]},A^{[2]}=\\sigma(Z^{[2]})其中，Z[1]的维度是（4,m），4是隐藏层神经元的个数；A[1]的维度与Z[1]相同；Z[2]和A[2]的维度均为（1,m）。（行表示神经元个数，列表示样本数目m） 激活函数几种常见激活函数sigmoid函数 表达式及图像： 导数： g^{'}(z)=a(1-a) tanh函数 表达式及图像： 导数： g^{'}(z)=1-a^{2} ReLU函数（线性整流函数，Rectified Linear Unit） 表达式及图像： 导数： g^{'}(z)=\\begin{cases} 1 & ,z>0 \\\\ 0 & ,z","categories":[],"tags":[{"name":"学习笔记","slug":"学习笔记","permalink":"http://yoursite.com/tags/学习笔记/"},{"name":"deeplearning.ai","slug":"deeplearning-ai","permalink":"http://yoursite.com/tags/deeplearning-ai/"}]},{"title":"deeplearning.ai学习笔记（二）神经网络基础——以二值分类问题的逻辑回归模型为例","slug":"deeplearning-ai学习笔记（二）神经网络基础——以二值分类问题的逻辑回归模型为例","date":"2019-08-11T02:18:57.000Z","updated":"2019-08-12T13:56:39.367Z","comments":true,"path":"2019/08/11/deeplearning-ai学习笔记（二）神经网络基础——以二值分类问题的逻辑回归模型为例/","link":"","permalink":"http://yoursite.com/2019/08/11/deeplearning-ai学习笔记（二）神经网络基础——以二值分类问题的逻辑回归模型为例/","excerpt":"二值分类（Binary Classification ）问题描述例如：猫咪检测器（Cat vs Non-Cat ） 目标是训练一个分类器，对于输入的照片，如果它是一张猫咪的照片就输出1，否则输出0。 输入将一张RGB三通道彩色图像展开为一个长的列向量做为输入。 若图片尺寸为64*64，则向量的维度n(x)=64*64*3=12288。","text":"二值分类（Binary Classification ）问题描述例如：猫咪检测器（Cat vs Non-Cat ） 目标是训练一个分类器，对于输入的照片，如果它是一张猫咪的照片就输出1，否则输出0。 输入将一张RGB三通道彩色图像展开为一个长的列向量做为输入。 若图片尺寸为64*64，则向量的维度n(x)=64*64*3=12288。 输入特征向量x 输出对于二值分类问题，输出结果只有两个——0或1。 输出标签的向量形式：（m维列向量） \\begin{bmatrix} y^{(1)}\\\\ y^{(2)} \\\\ \\vdots \\\\ y^{(i)} \\end{bmatrix}训练数据 单个样本由一对（x,y）表示，其中x是一个n(x)维的特征向量 ，y是取值为0或1的标签。 训练集包含m个训练样本（用小写m代表训练集的样本总数），(x(i),y(i)) 表示第i个样本的输入和输出。 写成矩阵形式如下：（X.shape=n_x*m） \\begin{bmatrix} \\vdots & \\vdots & \\vdots & \\vdots \\\\ x^{(1)} & x^{(2)} & \\cdots & x^{(i)} \\\\ \\vdots & \\vdots & \\vdots & \\vdots \\end{bmatrix}逻辑回归（Logistic Regression ）模型逻辑回归是一种用于解决输出是0/1的监督学习问题的学习算法，它使得预测值与训练数据之间的偏差最小。 定义以Cat vs No - cat为例： 把一张图片展开为特征向量x做为输入，算法将估计这张图片中包含猫咪的概率。 Given~x~,~\\hat{y}=P(y=1|x),where~0\\leq\\hat{y}\\leq1 逻辑回归模型 其中各个参数的意义： x：输入特征向量（一个n_x维列向量） y：训练集标签，y∈0,1 weights——w：权重（一个n_x维列向量） threshold——b：偏置量（一个实数） Sigmoid 函数： \\sigma(z)=\\frac{1}{1+e^{-z}} Sigmoid函数图像 该函数的特点： 当z趋于+∞，函数值趋于1 当z趋于-∞，函数值趋于0 当z=0，函数值为0.5 作用：将输出值限制在[0,1]，使其可以表示一个概率值 代价函数（Cost function ）损失函数 vs 代价函数 损失函数（Loss function）是定义在单个样本上的，算的是一个样本的误差。 代价函数（Cost function）是定义在整个训练集上的，是所有样本误差的平均，也就是损失函数的平均。 代价函数用来衡量参数在整个模型中的作用效果。 逻辑回归的损失函数 损失函数 推导过程： 我们希望算法输出ŷ 表示当给定输入特征x的时候y=1的概率。换句话说，如果y等于1，那么p(y|x)就等于ŷ ；相反地，当y=0时，p(y|x)就等于1-ŷ 。因此，如果ŷ表示当y=1的概率，那么1-ŷ就表示y=0的概率。 可以定义p(y|x)如下： 由于y只有0和1两种取值，因此上面的两个方程可以归纳为如下一个方程： p(y|x)=ŷ^y*(1-ŷ)^{1-y}因为对数函数是一个绝对的单调递增函数，最大化log(p(y|x))会得出和最大化p(y|x)相似的结果，因此可以取对数，简化公式。 log(ŷ^y*(1-ŷ)^{1-y}) =ylog(ŷ)+(1-y)log(1-ŷ)又因为通常在训练一个学习算法的时候，我们想要让概率变大。而在逻辑回归中，我们想要最小化L(ŷ,y)这个损失函数。最小化损失函数相当于最大化概率的对数，所以需要加一个负号。 L(ŷ,y)=-(ylog(ŷ)+(1-y)log(1-ŷ))逻辑回归的代价函数 代价函数 推导过程： 假设取出的训练样本相互独立，或者说服从独立同分布 (I.I.D: Independent and Identically Distributed) ，这些样本的概率就是各项概率的乘积。 给定X(i)从i=1到m时p(y(i))的乘积： p(y^{(1)})*p(y^{(2)})*\\cdots*p(y^{(m)})最大化这个式子本身和最大化它的对数效果相同，所以取对数： log(p(y^{(1)}))+log(p(y^{(2)}))+\\cdots+log(p(y^{(m)}))=\\sum_{i=1}^{m}log(p(y^{(i)}))=-\\sum_{i=1}^{m}L(ŷ^{(i)},y^{(i)})根据最大似然估计原理，选择能使该式最大化的参数。因为要最小化代价函数，而不是最大化似然值，所以要去掉这个负号。最后为了方便起见，使这些数值处于更好的尺度上，在前面添加一个缩放系数1/m。 综上，代价函数为： J(w,b)=\\frac{1}{m}\\sum_{i=1}^{m}L(ŷ^{(i)},y^{(i)})=-\\frac{1}{m}\\sum_{i=1}^{m}[y^{(i)}log(ŷ^{(i)})+(1-y^{(i)})log(1-ŷ^{(i)})]优化逻辑回归模型时，我们试着去找参数w和b，以此来缩小代价函数J， 逻辑回归可被视为一个非常小的神经网络 。 训练过程——梯度下降（Gradient Descent）算法算法思想在逻辑回归问题中，代价函数J是一个凸函数(convex function) 。为了去找到优的参数值，首先用一些初始值（0或随机）来初始化w和b，因为函数是凸函数，无论在哪里初始化，应该达到同一点或大致相同的点。 梯度下降法以初始点开始，然后朝最陡的下坡方向走一步，这是梯度下降的一次迭代。经过多次迭代收敛到全局最优值或接近全局最优值。 梯度下降示意图 参数的迭代公式： w:=w-\\alpha\\frac{\\partial{J(w,b)}}{\\partial{w}} b:=b-\\alpha\\frac{\\partial{J(w,b)}}{\\partial{b}}数学基础：计算图（Computation Graph）以下式为例 J(a,b,c)=3(a+bc)其计算图为： 计算图 计算图计算（前向传播过程）前向传播即为计算一个样本下J的值，计算过程如下： u=b*c=3*2=6 \\\\ v=a+u=5+6=11 \\\\ J=3V=33计算图求导（反向传播过程）反向传播即根据求导的链式法则，求解最终输出变量J对各个变量的导数。 计算过程如下： \\frac{dJ}{dv}=\\frac{d(3v)}{dv}=3 \\\\ \\frac{dJ}{da}=\\frac{dJ}{dv}*\\frac{dv}{da}=3*\\frac{d(a+u)}{da}=3 \\\\ \\frac{dJ}{du}=\\frac{dJ}{dv}*\\frac{dv}{du}=3*\\frac{d(a+u)}{du}=3 \\\\ \\frac{dJ}{db}=\\frac{dJ}{du}*\\frac{du}{db}=3*\\frac{d(bc)}{db}=3c=3*2=6 \\\\ \\frac{dJ}{dc}=\\frac{dJ}{du}*\\frac{du}{dc}=3*\\frac{d(bc)}{db}=3b=3*3=9为简化表示，将dJ/dval简记为dval，则在本例中： da=3,db=6,dc=9逻辑回归模型的前向传播和反向传播过程（单个样本）逻辑回归模型的计算图 逻辑回归模型的计算图 逻辑回归模型的前向传播顺序计算即可。 逻辑回归模型的反向传播计算过程如下： “da”=\\frac{dL}{da}=-\\frac{y}{a}+\\frac{1-y}{1-a} \\\\ “dz”=\\frac{dL}{dz}=\\frac{dL}{da}*\\frac{da}{dz}=(-\\frac{y}{a}+\\frac{1-y}{1-a})*\\frac{d\\sigma(z)}{dz} = a(1-a) \\\\ “dw_1”=\\frac{dL}{dw_1}=\\frac{dL}{dz}*\\frac{dz}{dw_1}=x_1*“dz” \\\\ “dw_2”=\\frac{dL}{dw_2}=\\frac{dL}{dz}*\\frac{dz}{dw_2}=x_2*“dz” \\\\ “db”=\\frac{dL}{db}=\\frac{dL}{dz}*\\frac{dz}{db}=“dz”非向量化的逻辑回归训练过程 非向量化的逻辑回归训练过程 缺点：显式地使用了两层for循环，时间效率低 解决方法：采用向量化的方法，可以极大地提高时间效率 向量化（Vectorizing）的逻辑回归训练过程 Z = W^{T}*X+b = np.dot(W.T,X)+b //此处存在python的广播机制 \\\\ A = σ(Z) \\\\ dZ = A - Y \\\\ dW = \\frac{1}{m}XdZ^{T}\\\\ db = \\frac{1}{m}*np.sum(dZ)\\\\ W := W - \\alpha dW\\\\ b := b - \\alpha db注意：此为一次迭代的过程，多次迭代使用显示循环不可避免。","categories":[],"tags":[{"name":"学习笔记","slug":"学习笔记","permalink":"http://yoursite.com/tags/学习笔记/"},{"name":"deeplearning.ai","slug":"deeplearning-ai","permalink":"http://yoursite.com/tags/deeplearning-ai/"}]},{"title":"那些既熟悉又陌生的C/C++知识点（长期更新）","slug":"那些既熟悉又陌生的C-C-知识点（长期更新）","date":"2019-08-09T15:21:44.000Z","updated":"2019-08-09T16:04:41.156Z","comments":true,"path":"2019/08/09/那些既熟悉又陌生的C-C-知识点（长期更新）/","link":"","permalink":"http://yoursite.com/2019/08/09/那些既熟悉又陌生的C-C-知识点（长期更新）/","excerpt":"","text":"头文件的等价写法在C++标准中，stdio.h更推荐使用等价写法：cstdio，也就是在前面加一个c，然后去掉.h即可。 123/* 以下两种写法等价 */#include&lt;stdio.h&gt;#include&lt;cstdio&gt; 整型变量类型表示数的范围 整型int：32位整数/绝对值在10^9范围以内的整数都可以定义为int型 长整型long long：64位整数/10^18以内（如10^10）的整数就要定义为long long型 long long型的使用long long型赋大于2^31-1的初值，需要在初值后面加上LL 经常利用typedef用LL来代替long long，以避免在程序中大量出现long long而降低编码的效率。 12345678#include&lt;cstdio&gt;typedef long long LL; //给long long起个别名LLint main()&#123; LL a = 123456789012345LL, b = 234567890123456LL; printf(\"%lld\\n\", a + b); return 0;&#125; 浮点数的存储类型对于浮点型，不要使用float，碰到浮点型的数据都应该用double来存储。 单精度float：有效精度只有6~7位 双精度double：有效精度有15~16位 double型的输入输出格式 输出格式：%f（与float型相同） 输入格式：scanf中是%lf 注：有些系统如果把输出格式写成%lf也不会出错，但尽量还是按标准来","categories":[],"tags":[{"name":"C/C++","slug":"C-C","permalink":"http://yoursite.com/tags/C-C/"}]},{"title":"deeplearning.ai学习笔记（一）深度学习引言","slug":"deeplearning-ai学习笔记（一）深度学习引言","date":"2019-08-09T06:30:38.000Z","updated":"2019-08-09T09:05:27.062Z","comments":true,"path":"2019/08/09/deeplearning-ai学习笔记（一）深度学习引言/","link":"","permalink":"http://yoursite.com/2019/08/09/deeplearning-ai学习笔记（一）深度学习引言/","excerpt":"AI is the new Electricity. ——吴恩达（Andrew Ng） 大约在一百年前，社会的电气化改变了每个主要行业。而如今我们见到了 AI令人惊讶的能量会产生同样巨大的转变。显然 AI的各个分支中，发展最为迅速的就是深度学习（deep learning）。而深度学习，一般指的是训练神经网络（有时是非常非常大/深的神经网络）。","text":"AI is the new Electricity. ——吴恩达（Andrew Ng） 大约在一百年前，社会的电气化改变了每个主要行业。而如今我们见到了 AI令人惊讶的能量会产生同样巨大的转变。显然 AI的各个分支中，发展最为迅速的就是深度学习（deep learning）。而深度学习，一般指的是训练神经网络（有时是非常非常大/深的神经网络）。 何为神经网络（What is a neural network?）引例：房价预测（Housing Price Prediction）如下图，对于单个因素（如房屋面积）的房屋预测，可以用ReLU函数进行拟合，相当于一个只有一个神经元的神经网络。 ReLU函数拟合 用房子的大小 x 作为神经网络的输入，它进入到这个节点(这个小圈)中 ，然后这个小圈就输出了房价 y 。所以这个小圈，也就是一个神经网络中的一个神经元，就会执行上图中画出的这个方程。 单个神经元的神经网络 一个很大的神经网络是由许多这样的单一神经元叠加在一起组成。比如下面这个例子，我们不仅仅根据房屋大小来预测房屋价格，我们引入其他特征量。能够容纳的家庭人口也会影响房屋价格 ，这个因素其实是取决于房屋大小以及卧室的数量这两个因素决定了这个房子是否能够容纳你的家庭 。 多因素房价预测 所以在这个例子中： x 表示所有这四个输入 (房屋大小、卧室数量、邮政编码、富裕程度) y表示试图去预测的价格 每一个小圆圈是ReLU 函数或者别的一些非线性函数 这就是最基本的神经网络。 简单神经网络 深度学习：监督学习的一种（Supervised Learning with Neural Networks）监督学习与非监督学习监督学习：给定标记好的训练样本集，如回归问题、分类问题 非监督学习：给定样本集，发现特征数据中的分布结构，如聚类问题 监督学习 非监督学习 深度学习在监督学习中的应用 &amp; 常用神经网络 深度学习的应用 几种常见神经网络 结构化数据与非结构化数据 结构化数据与非结构化数据示例 深度学习兴起的原因（Why is Deep Learning taking off?） Scale drives deep learning progress.——吴恩达（Andrew Ng） 深度学习的性能 x-axis is the amount of data （数据量） y-axis (vertical axis) is the performance of the algorithm. （算法性能） 数据量与算法性能关系 当数据量比较少时，深度学习方法性能不一定优于经典机器学习方法。 通过增加数据量和神经网络规模可提升深度学习方法的性能。 深度学习兴起的原因 Data：信息化社会产生了巨大的数据量 Computation：现代计算机计算性能的极大提升（GPU与CPU） Algorithms：算法的优化进一步提升了性能（如ReLU的使用）","categories":[],"tags":[{"name":"学习笔记","slug":"学习笔记","permalink":"http://yoursite.com/tags/学习笔记/"},{"name":"deeplearning.ai","slug":"deeplearning-ai","permalink":"http://yoursite.com/tags/deeplearning-ai/"}]}]}