<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
    

    

    



    <meta charset="utf-8">
    
    
    
    
    <title>deeplearning.ai学习笔记（十五）自然语言处理与词嵌入 | 一只程序喵 | 刘明辉的个人博客</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <meta name="theme-color" content="#3F51B5">
    
    
    <meta name="keywords" content="学习笔记,deeplearning.ai">
    <meta name="description" content="词的表示one-hot向量表示表示方法：  首先建立一个较大的词汇表（例如10000）;  然后使用one-hot的方式对每个单词进行编码。">
<meta name="keywords" content="学习笔记,deeplearning.ai">
<meta property="og:type" content="article">
<meta property="og:title" content="deeplearning.ai学习笔记（十五）自然语言处理与词嵌入">
<meta property="og:url" content="http://yoursite.com/2019/09/01/deeplearning-ai学习笔记（十五）自然语言处理与词嵌入/index.html">
<meta property="og:site_name" content="一只程序喵">
<meta property="og:description" content="词的表示one-hot向量表示表示方法：  首先建立一个较大的词汇表（例如10000）;  然后使用one-hot的方式对每个单词进行编码。">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://s2.ax1x.com/2019/09/03/nktjJ0.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/09/03/nktbZj.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/09/03/nktqds.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/09/03/nkt7LQ.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/09/03/nktTsg.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/09/03/nkt4Rf.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/09/03/nktgZd.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/09/03/nkt6qH.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/09/03/nktsMD.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/09/03/nkt2dA.jpg">
<meta property="og:image" content="https://s2.ax1x.com/2019/09/03/nktRII.jpg">
<meta property="og:updated_time" content="2019-09-03T07:40:24.806Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="deeplearning.ai学习笔记（十五）自然语言处理与词嵌入">
<meta name="twitter:description" content="词的表示one-hot向量表示表示方法：  首先建立一个较大的词汇表（例如10000）;  然后使用one-hot的方式对每个单词进行编码。">
<meta name="twitter:image" content="https://s2.ax1x.com/2019/09/03/nktjJ0.png">
    
        <link rel="alternate" type="application/atom+xml" title="一只程序喵" href="/atom.xml">
    
    <link rel="shortcut icon" href="/favicon.ico">
    <link rel="stylesheet" href="//unpkg.com/hexo-theme-material-indigo@latest/css/style.css">
    <script>window.lazyScripts=[]</script>

    <!-- custom head -->
    

</head>

<body>
    <div id="loading" class="active"></div>

    <aside id="menu" class="hide" >
  <div class="inner flex-row-vertical">
    <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menu-off">
        <i class="icon icon-lg icon-close"></i>
    </a>
    <div class="brand-wrap" style="background-image:url(/img/brand.jpg)">
      <div class="brand">
        <a href="/" class="avatar waves-effect waves-circle waves-light">
          <img src="/img/avatar.jpg">
        </a>
        <hgroup class="introduce">
          <h5 class="nickname">刘明辉</h5>
          <a href="mailto:549208791@qq.com" title="549208791@qq.com" class="mail">549208791@qq.com</a>
        </hgroup>
      </div>
    </div>
    <div class="scroll-wrap flex-col">
      <ul class="nav">
        
            <li class="waves-block waves-effect">
              <a href="/"  >
                <i class="icon icon-lg icon-home"></i>
                主页
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/archives"  >
                <i class="icon icon-lg icon-archives"></i>
                Archives
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/tags"  >
                <i class="icon icon-lg icon-tags"></i>
                Tags
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/categories"  >
                <i class="icon icon-lg icon-th-list"></i>
                Categories
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="https://github.com/liuminghui233" target="_blank" >
                <i class="icon icon-lg icon-github"></i>
                Github
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/about"  >
                <i class="icon icon-lg icon-link"></i>
                About
              </a>
            </li>
        
      </ul>
    </div>
  </div>
</aside>

    <main id="main">
        <header class="top-header" id="header">
    <div class="flex-row">
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light on" id="menu-toggle">
          <i class="icon icon-lg icon-navicon"></i>
        </a>
        <div class="flex-col header-title ellipsis">deeplearning.ai学习笔记（十五）自然语言处理与词嵌入</div>
        
        <div class="search-wrap" id="search-wrap">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back">
                <i class="icon icon-lg icon-chevron-left"></i>
            </a>
            <input type="text" id="key" class="search-input" autocomplete="off" placeholder="搜尋">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="search">
                <i class="icon icon-lg icon-search"></i>
            </a>
        </div>
        
        
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menuShare">
            <i class="icon icon-lg icon-share-alt"></i>
        </a>
        
    </div>
</header>
<header class="content-header post-header">

    <div class="container fade-scale">
        <h1 class="title">deeplearning.ai学习笔记（十五）自然语言处理与词嵌入</h1>
        <h5 class="subtitle">
            
                <time datetime="2019-09-01T06:27:57.000Z" itemprop="datePublished" class="page-time">
  2019-09-01
</time>


            
        </h5>
    </div>

    


</header>


<div class="container body-wrap">
    
    <aside class="post-widget">
        <nav class="post-toc-wrap post-toc-shrink" id="post-toc">
            <h4>TOC</h4>
            <ol class="post-toc"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#词的表示"><span class="post-toc-number">1.</span> <span class="post-toc-text">词的表示</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#one-hot向量表示"><span class="post-toc-number">1.1.</span> <span class="post-toc-text">one-hot向量表示</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#特征表示法（Featurized-representation）"><span class="post-toc-number">1.2.</span> <span class="post-toc-text">特征表示法（Featurized representation）</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#词嵌入（word-embeddings）"><span class="post-toc-number">2.</span> <span class="post-toc-text">词嵌入（word embeddings）</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#概念"><span class="post-toc-number">2.1.</span> <span class="post-toc-text">概念</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#词嵌入矩阵"><span class="post-toc-number">2.2.</span> <span class="post-toc-text">词嵌入矩阵</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#词嵌入与迁移学习"><span class="post-toc-number">2.3.</span> <span class="post-toc-text">词嵌入与迁移学习</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#词嵌入与类比推理"><span class="post-toc-number">2.4.</span> <span class="post-toc-text">词嵌入与类比推理</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#词嵌入学习算法"><span class="post-toc-number">3.</span> <span class="post-toc-text">词嵌入学习算法</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#神经概率语言模型（Neural-Probabilistic-Language-Model）"><span class="post-toc-number">3.1.</span> <span class="post-toc-text">神经概率语言模型（Neural Probabilistic Language Model）</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Word2Vec"><span class="post-toc-number">3.2.</span> <span class="post-toc-text">Word2Vec</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Skip-Gram模型"><span class="post-toc-number">3.2.1.</span> <span class="post-toc-text">Skip-Gram模型</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#降低计算量的方法"><span class="post-toc-number">3.2.2.</span> <span class="post-toc-text">降低计算量的方法</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#分层Softmax分类器（Hierarchical-Softmax-Classifier）"><span class="post-toc-number">3.2.2.1.</span> <span class="post-toc-text">分层Softmax分类器（Hierarchical Softmax Classifier）</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#负采样模型"><span class="post-toc-number">3.2.2.2.</span> <span class="post-toc-text">负采样模型</span></a></li></ol></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#GloVe（Global-Vectors）"><span class="post-toc-number">3.3.</span> <span class="post-toc-text">GloVe（Global Vectors）</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#词嵌入应用实例"><span class="post-toc-number">4.</span> <span class="post-toc-text">词嵌入应用实例</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#情感分类（Sentiment-Classification）"><span class="post-toc-number">4.1.</span> <span class="post-toc-text">情感分类（Sentiment Classification）</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#问题描述"><span class="post-toc-number">4.1.1.</span> <span class="post-toc-text">问题描述</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#简单模型"><span class="post-toc-number">4.1.2.</span> <span class="post-toc-text">简单模型</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#RNN模型（many-to-one）"><span class="post-toc-number">4.1.3.</span> <span class="post-toc-text">RNN模型（many-to-one）</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#词嵌入除偏（Debiasing-word-embeddings）"><span class="post-toc-number">4.2.</span> <span class="post-toc-text">词嵌入除偏（Debiasing word embeddings）</span></a></li></ol></li></ol>
        </nav>
    </aside>


<article id="post-deeplearning-ai学习笔记（十五）自然语言处理与词嵌入"
  class="post-article article-type-post fade" itemprop="blogPost">

    <div class="post-card">
        <h1 class="post-card-title">deeplearning.ai学习笔记（十五）自然语言处理与词嵌入</h1>
        <div class="post-meta">
            <time class="post-time" title="2019-09-01 14:27:57" datetime="2019-09-01T06:27:57.000Z"  itemprop="datePublished">2019-09-01</time>

            


            
<span id="busuanzi_container_page_pv" title="文章总阅读量" style='display:none'>
    <i class="icon icon-eye icon-pr"></i><span id="busuanzi_value_page_pv"></span>
</span>


        </div>
        <div class="post-content" id="post-content" itemprop="postContent">
            <h2 id="词的表示"><a href="#词的表示" class="headerlink" title="词的表示"></a>词的表示</h2><h3 id="one-hot向量表示"><a href="#one-hot向量表示" class="headerlink" title="one-hot向量表示"></a>one-hot向量表示</h3><p><strong>表示方法：</strong></p>
<ol>
<li><p>首先建立一个较大的词汇表（例如10000）;</p>
</li>
<li><p>然后使用one-hot的方式对每个单词进行编码。<a id="more"></a></p>
<p>例：单词Man、Woman、King、Queen、Apple、Orange分别出现在词汇表的第5391、9853、4914、7157、456、6257的位置，则它们分别用<script type="math/tex">O_{5391}、O_{9853}、O_{4914}、O_{7157}、O_{456}、O_{6257}</script>表示。</p>
</li>
</ol>
<p><strong>缺点：</strong>每个单词都是独立的、正交的，无法知道不同单词之间的相似程度。</p>
<h3 id="特征表示法（Featurized-representation）"><a href="#特征表示法（Featurized-representation）" class="headerlink" title="特征表示法（Featurized representation）"></a>特征表示法（Featurized representation）</h3><p>使用一个特征向量表征单词，特征向量的每个元素都是对该单词某一特征的量化描述，量化范围可以是[-1,1]之间。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://s2.ax1x.com/2019/09/03/nktjJ0.png" alt="特征表示实例" title>
                </div>
                <div class="image-caption">特征表示实例</div>
            </figure>
<p>特征向量的长度依情况而定，特征元素越多则对单词表征得越全面。每个单词用e+词汇表索引的方式标记，例如<script type="math/tex">e_{5391}、e_{9853}、e_{4914}、e_{7157}、e_{456}、e_{6257}</script>。</p>
<p>注：在实际应用中，特征向量很多特征元素并不一定对应到有物理意义的特征，是比较抽象的（使用各种词嵌入算法学到的词向量实际上大多都超出了人类的理解范围，难以从某个值中看出与语义的相关程度）。</p>
<h2 id="词嵌入（word-embeddings）"><a href="#词嵌入（word-embeddings）" class="headerlink" title="词嵌入（word embeddings）"></a>词嵌入（word embeddings）</h2><h3 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h3><p>词嵌入，即<strong>特征化单词的操作</strong>。</p>
<p>每个单词都由高维特征向量表征，为了可视化不同单词之间的相似性，可以使用降维操作，例如t-SNE算法。如下图所示：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://s2.ax1x.com/2019/09/03/nktbZj.png" alt="降维可视化" title>
                </div>
                <div class="image-caption">降维可视化</div>
            </figure>
<p>从上图可以看出相似的单词分布距离较近，从而也证明了词嵌入能有效表征单词的关键特征。</p>
<h3 id="词嵌入矩阵"><a href="#词嵌入矩阵" class="headerlink" title="词嵌入矩阵"></a>词嵌入矩阵</h3><p>假设某个词汇库包含了10000个单词，每个单词包含的特征维度为300，那么表征所有单词的词嵌入矩阵维度为300 x 10000，用<script type="math/tex">E</script>表示。某单词<script type="math/tex">w</script>的one-hot向量表示为<script type="math/tex">O_w</script>，维度为10000 x 1，则该单词的特征向量表达式为：</p>
<script type="math/tex; mode=display">
e_w=E·O_w</script><p>因此，只要知道了词嵌入矩阵，就能计算出所有单词的特征向量。</p>
<p>注：这种矩阵乘积运算效率并不高。通常做法是直接从<script type="math/tex">E</script>中选取第w列作为<script type="math/tex">e_w</script>即可。</p>
<h3 id="词嵌入与迁移学习"><a href="#词嵌入与迁移学习" class="headerlink" title="词嵌入与迁移学习"></a>词嵌入与迁移学习</h3><p>将单词用不同的特征来表示，即使是训练样本中没有的单词，也可以根据词嵌入的结果得到与其词性相近的单词，从而得到与该单词相近的结果，有效减少了训练样本的数量。</p>
<p>用词嵌入做迁移学习步骤如下：</p>
<ol>
<li>从大量的文本集中学习词嵌入，或者下载网上开源的、预训练好的词嵌入模型；</li>
<li>将这些词嵌入模型迁移到新的、只有少量标注训练集的任务中；</li>
<li>使用新数据微调词嵌入。（当标记数据集不是很大时可以省下这一步）</li>
</ol>
<h3 id="词嵌入与类比推理"><a href="#词嵌入与类比推理" class="headerlink" title="词嵌入与类比推理"></a>词嵌入与类比推理</h3><p>词嵌入可用于类比推理。例如，给定对应关系“男性（Man）”对“女性（Woman）”，想要类比出“国王（King）”对应的词汇。则可以有<script type="math/tex">e_{man}−e_{woman}≈e_{king}−e_?</script>，之后的目标就是找到词向量<script type="math/tex">w</script>，来找到使相似度<script type="math/tex">sim(e_w,e_{king}−e_{man}+e_{woman})</script>最大。</p>
<p>一个最常用的相似度计算函数是<strong>余弦相似度（cosine similarity）</strong>。公式为：</p>
<script type="math/tex; mode=display">
sim(u,v)=\frac{u^Tv}{||u||_2||v||_w}</script><p>相关论文：<a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/rvecs.pdf" target="_blank" rel="noopener">Mikolov et. al., 2013, Linguistic regularities in continuous space word representations</a></p>
<h2 id="词嵌入学习算法"><a href="#词嵌入学习算法" class="headerlink" title="词嵌入学习算法"></a>词嵌入学习算法</h2><blockquote>
<p>词的语义由其上下文决定（A word is characterized by thecompany it keeps.）</p>
</blockquote>
<p><strong>学习目标：</strong>词嵌入矩阵<script type="math/tex">E</script></p>
<h3 id="神经概率语言模型（Neural-Probabilistic-Language-Model）"><a href="#神经概率语言模型（Neural-Probabilistic-Language-Model）" class="headerlink" title="神经概率语言模型（Neural Probabilistic Language Model）"></a>神经概率语言模型（Neural Probabilistic Language Model）</h3><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://s2.ax1x.com/2019/09/03/nktqds.png" alt="神经概率语言模型" title>
                </div>
                <div class="image-caption">神经概率语言模型</div>
            </figure>
<p>训练过程中，将语料库中的某些词作为目标词，以目标词的部分上下文作为输入，Softmax 输出的预测结果为目标词，正确的输出label是“juice”。嵌入矩阵 E 和 w、b 为需要通过训练得到的参数。</p>
<p>为了让神经网络输入层数目固定，可以选择只取预测单词的前4个单词作为输入，例如该句中只选择“a glass of orange”四个单词作为输入。</p>
<p><strong>把输入叫做context，输出叫做target</strong>。对应到上面这句话里：</p>
<ul>
<li>context: a glass of orange</li>
<li>target: juice</li>
</ul>
<p><strong>context的选择方法：</strong></p>
<ul>
<li>target前n个单词或后n个单词，n可调</li>
<li>target前1个单词</li>
<li>target附近某1个单词（Skip-Gram）</li>
</ul>
<p>相关论文：<a href="http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf" target="_blank" rel="noopener">Bengio et. al., 2003, A neural probabilistic language model</a></p>
<h3 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h3><h4 id="Skip-Gram模型"><a href="#Skip-Gram模型" class="headerlink" title="Skip-Gram模型"></a>Skip-Gram模型</h4><ul>
<li><strong>学习样本：</strong></li>
</ul>
<ol>
<li>首先随机选择一个单词作为context，例如“orange”；</li>
<li>然后使用一个宽度为5或10（自定义）的滑动窗，在context附近选择一个单词作为target，可以是“juice”、“glass”、“my”等等。</li>
</ol>
<p>最终得到了<strong>多个context—target对</strong>作为监督式学习样本。</p>
<ul>
<li><p><strong>训练过程：</strong>构建自然语言模型</p>
</li>
<li><p><strong>softmax单元输出：</strong></p>
<script type="math/tex; mode=display">
\hat{y}=\frac{e^{\theta_t^{T}·e_c}}{\sum_{j=1}^{10000}e^{\theta_j^{T}·e_c}}</script><p>其中<script type="math/tex">θ</script>为target对应的参数，<script type="math/tex">e_c</script>为context的embedding vector。</p>
</li>
<li><p><strong>Loss Function：</strong></p>
<script type="math/tex; mode=display">
L(\hat{y},y)=-\sum_{i=1}^{10000}y_i log\hat{y}_i</script></li>
<li><p><strong>缺点：</strong>在 Softmax 中，每次计算条件概率时，需要对词典中所有词做求和操作，因此计算量很大。</p>
</li>
</ul>
<p>Skip-Gram模型是Word2Vec的一种，Word2Vec的另外一种模型是CBOW（Continuous Bag of Words），CBOW 模型的工作方式与 Skip-gram 相反，通过采样上下文中的词来预测中间的词。</p>
<p>相关论文：<a href="https://arxiv.org/pdf/1301.3781.pdf" target="_blank" rel="noopener">Mikolov et. al., 2013. Efficient estimation of word representations in vector space.</a></p>
<h4 id="降低计算量的方法"><a href="#降低计算量的方法" class="headerlink" title="降低计算量的方法"></a>降低计算量的方法</h4><h5 id="分层Softmax分类器（Hierarchical-Softmax-Classifier）"><a href="#分层Softmax分类器（Hierarchical-Softmax-Classifier）" class="headerlink" title="分层Softmax分类器（Hierarchical Softmax Classifier）"></a>分层Softmax分类器（Hierarchical Softmax Classifier）</h5><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://s2.ax1x.com/2019/09/03/nkt7LQ.png" alt="分层Softmax分类器" title>
                </div>
                <div class="image-caption">分层Softmax分类器</div>
            </figure>
<p>这种分类器是一种二分类器，它在每个数节点上对目标单词进行区间判断，最终定位到目标单词。</p>
<p>实际应用中常用<strong>哈夫曼树</strong>，把比较常用的单词放在树的顶层，而把不常用的单词放在树的底层，以提高搜索速度。</p>
<h5 id="负采样模型"><a href="#负采样模型" class="headerlink" title="负采样模型"></a>负采样模型</h5><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://s2.ax1x.com/2019/09/03/nktTsg.png" alt="负采样模型" title>
                </div>
                <div class="image-caption">负采样模型</div>
            </figure>
<p>如上图所示，当输入的词为一对上下文-目标词时，标签设置为 1。另外任意取 k 对非上下文-目标词作为负样本，标签设置为 0。对于小数据集，k 取 520 较为合适；而当有大量数据时，k 可以取 25。</p>
<p>改用多个 Sigmoid 输出上下文-目标词（c, t）为正样本的概率：</p>
<script type="math/tex; mode=display">
P(y=1|c,t)=\sigma(\theta_t^T e_c)</script><p>其中，<script type="math/tex">θ_t</script>、<script type="math/tex">e_c</script>分别代表目标词和上下文的词向量。</p>
<p>之前训练中每次要更新 n 维的多分类 Softmax 单元（n 为词典中词的数量）。现在每次只需要更新 k+1 维的二分类 Sigmoid 单元，计算量大大降低。</p>
<p>关于计算选择某个词作为负样本的概率，作者推荐采用以下公式（而非经验频率或均匀分布）：</p>
<script type="math/tex; mode=display">
p(w_i)=\frac{f(w_i)^{\frac{3}{4}}}{\sum_{j=0}^{m} f(w_j)^{\frac{3}{4}}}</script><p>其中，<script type="math/tex">f(w_i)</script>代表语料库中单词<script type="math/tex">w_i</script>出现的频率。上述公式更加平滑，能够增加低频词的选取可能。</p>
<p>相关论文：<a href="https://arxiv.org/pdf/1310.4546.pdf" target="_blank" rel="noopener">Mikolov et. al., 2013. Distributed representation of words and phrases and their compositionality</a></p>
<h3 id="GloVe（Global-Vectors）"><a href="#GloVe（Global-Vectors）" class="headerlink" title="GloVe（Global Vectors）"></a>GloVe（Global Vectors）</h3><p>Glove模型基于语料库统计了<strong>词共现矩阵<script type="math/tex">X</script></strong>，<script type="math/tex">X</script>中的元素<script type="math/tex">X_{ij}</script>表示单词<script type="math/tex">i</script>和单词<script type="math/tex">j</script>“为上下文-目标词”的次数。之后，用梯度下降法最小化以下损失函数：</p>
<script type="math/tex; mode=display">
J=\sum_{i=1}^{N} \sum_{j=1}^{N} f(X_{ij}) (\theta_i^t e_j +b_i+b_j^{'}-logX_{ij})^2</script><p><strong>其中：</strong></p>
<ul>
<li><script type="math/tex; mode=display">θ_i$$、$$e_j$$是单词$$i$$和单词$$j$$的词向量，是对称的，最终的词向量表示为：</script><p>e_w = \frac{e_w+\theta_w}{2}<br>$$</p>
</li>
<li><p>权重因子<script type="math/tex">f()</script>是一个用来避免<script type="math/tex">X_{ij}=0</script>时<script type="math/tex">log(X_{ij})</script>为负无穷大、并在其他情况下调整权重的函数；当<script type="math/tex">X_{ij}=0</script>时，<script type="math/tex">f(X_{ij})=0</script>忽略了无任何相关性的context和target；</p>
</li>
<li><p><script type="math/tex">b_i</script>、<script type="math/tex">b_j</script>是偏移量。</p>
</li>
</ul>
<p>相关论文：<a href="https://nlp.stanford.edu/pubs/glove.pdf" target="_blank" rel="noopener">Pennington st. al., 2014. Glove: Global Vectors for Word Representation</a></p>
<h2 id="词嵌入应用实例"><a href="#词嵌入应用实例" class="headerlink" title="词嵌入应用实例"></a>词嵌入应用实例</h2><h3 id="情感分类（Sentiment-Classification）"><a href="#情感分类（Sentiment-Classification）" class="headerlink" title="情感分类（Sentiment Classification）"></a>情感分类（Sentiment Classification）</h3><h4 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h4><p>情感分类是指分析一段文本对某个对象的情感是正面的还是负面的。</p>
<p><strong>数据集：</strong></p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://s2.ax1x.com/2019/09/03/nkt4Rf.png" alt="情感分析数据" title>
                </div>
                <div class="image-caption">情感分析数据</div>
            </figure>
<p>情感分类问题难点是缺少足够多的训练样本，而词嵌入可以帮助解决训练样本不足的问题。</p>
<h4 id="简单模型"><a href="#简单模型" class="headerlink" title="简单模型"></a>简单模型</h4><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://s2.ax1x.com/2019/09/03/nktgZd.png" alt="情感分类简单模型" title>
                </div>
                <div class="image-caption">情感分类简单模型</div>
            </figure>
<p><strong>缺点：</strong>使用平均法，没有考虑句子中单词出现的次序，忽略其位置信息。而有时候，不同单词出现的次序直接决定了句意（如“Completely lacking in good taste, good service, and good ambience.”）。</p>
<h4 id="RNN模型（many-to-one）"><a href="#RNN模型（many-to-one）" class="headerlink" title="RNN模型（many-to-one）"></a>RNN模型（many-to-one）</h4><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://s2.ax1x.com/2019/09/03/nkt6qH.png" alt="RNN模型" title>
                </div>
                <div class="image-caption">RNN模型</div>
            </figure>
<h3 id="词嵌入除偏（Debiasing-word-embeddings）"><a href="#词嵌入除偏（Debiasing-word-embeddings）" class="headerlink" title="词嵌入除偏（Debiasing word embeddings）"></a>词嵌入除偏（Debiasing word embeddings）</h3><p>词嵌入的结果中存在一些性别、宗教、种族等偏见或者歧视。如：</p>
<blockquote>
<p>Man: Computer programmer as Woman: Homemaker</p>
<p>Father: Doctor as Mother: Nurse</p>
</blockquote>
<p><strong>词嵌入除偏的步骤：</strong>以性别歧视为例</p>
<ol>
<li><p><strong>确定偏见方向：</strong>对所有性别对立的单词求差值，再平均。</p>
<script type="math/tex; mode=display">
bias~direction = \frac{1}{N} ((e_{he}-e_{she})+(e_{mela}-e_{fmela})+\cdots)</script><p><img src="https://s2.ax1x.com/2019/09/03/nktsMD.png" alt="偏见方向可视化结果"></p>
</li>
<li><p><strong>中立化（Neutralize）本身与性别无关词汇：</strong>将要除偏的词向量在与性别相关的方向<script type="math/tex">g</script>上的值置为0。</p>
<p><img src="https://s2.ax1x.com/2019/09/03/nkt2dA.jpg" alt="中立化"></p>
</li>
<li><p><strong>均衡化（Equalization）本身与性别有关词汇：</strong>确保一对词（如actor&amp;actress）到<script type="math/tex">g</script>和<script type="math/tex">g_⊥</script>的距离相等。</p>
<p><img src="https://s2.ax1x.com/2019/09/03/nktRII.jpg" alt="均衡化"></p>
</li>
</ol>
<p>相关论文：<a href="https://arxiv.org/pdf/1607.06520.pdf" target="_blank" rel="noopener">Bolukbasi et. al., 2016. Man is to computer programmer as woman is to homemaker? Debiasing word embeddings</a></p>

        </div>

        <blockquote class="post-copyright">
    
    <div class="content">
        
<span class="post-time">
    最後更新時間：<time datetime="2019-09-03T07:40:24.806Z" itemprop="dateUpdated">2019-09-03 15:40:24</time>
</span><br>


        
        版权声明:本文为博主原创文章,未经博主允许不得转载。<a href="/2019/09/01/deeplearning-ai学习笔记（十五）自然语言处理与词嵌入/" target="_blank" rel="external">http://yoursite.com/2019/09/01/deeplearning-ai学习笔记（十五）自然语言处理与词嵌入/</a>
        
    </div>
    
    <footer>
        <a href="http://yoursite.com">
            <img src="/img/avatar.jpg" alt="刘明辉">
            刘明辉
        </a>
    </footer>
</blockquote>

        
<div class="page-reward">
    <a id="rewardBtn" href="javascript:;" class="page-reward-btn waves-effect waves-circle waves-light">赏</a>
</div>



        <div class="post-footer">
            
	<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/deeplearning-ai/">deeplearning.ai</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/学习笔记/">学习笔记</a></li></ul>


            
<div class="page-share-wrap">
    

<div class="page-share" id="pageShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://yoursite.com/2019/09/01/deeplearning-ai学习笔记（十五）自然语言处理与词嵌入/&title=《deeplearning.ai学习笔记（十五）自然语言处理与词嵌入》 — 一只程序喵&pic=http://yoursite.com/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://yoursite.com/2019/09/01/deeplearning-ai学习笔记（十五）自然语言处理与词嵌入/&title=《deeplearning.ai学习笔记（十五）自然语言处理与词嵌入》 — 一只程序喵&source=词的表示one-hot向量表示表示方法：

首先建立一个较大的词汇表（例如10000）;

然后使用one-hot的方式对每个单词进行编码。" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://yoursite.com/2019/09/01/deeplearning-ai学习笔记（十五）自然语言处理与词嵌入/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《deeplearning.ai学习笔记（十五）自然语言处理与词嵌入》 — 一只程序喵&url=http://yoursite.com/2019/09/01/deeplearning-ai学习笔记（十五）自然语言处理与词嵌入/&via=http://yoursite.com" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://yoursite.com/2019/09/01/deeplearning-ai学习笔记（十五）自然语言处理与词嵌入/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>



    <a href="javascript:;" id="shareFab" class="page-share-fab waves-effect waves-circle">
        <i class="icon icon-share-alt icon-lg"></i>
    </a>
</div>



        </div>
    </div>

    
<nav class="post-nav flex-row flex-justify-between">
  
    <div class="waves-block waves-effect prev">
      <a href="/2019/09/02/deeplearning-ai学习笔记（十六）序列模型和注意力机制/" id="post-prev" class="post-nav-link">
        <div class="tips"><i class="icon icon-angle-left icon-lg icon-pr"></i> Prev</div>
        <h4 class="title">deeplearning.ai学习笔记（十六）序列模型和注意力机制</h4>
      </a>
    </div>
  

  
    <div class="waves-block waves-effect next">
      <a href="/2019/08/31/deeplearning-ai学习笔记（十四）循环神经网络/" id="post-next" class="post-nav-link">
        <div class="tips">Next <i class="icon icon-angle-right icon-lg icon-pl"></i></div>
        <h4 class="title">deeplearning.ai学习笔记（十四）循环神经网络（Recurrent Neural Network）</h4>
      </a>
    </div>
  
</nav>



    




















</article>

<div id="reward" class="page-modal reward-lay">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <h3 class="reward-title">
        <i class="icon icon-quote-left"></i>
        谢谢打赏ο(=•ω＜=)ρ⌒☆
        <i class="icon icon-quote-right"></i>
    </h3>
    <div class="reward-content">
        
        <div class="reward-code">
            <img id="rewardCode" src="/img/wechat.jpg" alt="打赏二维码">
        </div>
        
        <label class="reward-toggle">
            <input id="rewardToggle" type="checkbox" class="reward-toggle-check"
                data-wechat="/img/wechat.jpg" data-alipay="/img/alipay.jpg">
            <div class="reward-toggle-ctrol">
                <span class="reward-toggle-item wechat">微信</span>
                <span class="reward-toggle-label"></span>
                <span class="reward-toggle-item alipay">支付宝</span>
            </div>
        </label>
        
    </div>
</div>



</div>

        <footer class="footer">
    <div class="top">
        
<p>
    <span id="busuanzi_container_site_uv" style='display:none'>
        站点总访客数：<span id="busuanzi_value_site_uv"></span>
    </span>
    <span id="busuanzi_container_site_pv" style='display:none'>
        站点总访问量：<span id="busuanzi_value_site_pv"></span>
    </span>
</p>


        <p>
            
                <span><a href="/atom.xml" target="_blank" class="rss" title="rss"><i class="icon icon-lg icon-rss"></i></a></span>
            
            <span>本部落格係採用<a rel="license" href="https://creativecommons.org/licenses/by/4.0/deed.zh_TW">創用 CC 姓名標示 4.0 國際 授權條款授權</a></span>
        </p>
    </div>
    <div class="bottom">
        <p><span>刘明辉 &copy; 2015 - 2019</span>
            <span>
                
                Power by <a href="http://hexo.io/" target="_blank">Hexo</a> Theme <a href="https://github.com/yscoder/hexo-theme-indigo" target="_blank">indigo</a>
            </span>
        </p>
    </div>
</footer>

    </main>
    <div class="mask" id="mask"></div>
<a href="javascript:;" id="gotop" class="waves-effect waves-circle waves-light"><span class="icon icon-lg icon-chevron-up"></span></a>



<div class="global-share" id="globalShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://yoursite.com/2019/09/01/deeplearning-ai学习笔记（十五）自然语言处理与词嵌入/&title=《deeplearning.ai学习笔记（十五）自然语言处理与词嵌入》 — 一只程序喵&pic=http://yoursite.com/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://yoursite.com/2019/09/01/deeplearning-ai学习笔记（十五）自然语言处理与词嵌入/&title=《deeplearning.ai学习笔记（十五）自然语言处理与词嵌入》 — 一只程序喵&source=词的表示one-hot向量表示表示方法：

首先建立一个较大的词汇表（例如10000）;

然后使用one-hot的方式对每个单词进行编码。" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://yoursite.com/2019/09/01/deeplearning-ai学习笔记（十五）自然语言处理与词嵌入/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《deeplearning.ai学习笔记（十五）自然语言处理与词嵌入》 — 一只程序喵&url=http://yoursite.com/2019/09/01/deeplearning-ai学习笔记（十五）自然语言处理与词嵌入/&via=http://yoursite.com" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://yoursite.com/2019/09/01/deeplearning-ai学习笔记（十五）自然语言处理与词嵌入/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>


<div class="page-modal wx-share" id="wxShare">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <p>扫一扫，分享到微信</p>
    <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAPYAAAD2CAAAAADAeSUUAAADHklEQVR42u3aUW7jMAwE0N7/0tkDLOwOOSkQK89fQRHLfgrQgUj+/MTX6/a6+s79av9/vnri1V3Js6oLGxsb+yHsnnT//Z6aPD23YGNjY5/K3sVMvkK+fXn6TDcUGxsbG/s+MKYRsju63N+LjY2Njd2wpy/RHzw+IsCwsbGxP4bdAN71l6Zs9Ie1NGxsbOyPZ7+riPMJn/+kv42NjY39wezX8Orbw/mxZxpOAwU2Njb2Qew8AKbbkReA8u2YbuX9MBA2Njb2Sexd+b4p4iSlq2asZ3kvNjY29qHsvGW7a/32Iz75j/Qz3S1sbGzsh7ATUvIqSXhMV8ufMj4aYWNjYx/Bnh458iGbZlAyj9JlVmNjY2MfxE5K9tOY2YViv0FVVwQbGxv7aHb+/79pLUwbw8nmVgGGjY2N/Sh2wpiCp9GVt2Z3A0aX92JjY2M/nN289DRUpr/DrpwUbRY2Njb2cex3PT4vP03Hg/KjRXQvNjY29qHsaSEpH6mZMnbbOghXbGxs7IPY05r5rqX63g3dPWUwwYSNjY39cPauJLRr9+7aCVUPZLrr2NjY2I9iT18xGZfZlavyFZrNwsbGxj6DvfvX35R4miJUvvIvd2FjY2Mfx85viJKw6KXmIdoEGzY2NvZ57Hy55uWSGMuPNLsxIGxsbOzz2NPomrYTpqMz+XFletd4MggbGxv7Uexdq+Aen0TddJJoF5/Y2NjY38N+xdeuTZts346RRNplmGFjY2Mfx87LPYOl65jJn7XMamxsbOyHs/NxmR2sCZu+AfxLLQ0bGxv7IHYeRfkg5rTQn+DzuMLGxsb+Tva09DN9iV2DdhpskQIbGxv7UHZ/kEheqNn0cbXs6i5sbGzsI9iv4TVtJ+Tlp7xUlP9Ul5uOjY2NfRB7FwB9U3baiugPLUs8NjY29kPYedM0OZDkg0G7FXYBho2Njf0N7D6K+oL+rs2QbzE2NjY2dlMqSlZOjiW7A8kyS7GxsbG/gN3UrpI2QH9dro+NjY19HDs5MPSl+aZl2xyHqloaNjY29sez31WObw4bu1JRP1SEjY2N/XD2P3qX2YU+3u3rAAAAAElFTkSuQmCC" alt="微信分享二维码">
</div>




    <script src="//cdn.bootcss.com/node-waves/0.7.4/waves.min.js"></script>
<script>
var BLOG = { ROOT: '/', SHARE: true, REWARD: true };


</script>

<script src="//unpkg.com/hexo-theme-material-indigo@latest/js/main.min.js"></script>


<div class="search-panel" id="search-panel">
    <ul class="search-result" id="search-result"></ul>
</div>
<template id="search-tpl">
<li class="item">
    <a href="{path}" class="waves-block waves-effect">
        <div class="title ellipsis" title="{title}">{title}</div>
        <div class="flex-row flex-middle">
            <div class="tags ellipsis">
                {tags}
            </div>
            <time class="flex-col time">{date}</time>
        </div>
    </a>
</li>
</template>

<script src="//unpkg.com/hexo-theme-material-indigo@latest/js/search.min.js" async></script>






<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>



<script>
(function() {
    var OriginTitile = document.title, titleTime;
    document.addEventListener('visibilitychange', function() {
        if (document.hidden) {
            document.title = '一只程序喵 | 刘明辉的个人博客';
            clearTimeout(titleTime);
        } else {
            document.title = '一只程序喵 | 刘明辉的个人博客';
            titleTime = setTimeout(function() {
                document.title = OriginTitile;
            },2000);
        }
    });
})();
</script>



<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

</body>
</html>
