{"meta":{"title":"一只程序喵","subtitle":"刘明辉的个人博客","description":null,"author":"刘明辉","url":"http://yoursite.com","root":"/"},"pages":[{"title":"","date":"2019-08-08T07:34:55.547Z","updated":"2019-08-08T07:34:55.546Z","comments":false,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"About","date":"2019-08-08T12:10:35.000Z","updated":"2019-08-09T09:10:45.569Z","comments":true,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":""},{"title":"custom","date":"2019-08-08T07:46:19.000Z","updated":"2019-08-08T07:46:19.813Z","comments":true,"path":"custom/index.html","permalink":"http://yoursite.com/custom/index.html","excerpt":"","text":""},{"title":"","date":"2019-08-08T07:34:30.115Z","updated":"2019-08-08T07:34:30.115Z","comments":false,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"deeplearning-ai学习笔记（二）神经网络基础——以二值分类问题的逻辑回归模型为例","slug":"deeplearning-ai学习笔记（二）神经网络基础——以二值分类问题的逻辑回归模型为例","date":"2019-08-11T02:18:57.000Z","updated":"2019-08-12T13:41:46.528Z","comments":true,"path":"2019/08/11/deeplearning-ai学习笔记（二）神经网络基础——以二值分类问题的逻辑回归模型为例/","link":"","permalink":"http://yoursite.com/2019/08/11/deeplearning-ai学习笔记（二）神经网络基础——以二值分类问题的逻辑回归模型为例/","excerpt":"","text":"二值分类（Binary Classification ）问题描述例如：猫咪检测器（Cat vs Non-Cat ） 目标是训练一个分类器，对于输入的照片，如果它是一张猫咪的照片就输出1，否则输出0。 输入将一张RGB三通道彩色图像展开为一个长的列向量做为输入。 若图片尺寸为64*64，则向量的维度n(x)=64*64*3=12288。 输入特征向量x 输出对于二值分类问题，输出结果只有两个——0或1。 输出标签的向量形式：（m维列向量） \\begin{bmatrix} y^{(1)}\\\\ y^{(2)} \\\\ \\vdots \\\\ y^{(i)} \\end{bmatrix}训练数据 单个样本由一对（x,y）表示，其中x是一个n(x)维的特征向量 ，y是取值为0或1的标签。 训练集包含m个训练样本（用小写m代表训练集的样本总数），(x(i),y(i)) 表示第i个样本的输入和输出。 写成矩阵形式如下：（X.shape=n_x*m） \\begin{bmatrix} \\vdots & \\vdots & \\vdots & \\vdots \\\\ x^{(1)} & x^{(2)} & \\cdots & x^{(i)} \\\\ \\vdots & \\vdots & \\vdots & \\vdots \\end{bmatrix}逻辑回归（Logistic Regression ）模型逻辑回归是一种用于解决输出是0/1的监督学习问题的学习算法，它使得预测值与训练数据之间的偏差最小。 定义以Cat vs No - cat为例： 把一张图片展开为特征向量x做为输入，算法将估计这张图片中包含猫咪的概率。 Given~x~,~\\hat{y}=P(y=1|x),where~0\\leq\\hat{y}\\leq1 逻辑回归模型 其中各个参数的意义： x：输入特征向量（一个n_x维列向量） y：训练集标签，y∈0,1 weights——w：权重（一个n_x维列向量） threshold——b：偏置量（一个实数） Sigmoid 函数： \\sigma(z)=\\frac{1}{1+e^{-z}} Sigmoid函数图像 该函数的特点： 当z趋于+∞，函数值趋于1 当z趋于-∞，函数值趋于0 当z=0，函数值为0.5 作用：将输出值限制在[0,1]，使其可以表示一个概率值 代价函数（Cost function ）损失函数 vs 代价函数 损失函数（Loss function）是定义在单个样本上的，算的是一个样本的误差。 代价函数（Cost function）是定义在整个训练集上的，是所有样本误差的平均，也就是损失函数的平均。 代价函数用来衡量参数在整个模型中的作用效果。 逻辑回归的损失函数 损失函数 推导过程： 我们希望算法输出ŷ 表示当给定输入特征x的时候y=1的概率。换句话说，如果y等于1，那么p(y|x)就等于ŷ ；相反地，当y=0时，p(y|x)就等于1-ŷ 。因此，如果ŷ表示当y=1的概率，那么1-ŷ就表示y=0的概率。 可以定义p(y|x)如下： 由于y只有0和1两种取值，因此上面的两个方程可以归纳为如下一个方程： p(y|x)=ŷ^y*(1-ŷ)^{1-y}因为对数函数是一个绝对的单调递增函数，最大化log(p(y|x))会得出和最大化p(y|x)相似的结果，因此可以取对数，简化公式。 log(ŷ^y*(1-ŷ)^{1-y}) =ylog(ŷ)+(1-y)log(1-ŷ)又因为通常在训练一个学习算法的时候，我们想要让概率变大。而在逻辑回归中，我们想要最小化L(ŷ,y)这个损失函数。最小化损失函数相当于最大化概率的对数，所以需要加一个负号。 L(ŷ,y)=-(ylog(ŷ)+(1-y)log(1-ŷ))逻辑回归的代价函数 代价函数 推导过程： 假设取出的训练样本相互独立，或者说服从独立同分布 (I.I.D: Independent and Identically Distributed) ，这些样本的概率就是各项概率的乘积。 给定X(i)从i=1到m时p(y(i))的乘积： p(y^{(1)})*p(y^{(2)})*\\cdots*p(y^{(m)})最大化这个式子本身和最大化它的对数效果相同，所以取对数： log(p(y^{(1)}))+log(p(y^{(2)}))+\\cdots+log(p(y^{(m)}))=\\sum_{i=1}^{m}log(p(y^{(i)}))=-\\sum_{i=1}^{m}L(ŷ^{(i)},y^{(i)})根据最大似然估计原理，选择能使该式最大化的参数。因为要最小化代价函数，而不是最大化似然值，所以要去掉这个负号。最后为了方便起见，使这些数值处于更好的尺度上，在前面添加一个缩放系数1/m。 综上，代价函数为： J(w,b)=\\frac{1}{m}\\sum_{i=1}^{m}L(ŷ^{(i)},y^{(i)})=-\\frac{1}{m}\\sum_{i=1}^{m}[y^{(i)}log(ŷ^{(i)})+(1-y^{(i)})log(1-ŷ^{(i)})]优化逻辑回归模型时，我们试着去找参数w和b，以此来缩小代价函数J， 逻辑回归可被视为一个非常小的神经网络 。 训练过程——梯度下降（Gradient Descent）算法算法思想在逻辑回归问题中，代价函数J是一个凸函数(convex function) 。为了去找到优的参数值，首先用一些初始值（0或随机）来初始化w和b，因为函数是凸函数，无论在哪里初始化，应该达到同一点或大致相同的点。 梯度下降法以初始点开始，然后朝最陡的下坡方向走一步，这是梯度下降的一次迭代。经过多次迭代收敛到全局最优值或接近全局最优值。 梯度下降示意图 参数的迭代公式： w:=w-\\alpha\\frac{\\partial{J(w,b)}}{\\partial{w}} b:=b-\\alpha\\frac{\\partial{J(w,b)}}{\\partial{b}}数学基础：计算图（Computation Graph）以下式为例 J(a,b,c)=3(a+bc)其计算图为： 计算图 计算图计算（前向传播过程）前向传播即为计算一个样本下J的值，计算过程如下： u=b*c=3*2=6 \\\\ v=a+u=5+6=11 \\\\ J=3V=33计算图求导（反向传播过程）反向传播即根据求导的链式法则，求解最终输出变量J对各个变量的导数。 计算过程如下： \\frac{dJ}{dv}=\\frac{d(3v)}{dv}=3 \\\\ \\frac{dJ}{da}=\\frac{dJ}{dv}*\\frac{dv}{da}=3*\\frac{d(a+u)}{da}=3 \\\\ \\frac{dJ}{du}=\\frac{dJ}{dv}*\\frac{dv}{du}=3*\\frac{d(a+u)}{du}=3 \\\\ \\frac{dJ}{db}=\\frac{dJ}{du}*\\frac{du}{db}=3*\\frac{d(bc)}{db}=3c=3*2=6 \\\\ \\frac{dJ}{dc}=\\frac{dJ}{du}*\\frac{du}{dc}=3*\\frac{d(bc)}{db}=3b=3*3=9为简化表示，将dJ/dval简记为dval，则在本例中： da=3,db=6,dc=9逻辑回归模型的前向传播和反向传播过程（单个样本）逻辑回归模型的计算图 逻辑回归模型的计算图 逻辑回归模型的前向传播顺序计算即可。 逻辑回归模型的反向传播计算过程如下： “da”=\\frac{dL}{da}=-\\frac{y}{a}+\\frac{1-y}{1-a} \\\\ “dz”=\\frac{dL}{dz}=\\frac{dL}{da}*\\frac{da}{dz}=(-\\frac{y}{a}+\\frac{1-y}{1-a})*\\frac{d\\sigma(z)}{dz} = a(1-a) \\\\ “dw_1”=\\frac{dL}{dw_1}=\\frac{dL}{dz}*\\frac{dz}{dw_1}=x_1*“dz” \\\\ “dw_2”=\\frac{dL}{dw_2}=\\frac{dL}{dz}*\\frac{dz}{dw_2}=x_2*“dz” \\\\ “db”=\\frac{dL}{db}=\\frac{dL}{dz}*\\frac{dz}{db}=“dz”非向量化的逻辑回归训练过程 非向量化的逻辑回归训练过程 缺点：显式地使用了两层for循环，时间效率低 解决方法：采用向量化的方法，可以极大地提高时间效率 向量化（Vectorizing）的逻辑回归训练过程 Z = W^{T}*X+b = np.dot(W.T,X)+b //此处存在python的广播机制 \\\\ A = σ(Z) \\\\ dZ = A - Y \\\\ dW = \\frac{1}{m}XdZ^{T}\\\\ db = \\frac{1}{m}*np.sum(dZ)\\\\ W := W - \\alpha dW\\\\ b := b - \\alpha db注意：此为一次迭代的过程，多次迭代使用显示循环不可避免。","categories":[],"tags":[{"name":"学习笔记","slug":"学习笔记","permalink":"http://yoursite.com/tags/学习笔记/"},{"name":"deeplearning.ai","slug":"deeplearning-ai","permalink":"http://yoursite.com/tags/deeplearning-ai/"}]},{"title":"那些既熟悉又陌生的C/C++知识点（长期更新）","slug":"那些既熟悉又陌生的C-C-知识点（长期更新）","date":"2019-08-09T15:21:44.000Z","updated":"2019-08-09T16:04:41.156Z","comments":true,"path":"2019/08/09/那些既熟悉又陌生的C-C-知识点（长期更新）/","link":"","permalink":"http://yoursite.com/2019/08/09/那些既熟悉又陌生的C-C-知识点（长期更新）/","excerpt":"","text":"头文件的等价写法在C++标准中，stdio.h更推荐使用等价写法：cstdio，也就是在前面加一个c，然后去掉.h即可。 123/* 以下两种写法等价 */#include&lt;stdio.h&gt;#include&lt;cstdio&gt; 整型变量类型表示数的范围 整型int：32位整数/绝对值在10^9范围以内的整数都可以定义为int型 长整型long long：64位整数/10^18以内（如10^10）的整数就要定义为long long型 long long型的使用long long型赋大于2^31-1的初值，需要在初值后面加上LL 经常利用typedef用LL来代替long long，以避免在程序中大量出现long long而降低编码的效率。 12345678#include&lt;cstdio&gt;typedef long long LL; //给long long起个别名LLint main()&#123; LL a = 123456789012345LL, b = 234567890123456LL; printf(\"%lld\\n\", a + b); return 0;&#125; 浮点数的存储类型对于浮点型，不要使用float，碰到浮点型的数据都应该用double来存储。 单精度float：有效精度只有6~7位 双精度double：有效精度有15~16位 double型的输入输出格式 输出格式：%f（与float型相同） 输入格式：scanf中是%lf 注：有些系统如果把输出格式写成%lf也不会出错，但尽量还是按标准来","categories":[],"tags":[{"name":"C/C++","slug":"C-C","permalink":"http://yoursite.com/tags/C-C/"}]},{"title":"deeplearning.ai学习笔记（一）深度学习引言","slug":"deeplearning-ai学习笔记（一）深度学习引言","date":"2019-08-09T06:30:38.000Z","updated":"2019-08-09T09:05:27.062Z","comments":true,"path":"2019/08/09/deeplearning-ai学习笔记（一）深度学习引言/","link":"","permalink":"http://yoursite.com/2019/08/09/deeplearning-ai学习笔记（一）深度学习引言/","excerpt":"AI is the new Electricity. ——吴恩达（Andrew Ng） 大约在一百年前，社会的电气化改变了每个主要行业。而如今我们见到了 AI令人惊讶的能量会产生同样巨大的转变。显然 AI的各个分支中，发展最为迅速的就是深度学习（deep learning）。而深度学习，一般指的是训练神经网络（有时是非常非常大/深的神经网络）。","text":"AI is the new Electricity. ——吴恩达（Andrew Ng） 大约在一百年前，社会的电气化改变了每个主要行业。而如今我们见到了 AI令人惊讶的能量会产生同样巨大的转变。显然 AI的各个分支中，发展最为迅速的就是深度学习（deep learning）。而深度学习，一般指的是训练神经网络（有时是非常非常大/深的神经网络）。 何为神经网络（What is a neural network?）引例：房价预测（Housing Price Prediction）如下图，对于单个因素（如房屋面积）的房屋预测，可以用ReLU函数进行拟合，相当于一个只有一个神经元的神经网络。 ReLU函数拟合 用房子的大小 x 作为神经网络的输入，它进入到这个节点(这个小圈)中 ，然后这个小圈就输出了房价 y 。所以这个小圈，也就是一个神经网络中的一个神经元，就会执行上图中画出的这个方程。 单个神经元的神经网络 一个很大的神经网络是由许多这样的单一神经元叠加在一起组成。比如下面这个例子，我们不仅仅根据房屋大小来预测房屋价格，我们引入其他特征量。能够容纳的家庭人口也会影响房屋价格 ，这个因素其实是取决于房屋大小以及卧室的数量这两个因素决定了这个房子是否能够容纳你的家庭 。 多因素房价预测 所以在这个例子中： x 表示所有这四个输入 (房屋大小、卧室数量、邮政编码、富裕程度) y表示试图去预测的价格 每一个小圆圈是ReLU 函数或者别的一些非线性函数 这就是最基本的神经网络。 简单神经网络 深度学习：监督学习的一种（Supervised Learning with Neural Networks）监督学习与非监督学习监督学习：给定标记好的训练样本集，如回归问题、分类问题 非监督学习：给定样本集，发现特征数据中的分布结构，如聚类问题 监督学习 非监督学习 深度学习在监督学习中的应用 &amp; 常用神经网络 深度学习的应用 几种常见神经网络 结构化数据与非结构化数据 结构化数据与非结构化数据示例 深度学习兴起的原因（Why is Deep Learning taking off?） Scale drives deep learning progress.——吴恩达（Andrew Ng） 深度学习的性能 x-axis is the amount of data （数据量） y-axis (vertical axis) is the performance of the algorithm. （算法性能） 数据量与算法性能关系 当数据量比较少时，深度学习方法性能不一定优于经典机器学习方法。 通过增加数据量和神经网络规模可提升深度学习方法的性能。 深度学习兴起的原因 Data：信息化社会产生了巨大的数据量 Computation：现代计算机计算性能的极大提升（GPU与CPU） Algorithms：算法的优化进一步提升了性能（如ReLU的使用）","categories":[],"tags":[{"name":"学习笔记","slug":"学习笔记","permalink":"http://yoursite.com/tags/学习笔记/"},{"name":"deeplearning.ai","slug":"deeplearning-ai","permalink":"http://yoursite.com/tags/deeplearning-ai/"}]}]}