<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>一只程序喵</title>
  
  <subtitle>刘明辉的个人博客</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-08-11T15:02:08.738Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>刘明辉</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>deeplearning-ai学习笔记（二）神经网络基础——以二值分类问题的逻辑回归模型为例</title>
    <link href="http://yoursite.com/2019/08/11/deeplearning-ai%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80%E2%80%94%E2%80%94%E4%BB%A5%E4%BA%8C%E5%80%BC%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E7%9A%84%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%E4%B8%BA%E4%BE%8B/"/>
    <id>http://yoursite.com/2019/08/11/deeplearning-ai学习笔记（二）神经网络基础——以二值分类问题的逻辑回归模型为例/</id>
    <published>2019-08-11T02:18:57.000Z</published>
    <updated>2019-08-11T15:02:08.738Z</updated>
    
    <content type="html"><![CDATA[<h2 id="二值分类（Binary-Classification-）问题"><a href="#二值分类（Binary-Classification-）问题" class="headerlink" title="二值分类（Binary Classification ）问题"></a>二值分类（Binary Classification ）问题</h2><h3 id="描述"><a href="#描述" class="headerlink" title="描述"></a>描述</h3><p>例如：<strong>猫咪检测器</strong>（Cat vs Non-Cat ）</p><p>目标是训练一个分类器，对于输入的照片，如果它是一张猫咪的照片就输出1，否则输出0。</p><h3 id="输入"><a href="#输入" class="headerlink" title="输入"></a>输入</h3><p>将一张RGB三通道彩色图像展开为一个长的列向量做为输入。</p><p>若图片尺寸为64*64，则向量的维度n(x)=64*64*3=12288。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://s2.ax1x.com/2019/08/11/exSOED.md.png" alt="输入特征向量x" title>                </div>                <div class="image-caption">输入特征向量x</div>            </figure>](https://imgc### 输出对于二值分类问题，输出结果只有两个——**0或1**。输出标签的向量形式：（m维列向量）$$\begin{bmatrix}y^{(1)}\\y^{(2)}  \\\vdots \\y^{(i)}  \end{bmatrix}$$### 训练数据- 单个样本由一对（x,y）表示，其中x是一个n(x)维的特征向量 ，y是取值为0或1的标签。- 训练集包含m个训练样本（用小写m代表训练集的样本总数），(x(i),y(i)) 表示第i个样本的输入和输出。- 写成矩阵形式如下：（X.shape=n_x\*m）$$\begin{bmatrix}\vdots & \vdots & \vdots & \vdots \\x^{(1)} & x^{(2)} & \cdots & x^{(i)} \\\vdots & \vdots & \vdots & \vdots \end{bmatrix}$$## **逻辑回归**（Logistic Regression ）模型逻辑回归是一种用于解决输出是0/1的监督学习问题的学习算法，它使得预测值与训练数据之间的偏差最小。### 定义以Cat vs No - cat为例：把一张图片展开为特征向量x做为输入，算法将估计这张图片中包含猫咪的概率。$$Given~x~,~\hat{y}=P(y=1|x),where~0\leq\hat{y}\leq1$$<figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://s2.ax1x.com/2019/08/11/exSWEF.md.png" alt="逻辑回归模型" title>                </div>                <div class="image-caption">逻辑回归模型</div>            </figure><p>其中各个参数的意义：</p><ul><li>x：输入特征向量（一个n_x维列向量）</li><li>y：训练集标签，y∈0,1</li><li>weights——w：权重（一个n_x维列向量）</li><li>threshold——b：偏置量（一个实数）</li></ul><blockquote><p>Sigmoid 函数：</p><script type="math/tex; mode=display">\sigma(z)=\frac{1}{1+e^{-z}}</script><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://s2.ax1x.com/2019/08/11/exSog1.md.png" alt="Sigmoid函数图像" title>                </div>                <div class="image-caption">Sigmoid函数图像</div>            </figure><p>该函数的特点：</p><ul><li>当z趋于+∞，函数值趋于1</li><li>当z趋于-∞，函数值趋于0</li><li>当z=0，函数值为0.5</li></ul><p>作用：将输出值限制在[0,1]，使其可以表示一个概率值</p></blockquote><h3 id="代价函数（Cost-function-）"><a href="#代价函数（Cost-function-）" class="headerlink" title="代价函数（Cost function ）"></a>代价函数（Cost function ）</h3><h4 id="损失函数-vs-代价函数"><a href="#损失函数-vs-代价函数" class="headerlink" title="损失函数 vs 代价函数"></a>损失函数 vs 代价函数</h4><blockquote><p>损失函数（Loss function）是定义在<strong>单个样本</strong>上的，算的是一个样本的误差。</p><p>代价函数（Cost function）是定义在<strong>整个训练集</strong>上的，是所有样本误差的平均，也就是损失函数的平均。</p></blockquote><p><strong>代价函数用来衡量参数在整个模型中的作用效果。</strong></p><h4 id="逻辑回归的损失函数"><a href="#逻辑回归的损失函数" class="headerlink" title="逻辑回归的损失函数"></a>逻辑回归的损失函数</h4><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://s2.ax1x.com/2019/08/11/exSh4J.md.png" alt="损失函数" title>                </div>                <div class="image-caption">损失函数</div>            </figure><p><strong>推导过程：</strong></p><p>我们希望算法输出ŷ 表示当给定输入特征x的时候y=1的概率。换句话说，如果y等于1，那么p(y|x)就等于ŷ ；相反地，当y=0时，p(y|x)就等于1-ŷ 。因此，如果ŷ表示当y=1的概率，那么1-ŷ就表示y=0的概率。</p><p>可以定义p(y|x)如下：</p><p>由于y只有0和1两种取值，因此上面的两个方程可以归纳为如下一个方程：</p><script type="math/tex; mode=display">p(y|x)=ŷ^y*(1-ŷ)^{1-y}</script><p>因为对数函数是一个绝对的单调递增函数，最大化log(p(y|x))会得出和最大化p(y|x)相似的结果，因此可以取对数，简化公式。</p><script type="math/tex; mode=display">log(ŷ^y*(1-ŷ)^{1-y}) =ylog(ŷ)+(1-y)log(1-ŷ)</script><p>又因为通常在训练一个学习算法的时候，我们想要让概率变大。而在逻辑回归中，我们想要最小化L(ŷ,y)这个损失函数。最小化损失函数相当于最大化概率的对数，所以需要加一个负号。</p><script type="math/tex; mode=display">L(ŷ,y)=-(ylog(ŷ)+(1-y)log(1-ŷ))</script><h4 id="逻辑回归的代价函数"><a href="#逻辑回归的代价函数" class="headerlink" title="逻辑回归的代价函数"></a>逻辑回归的代价函数</h4><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://s2.ax1x.com/2019/08/11/exSI3R.md.png" alt="代价函数" title>                </div>                <div class="image-caption">代价函数</div>            </figure><p><strong>推导过程：</strong></p><p>假设取出的训练样本相互独立，或者说服从独立同分布 (I.I.D: Independent and Identically Distributed) ，这些样本的概率就是各项概率的乘积。</p><p>给定X(i)从i=1到m时p(y(i))的乘积：</p><script type="math/tex; mode=display">p(y^{(1)})*p(y^{(2)})*\cdots*p(y^{(m)})</script><p>最大化这个式子本身和最大化它的对数效果相同，所以取对数：</p><script type="math/tex; mode=display">log(p(y^{(1)}))+log(p(y^{(2)}))+\cdots+log(p(y^{(m)}))=\sum_{i=1}^{m}log(p(y^{(i)}))=-\sum_{i=1}^{m}L(ŷ^{(i)},y^{(i)})</script><p>根据最大似然估计原理，选择能使该式最大化的参数。因为要最小化代价函数，而不是最大化似然值，所以要去掉这个负号。最后为了方便起见，使这些数值处于更好的尺度上，在前面添加一个缩放系数1/m。</p><p>综上，代价函数为：</p><script type="math/tex; mode=display">J(w,b)=\frac{1}{m}\sum_{i=1}^{m}L(ŷ^{(i)},y^{(i)})=-\frac{1}{m}\sum_{i=1}^{m}[y^{(i)}log(ŷ^{(i)})+(1-y^{(i)})log(1-ŷ^{(i)})]</script><p><strong>优化逻辑回归模型时，我们试着去找参数w和b，以此来缩小代价函数J， 逻辑回归可被视为一个非常小的神经网络 。</strong></p><h2 id="训练过程——梯度下降（Gradient-Descent）算法"><a href="#训练过程——梯度下降（Gradient-Descent）算法" class="headerlink" title="训练过程——梯度下降（Gradient Descent）算法"></a>训练过程——<strong>梯度下降</strong>（Gradient Descent）算法</h2><h3 id="算法思想"><a href="#算法思想" class="headerlink" title="算法思想"></a>算法思想</h3><p>在逻辑回归问题中，代价函数J是一个凸函数(convex function) 。为了去找到优的参数值，首先用一些初始值（0或随机）来初始化w和b，因为函数是凸函数，无论在哪里初始化，应该达到同一点或大致相同的点。</p><p>梯度下降法以初始点开始，然后朝最陡的下坡方向走一步，这是梯度下降的一次迭代。经过多次迭代收敛到全局最优值或接近全局最优值。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://s2.ax1x.com/2019/08/11/exS5C9.md.png" alt="梯度下降示意图" title>                </div>                <div class="image-caption">梯度下降示意图</div>            </figure><p>参数的迭代公式：</p><script type="math/tex; mode=display">w:=w-\alpha\frac{\partial{J(w,b)}}{\partial{w}}</script><script type="math/tex; mode=display">b:=b-\alpha\frac{\partial{J(w,b)}}{\partial{b}}</script><h3 id="数学基础：计算图（Computation-Graph）"><a href="#数学基础：计算图（Computation-Graph）" class="headerlink" title="数学基础：计算图（Computation Graph）"></a>数学基础：计算图（Computation Graph）</h3><p>以下式为例</p><script type="math/tex; mode=display">J(a,b,c)=3(a+bc)</script><p>其计算图为：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://s2.ax1x.com/2019/08/11/exSHu6.md.png" alt="计算图" title>                </div>                <div class="image-caption">计算图</div>            </figure><h4 id="计算图计算（前向传播过程）"><a href="#计算图计算（前向传播过程）" class="headerlink" title="计算图计算（前向传播过程）"></a>计算图计算（前向传播过程）</h4><p>前向传播即为计算一个样本下J的值，计算过程如下：</p><script type="math/tex; mode=display">u=b*c=3*2=6 \\v=a+u=5+6=11 \\J=3V=33</script><h4 id="计算图求导（反向传播过程）"><a href="#计算图求导（反向传播过程）" class="headerlink" title="计算图求导（反向传播过程）"></a>计算图求导（反向传播过程）</h4><p>反向传播即根据<strong>求导的链式法则</strong>，求解最终输出变量J对各个变量的导数。</p><p>计算过程如下：</p><script type="math/tex; mode=display">\frac{dJ}{dv}=\frac{d(3v)}{dv}=3 \\\frac{dJ}{da}=\frac{dJ}{dv}*\frac{dv}{da}=3*\frac{d(a+u)}{da}=3 \\\frac{dJ}{du}=\frac{dJ}{dv}*\frac{dv}{du}=3*\frac{d(a+u)}{du}=3 \\\frac{dJ}{db}=\frac{dJ}{du}*\frac{du}{db}=3*\frac{d(bc)}{db}=3c=3*2=6 \\\frac{dJ}{dc}=\frac{dJ}{du}*\frac{du}{dc}=3*\frac{d(bc)}{db}=3b=3*3=9</script><p>为简化表示，将dJ/dval简记为dval，则在本例中：</p><script type="math/tex; mode=display">da=3,db=6,dc=9</script><h3 id="逻辑回归模型的前向传播和反向传播过程（单个样本）"><a href="#逻辑回归模型的前向传播和反向传播过程（单个样本）" class="headerlink" title="逻辑回归模型的前向传播和反向传播过程（单个样本）"></a>逻辑回归模型的前向传播和反向传播过程（单个样本）</h3><h4 id="逻辑回归模型的计算图"><a href="#逻辑回归模型的计算图" class="headerlink" title="逻辑回归模型的计算图"></a>逻辑回归模型的计算图</h4><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://s2.ax1x.com/2019/08/11/exSbDK.md.png" alt="逻辑回归模型的计算图" title>                </div>                <div class="image-caption">逻辑回归模型的计算图</div>            </figure><h4 id="逻辑回归模型的前向传播"><a href="#逻辑回归模型的前向传播" class="headerlink" title="逻辑回归模型的前向传播"></a>逻辑回归模型的前向传播</h4><p>顺序计算即可。</p><h4 id="逻辑回归模型的反向传播"><a href="#逻辑回归模型的反向传播" class="headerlink" title="逻辑回归模型的反向传播"></a>逻辑回归模型的反向传播</h4><p>计算过程如下：</p><script type="math/tex; mode=display">“da”=\frac{dL}{da}=-\frac{y}{a}+\frac{1-y}{1-a} \\“dz”=\frac{dL}{dz}=\frac{dL}{da}*\frac{da}{dz}=(-\frac{y}{a}+\frac{1-y}{1-a})*\frac{d\sigma(z)}{dz} = a(1-a) \\“dw_1”=\frac{dL}{dw_1}=\frac{dL}{dz}*\frac{dz}{dw_1}=x_1*“dz” \\“dw_2”=\frac{dL}{dw_2}=\frac{dL}{dz}*\frac{dz}{dw_2}=x_2*“dz” \\“db”=\frac{dL}{db}=\frac{dL}{dz}*\frac{dz}{db}=“dz”</script><h3 id="非向量化的逻辑回归训练过程"><a href="#非向量化的逻辑回归训练过程" class="headerlink" title="非向量化的逻辑回归训练过程"></a>非向量化的逻辑回归训练过程</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://s2.ax1x.com/2019/08/11/exSqHO.md.png" alt="非向量化的逻辑回归训练过程" title>                </div>                <div class="image-caption">非向量化的逻辑回归训练过程</div>            </figure><ul><li>缺点：显式地使用了两层for循环，时间效率低</li><li>解决方法：采用向量化的方法，可以极大地提高时间效率</li></ul><h3 id="向量化（Vectorizing）的逻辑回归训练过程"><a href="#向量化（Vectorizing）的逻辑回归训练过程" class="headerlink" title="向量化（Vectorizing）的逻辑回归训练过程"></a>向量化（Vectorizing）的逻辑回归训练过程</h3><script type="math/tex; mode=display">Z = W^{T}*X+b = np.dot(W.T,X)+b //此处存在python的广播机制 \\A = σ(Z) \\dZ = A - Y \\dW = \frac{1}{m}XdZ^{T}\\db = \frac{1}{m}*np.sum(dZ)\\W := W - \alpha dW\\b := b - \alpha db</script><p>注意：此为一次迭代的过程，多次迭代使用显示循环不可避免。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;二值分类（Binary-Classification-）问题&quot;&gt;&lt;a href=&quot;#二值分类（Binary-Classification-）问题&quot; class=&quot;headerlink&quot; title=&quot;二值分类（Binary Classification ）问题&quot;&gt;
      
    
    </summary>
    
    
      <category term="学习笔记" scheme="http://yoursite.com/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
      <category term="deeplearning.ai" scheme="http://yoursite.com/tags/deeplearning-ai/"/>
    
  </entry>
  
  <entry>
    <title>那些既熟悉又陌生的C/C++知识点（长期更新）</title>
    <link href="http://yoursite.com/2019/08/09/%E9%82%A3%E4%BA%9B%E6%97%A2%E7%86%9F%E6%82%89%E5%8F%88%E9%99%8C%E7%94%9F%E7%9A%84C-C-%E7%9F%A5%E8%AF%86%E7%82%B9%EF%BC%88%E9%95%BF%E6%9C%9F%E6%9B%B4%E6%96%B0%EF%BC%89/"/>
    <id>http://yoursite.com/2019/08/09/那些既熟悉又陌生的C-C-知识点（长期更新）/</id>
    <published>2019-08-09T15:21:44.000Z</published>
    <updated>2019-08-09T16:04:41.156Z</updated>
    
    <content type="html"><![CDATA[<h2 id="头文件的等价写法"><a href="#头文件的等价写法" class="headerlink" title="头文件的等价写法"></a>头文件的等价写法</h2><p>在C++标准中，<strong>stdio.h</strong>更推荐使用等价写法：<strong>cstdio</strong>，也就是在前面加一个<code>c</code>，然后去掉<code>.h</code>即可。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* 以下两种写法等价 */</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;cstdio&gt;</span></span></span><br></pre></td></tr></table></figure><h2 id="整型变量类型表示数的范围"><a href="#整型变量类型表示数的范围" class="headerlink" title="整型变量类型表示数的范围"></a>整型变量类型表示数的范围</h2><ul><li>整型<code>int</code>：<strong>32位</strong>整数/<strong>绝对值在10^9范围以内</strong>的整数都可以定义为int型</li><li>长整型<code>long long</code>：<strong>64位</strong>整数/<strong>10^18以内（如10^10）</strong>的整数就要定义为long long型</li></ul><h2 id="long-long型的使用"><a href="#long-long型的使用" class="headerlink" title="long long型的使用"></a>long long型的使用</h2><p>long long型赋<strong>大于2^31-1</strong>的初值，需要在初值后面加上LL</p><p>经常<strong>利用typedef用LL来代替long long</strong>，以避免在程序中大量出现long long而降低编码的效率。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;cstdio&gt;</span></span></span><br><span class="line"><span class="keyword">typedef</span> <span class="keyword">long</span> <span class="keyword">long</span> LL; <span class="comment">//给long long起个别名LL</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    LL a = <span class="number">123456789012345L</span>L, b = <span class="number">234567890123456L</span>L;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"%lld\n"</span>, a + b);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="浮点数的存储类型"><a href="#浮点数的存储类型" class="headerlink" title="浮点数的存储类型"></a>浮点数的存储类型</h2><p>对于浮点型，<strong>不要使用float，碰到浮点型的数据都应该用double来存储</strong>。</p><ul><li>单精度<code>float</code>：有效精度只有<strong>6~7位</strong></li><li>双精度<code>double</code>：有效精度有<strong>15~16位</strong></li></ul><h2 id="double型的输入输出格式"><a href="#double型的输入输出格式" class="headerlink" title="double型的输入输出格式"></a>double型的输入输出格式</h2><ul><li>输出格式：<code>%f</code>（与float型相同）</li><li>输入格式：scanf中是<code>%lf</code></li></ul><p>注：有些系统如果把输出格式写成%lf也不会出错，但尽量还是按标准来</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;头文件的等价写法&quot;&gt;&lt;a href=&quot;#头文件的等价写法&quot; class=&quot;headerlink&quot; title=&quot;头文件的等价写法&quot;&gt;&lt;/a&gt;头文件的等价写法&lt;/h2&gt;&lt;p&gt;在C++标准中，&lt;strong&gt;stdio.h&lt;/strong&gt;更推荐使用等价写法：&lt;str
      
    
    </summary>
    
    
      <category term="C/C++" scheme="http://yoursite.com/tags/C-C/"/>
    
  </entry>
  
  <entry>
    <title>deeplearning.ai学习笔记（一）深度学习引言</title>
    <link href="http://yoursite.com/2019/08/09/deeplearning-ai%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BC%95%E8%A8%80/"/>
    <id>http://yoursite.com/2019/08/09/deeplearning-ai学习笔记（一）深度学习引言/</id>
    <published>2019-08-09T06:30:38.000Z</published>
    <updated>2019-08-09T09:05:27.062Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>AI is the new Electricity. ——吴恩达（<a href="https://www.coursera.org/instructor/andrewng" target="_blank" rel="noopener">Andrew Ng</a>）</p></blockquote><p>大约在一百年前，社会的电气化改变了每个主要行业。而如今我们见到了 AI令人惊讶的能量会产生同样巨大的转变。显然 AI的各个分支中，发展最为迅速的就是<strong>深度学习</strong>（deep learning）。而<strong>深度学习</strong>，一般指的是训练<strong>神经网络</strong>（有时是非常非常大/深的神经网络）。</p><a id="more"></a><h2 id="何为神经网络（What-is-a-neural-network-）"><a href="#何为神经网络（What-is-a-neural-network-）" class="headerlink" title="何为神经网络（What is a neural network?）"></a>何为神经网络（What is a <strong>neural network</strong>?）</h2><h3 id="引例：房价预测（Housing-Price-Prediction）"><a href="#引例：房价预测（Housing-Price-Prediction）" class="headerlink" title="引例：房价预测（Housing Price Prediction）"></a>引例：房价预测（Housing Price Prediction）</h3><p>如下图，对于单个因素（如房屋面积）的房屋预测，可以用ReLU函数进行拟合，相当于一个只有一个神经元的神经网络。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://s2.ax1x.com/2019/08/09/ebRgtx.md.png" alt="ReLU函数拟合" title>                </div>                <div class="image-caption">ReLU函数拟合</div>            </figure><p>用房子的大小 x 作为神经网络的输入，它进入到这个节点(这个小圈)中 ，然后这个小圈就输出了房价 y 。所以这个小圈，也就是一个神经网络中的一个神经元，就会执行上图中画出的这个方程。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://s2.ax1x.com/2019/08/09/ebfBw9.png" alt="单个神经元的神经网络" title>                </div>                <div class="image-caption">单个神经元的神经网络</div>            </figure><p><strong>一个很大的神经网络是由许多这样的单一神经元叠加在一起组成。</strong>比如下面这个例子，我们不仅仅根据房屋大小来预测房屋价格，我们引入其他特征量。能够容纳的家庭人口也会影响房屋价格 ，这个因素其实是取决于房屋大小以及卧室的数量这两个因素决定了这个房子是否能够容纳你的家庭 。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://s2.ax1x.com/2019/08/09/ebfvwj.md.png" alt="多因素房价预测" title>                </div>                <div class="image-caption">多因素房价预测</div>            </figure><p>所以在这个例子中：</p><ul><li>x 表示所有这四个输入 (房屋大小、卧室数量、邮政编码、富裕程度)</li><li>y表示试图去预测的价格</li><li>每一个小圆圈是ReLU 函数或者别的一些非线性函数</li></ul><p>这就是最基本的神经网络。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://s2.ax1x.com/2019/08/09/ebftzT.md.png" alt="简单神经网络" title>                </div>                <div class="image-caption">简单神经网络</div>            </figure><h2 id="深度学习：监督学习的一种（Supervised-Learning-with-Neural-Networks）"><a href="#深度学习：监督学习的一种（Supervised-Learning-with-Neural-Networks）" class="headerlink" title="深度学习：监督学习的一种（Supervised Learning with Neural Networks）"></a>深度学习：监督学习的一种（Supervised Learning with Neural Networks）</h2><h3 id="监督学习与非监督学习"><a href="#监督学习与非监督学习" class="headerlink" title="监督学习与非监督学习"></a>监督学习与非监督学习</h3><p><code>监督学习</code>：给定标记好的训练样本集，如回归问题、分类问题</p><p><code>非监督学习</code>：给定样本集，发现特征数据中的分布结构，如聚类问题</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://s2.ax1x.com/2019/08/09/ebfayF.md.png" alt="监督学习" title>                </div>                <div class="image-caption">监督学习</div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://s2.ax1x.com/2019/08/09/ebfdL4.md.png" alt="非监督学习" title>                </div>                <div class="image-caption">非监督学习</div>            </figure><h3 id="深度学习在监督学习中的应用-amp-常用神经网络"><a href="#深度学习在监督学习中的应用-amp-常用神经网络" class="headerlink" title="深度学习在监督学习中的应用 &amp; 常用神经网络"></a>深度学习在监督学习中的应用 &amp; 常用神经网络</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://s2.ax1x.com/2019/08/09/ebfDoR.md.png" alt="深度学习的应用" title>                </div>                <div class="image-caption">深度学习的应用</div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://s2.ax1x.com/2019/08/09/ebfTYt.md.png" alt="几种常见神经网络" title>                </div>                <div class="image-caption">几种常见神经网络</div>            </figure><h3 id="结构化数据与非结构化数据"><a href="#结构化数据与非结构化数据" class="headerlink" title="结构化数据与非结构化数据"></a>结构化数据与非结构化数据</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://s2.ax1x.com/2019/08/09/ebfoFI.md.png" alt="结构化数据与非结构化数据示例" title>                </div>                <div class="image-caption">结构化数据与非结构化数据示例</div>            </figure><h2 id="深度学习兴起的原因（Why-is-Deep-Learning-taking-off-）"><a href="#深度学习兴起的原因（Why-is-Deep-Learning-taking-off-）" class="headerlink" title="深度学习兴起的原因（Why is Deep Learning taking off?）"></a>深度学习兴起的原因（Why is Deep Learning taking off?）</h2><blockquote><p>Scale drives deep learning progress.——吴恩达（<a href="https://www.coursera.org/instructor/andrewng" target="_blank" rel="noopener">Andrew Ng</a>）</p></blockquote><h3 id="深度学习的性能"><a href="#深度学习的性能" class="headerlink" title="深度学习的性能"></a>深度学习的性能</h3><ul><li>x-axis is the amount of data （数据量）</li><li>y-axis (vertical axis) is the performance of the algorithm. （算法性能）</li></ul><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/JP--G3ooEeeJIwrF5BVsIg_b60d752c05bec0881d8ca08cfc2646d2_Screen-Shot-2017-08-05-at-2.30.09-PM.png?expiry=1565481600000&hmac=I-FnXoQoONjGN17_lNuk7vXj7BK4h_mP1txu2K-Y830" alt="数据量与算法性能关系" title>                </div>                <div class="image-caption">数据量与算法性能关系</div>            </figure><ul><li>当数据量比较少时，深度学习方法性能不一定优于经典机器学习方法。</li><li>通过增加数据量和神经网络规模可提升深度学习方法的性能。</li></ul><h3 id="深度学习兴起的原因"><a href="#深度学习兴起的原因" class="headerlink" title="深度学习兴起的原因"></a>深度学习兴起的原因</h3><ul><li>Data：信息化社会产生了巨大的数据量</li><li>Computation：现代计算机计算性能的极大提升（GPU与CPU）</li><li>Algorithms：算法的优化进一步提升了性能（如ReLU的使用）</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;AI is the new Electricity. ——吴恩达（&lt;a href=&quot;https://www.coursera.org/instructor/andrewng&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Andrew Ng&lt;/a&gt;）&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;大约在一百年前，社会的电气化改变了每个主要行业。而如今我们见到了 AI令人惊讶的能量会产生同样巨大的转变。显然 AI的各个分支中，发展最为迅速的就是&lt;strong&gt;深度学习&lt;/strong&gt;（deep learning）。而&lt;strong&gt;深度学习&lt;/strong&gt;，一般指的是训练&lt;strong&gt;神经网络&lt;/strong&gt;（有时是非常非常大/深的神经网络）。&lt;/p&gt;
    
    </summary>
    
    
      <category term="学习笔记" scheme="http://yoursite.com/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
      <category term="deeplearning.ai" scheme="http://yoursite.com/tags/deeplearning-ai/"/>
    
  </entry>
  
</feed>
