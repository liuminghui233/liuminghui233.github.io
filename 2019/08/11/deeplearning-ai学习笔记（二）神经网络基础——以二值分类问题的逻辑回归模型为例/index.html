<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
    

    

    



    <meta charset="utf-8">
    
    
    
    
    <title>deeplearning-ai学习笔记（二）神经网络基础——以二值分类问题的逻辑回归模型为例 | 一只程序喵 | 刘明辉的个人博客</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <meta name="theme-color" content="#3F51B5">
    
    
    <meta name="keywords" content="学习笔记,deeplearning.ai">
    <meta name="description" content="二值分类（Binary Classification ）问题描述例如：猫咪检测器（Cat vs Non-Cat ） 目标是训练一个分类器，对于输入的照片，如果它是一张猫咪的照片就输出1，否则输出0。 输入将一张RGB三通道彩色图像展开为一个长的列向量做为输入。 若图片尺寸为64*64，则向量的维度n(x)=64*64*3=12288。">
<meta name="keywords" content="学习笔记,deeplearning.ai">
<meta property="og:type" content="article">
<meta property="og:title" content="deeplearning-ai学习笔记（二）神经网络基础——以二值分类问题的逻辑回归模型为例">
<meta property="og:url" content="http://yoursite.com/2019/08/11/deeplearning-ai学习笔记（二）神经网络基础——以二值分类问题的逻辑回归模型为例/index.html">
<meta property="og:site_name" content="一只程序喵">
<meta property="og:description" content="二值分类（Binary Classification ）问题描述例如：猫咪检测器（Cat vs Non-Cat ） 目标是训练一个分类器，对于输入的照片，如果它是一张猫咪的照片就输出1，否则输出0。 输入将一张RGB三通道彩色图像展开为一个长的列向量做为输入。 若图片尺寸为64*64，则向量的维度n(x)=64*64*3=12288。">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://s2.ax1x.com/2019/08/11/exSOED.md.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/08/11/exSWEF.md.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/08/11/exSog1.md.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/08/11/exSh4J.md.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/08/11/exSI3R.md.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/08/11/exS5C9.md.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/08/11/exSHu6.md.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/08/11/exSbDK.md.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/08/11/exSqHO.md.png">
<meta property="og:updated_time" content="2019-08-11T15:02:08.738Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="deeplearning-ai学习笔记（二）神经网络基础——以二值分类问题的逻辑回归模型为例">
<meta name="twitter:description" content="二值分类（Binary Classification ）问题描述例如：猫咪检测器（Cat vs Non-Cat ） 目标是训练一个分类器，对于输入的照片，如果它是一张猫咪的照片就输出1，否则输出0。 输入将一张RGB三通道彩色图像展开为一个长的列向量做为输入。 若图片尺寸为64*64，则向量的维度n(x)=64*64*3=12288。">
<meta name="twitter:image" content="https://s2.ax1x.com/2019/08/11/exSOED.md.png">
    
        <link rel="alternate" type="application/atom+xml" title="一只程序喵" href="/atom.xml">
    
    <link rel="shortcut icon" href="/favicon.ico">
    <link rel="stylesheet" href="//unpkg.com/hexo-theme-material-indigo@latest/css/style.css">
    <script>window.lazyScripts=[]</script>

    <!-- custom head -->
    

</head>

<body>
    <div id="loading" class="active"></div>

    <aside id="menu" class="hide" >
  <div class="inner flex-row-vertical">
    <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menu-off">
        <i class="icon icon-lg icon-close"></i>
    </a>
    <div class="brand-wrap" style="background-image:url(/img/brand.jpg)">
      <div class="brand">
        <a href="/" class="avatar waves-effect waves-circle waves-light">
          <img src="/img/avatar.jpg">
        </a>
        <hgroup class="introduce">
          <h5 class="nickname">刘明辉</h5>
          <a href="mailto:549208791@qq.com" title="549208791@qq.com" class="mail">549208791@qq.com</a>
        </hgroup>
      </div>
    </div>
    <div class="scroll-wrap flex-col">
      <ul class="nav">
        
            <li class="waves-block waves-effect">
              <a href="/"  >
                <i class="icon icon-lg icon-home"></i>
                主页
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/archives"  >
                <i class="icon icon-lg icon-archives"></i>
                Archives
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/tags"  >
                <i class="icon icon-lg icon-tags"></i>
                Tags
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/categories"  >
                <i class="icon icon-lg icon-th-list"></i>
                Categories
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="https://github.com/liuminghui233" target="_blank" >
                <i class="icon icon-lg icon-github"></i>
                Github
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/about"  >
                <i class="icon icon-lg icon-link"></i>
                About
              </a>
            </li>
        
      </ul>
    </div>
  </div>
</aside>

    <main id="main">
        <header class="top-header" id="header">
    <div class="flex-row">
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light on" id="menu-toggle">
          <i class="icon icon-lg icon-navicon"></i>
        </a>
        <div class="flex-col header-title ellipsis">deeplearning-ai学习笔记（二）神经网络基础——以二值分类问题的逻辑回归模型为例</div>
        
        <div class="search-wrap" id="search-wrap">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back">
                <i class="icon icon-lg icon-chevron-left"></i>
            </a>
            <input type="text" id="key" class="search-input" autocomplete="off" placeholder="Search">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="search">
                <i class="icon icon-lg icon-search"></i>
            </a>
        </div>
        
        
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menuShare">
            <i class="icon icon-lg icon-share-alt"></i>
        </a>
        
    </div>
</header>
<header class="content-header post-header">

    <div class="container fade-scale">
        <h1 class="title">deeplearning-ai学习笔记（二）神经网络基础——以二值分类问题的逻辑回归模型为例</h1>
        <h5 class="subtitle">
            
                <time datetime="2019-08-11T02:18:57.000Z" itemprop="datePublished" class="page-time">
  2019-08-11
</time>


            
        </h5>
    </div>

    


</header>


<div class="container body-wrap">
    
    <aside class="post-widget">
        <nav class="post-toc-wrap post-toc-shrink" id="post-toc">
            <h4>TOC</h4>
            <ol class="post-toc"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#二值分类（Binary-Classification-）问题"><span class="post-toc-number">1.</span> <span class="post-toc-text">二值分类（Binary Classification ）问题</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#描述"><span class="post-toc-number">1.1.</span> <span class="post-toc-text">描述</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#输入"><span class="post-toc-number">1.2.</span> <span class="post-toc-text">输入</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#代价函数（Cost-function-）"><span class="post-toc-number">1.3.</span> <span class="post-toc-text">代价函数（Cost function ）</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#损失函数-vs-代价函数"><span class="post-toc-number">1.3.1.</span> <span class="post-toc-text">损失函数 vs 代价函数</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#逻辑回归的损失函数"><span class="post-toc-number">1.3.2.</span> <span class="post-toc-text">逻辑回归的损失函数</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#逻辑回归的代价函数"><span class="post-toc-number">1.3.3.</span> <span class="post-toc-text">逻辑回归的代价函数</span></a></li></ol></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#训练过程——梯度下降（Gradient-Descent）算法"><span class="post-toc-number">2.</span> <span class="post-toc-text">训练过程——梯度下降（Gradient Descent）算法</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#算法思想"><span class="post-toc-number">2.1.</span> <span class="post-toc-text">算法思想</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#数学基础：计算图（Computation-Graph）"><span class="post-toc-number">2.2.</span> <span class="post-toc-text">数学基础：计算图（Computation Graph）</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#计算图计算（前向传播过程）"><span class="post-toc-number">2.2.1.</span> <span class="post-toc-text">计算图计算（前向传播过程）</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#计算图求导（反向传播过程）"><span class="post-toc-number">2.2.2.</span> <span class="post-toc-text">计算图求导（反向传播过程）</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#逻辑回归模型的前向传播和反向传播过程（单个样本）"><span class="post-toc-number">2.3.</span> <span class="post-toc-text">逻辑回归模型的前向传播和反向传播过程（单个样本）</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#逻辑回归模型的计算图"><span class="post-toc-number">2.3.1.</span> <span class="post-toc-text">逻辑回归模型的计算图</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#逻辑回归模型的前向传播"><span class="post-toc-number">2.3.2.</span> <span class="post-toc-text">逻辑回归模型的前向传播</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#逻辑回归模型的反向传播"><span class="post-toc-number">2.3.3.</span> <span class="post-toc-text">逻辑回归模型的反向传播</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#非向量化的逻辑回归训练过程"><span class="post-toc-number">2.4.</span> <span class="post-toc-text">非向量化的逻辑回归训练过程</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#向量化（Vectorizing）的逻辑回归训练过程"><span class="post-toc-number">2.5.</span> <span class="post-toc-text">向量化（Vectorizing）的逻辑回归训练过程</span></a></li></ol></li></ol>
        </nav>
    </aside>


<article id="post-deeplearning-ai学习笔记（二）神经网络基础——以二值分类问题的逻辑回归模型为例"
  class="post-article article-type-post fade" itemprop="blogPost">

    <div class="post-card">
        <h1 class="post-card-title">deeplearning-ai学习笔记（二）神经网络基础——以二值分类问题的逻辑回归模型为例</h1>
        <div class="post-meta">
            <time class="post-time" title="2019-08-11 10:18:57" datetime="2019-08-11T02:18:57.000Z"  itemprop="datePublished">2019-08-11</time>

            


            
<span id="busuanzi_container_page_pv" title="文章总阅读量" style='display:none'>
    <i class="icon icon-eye icon-pr"></i><span id="busuanzi_value_page_pv"></span>
</span>


        </div>
        <div class="post-content" id="post-content" itemprop="postContent">
            <h2 id="二值分类（Binary-Classification-）问题"><a href="#二值分类（Binary-Classification-）问题" class="headerlink" title="二值分类（Binary Classification ）问题"></a>二值分类（Binary Classification ）问题</h2><h3 id="描述"><a href="#描述" class="headerlink" title="描述"></a>描述</h3><p>例如：<strong>猫咪检测器</strong>（Cat vs Non-Cat ）</p>
<p>目标是训练一个分类器，对于输入的照片，如果它是一张猫咪的照片就输出1，否则输出0。</p>
<h3 id="输入"><a href="#输入" class="headerlink" title="输入"></a>输入</h3><p>将一张RGB三通道彩色图像展开为一个长的列向量做为输入。</p>
<p>若图片尺寸为64*64，则向量的维度n(x)=64*64*3=12288。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://s2.ax1x.com/2019/08/11/exSOED.md.png" alt="输入特征向量x" title>
                </div>
                <div class="image-caption">输入特征向量x</div>
            </figure>](https://imgc

### 输出

对于二值分类问题，输出结果只有两个——**0或1**。

输出标签的向量形式：（m维列向量）
$$
\begin{bmatrix}
y^{(1)}\\
y^{(2)}  \\
\vdots \\
y^{(i)}  
\end{bmatrix}
$$


### 训练数据

- 单个样本由一对（x,y）表示，其中x是一个n(x)维的特征向量 ，y是取值为0或1的标签。
- 训练集包含m个训练样本（用小写m代表训练集的样本总数），(x(i),y(i)) 表示第i个样本的输入和输出。
- 写成矩阵形式如下：（X.shape=n_x\*m）

$$
\begin{bmatrix}
\vdots & \vdots & \vdots & \vdots \\
x^{(1)} & x^{(2)} & \cdots & x^{(i)} \\
\vdots & \vdots & \vdots & \vdots 
\end{bmatrix}
$$



## **逻辑回归**（Logistic Regression ）模型

逻辑回归是一种用于解决输出是0/1的监督学习问题的学习算法，它使得预测值与训练数据之间的偏差最小。

### 定义

以Cat vs No - cat为例：

把一张图片展开为特征向量x做为输入，算法将估计这张图片中包含猫咪的概率。
$$
Given~x~,~\hat{y}=P(y=1|x),where~0\leq\hat{y}\leq1
$$
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://s2.ax1x.com/2019/08/11/exSWEF.md.png" alt="逻辑回归模型" title>
                </div>
                <div class="image-caption">逻辑回归模型</div>
            </figure>
<p>其中各个参数的意义：</p>
<ul>
<li>x：输入特征向量（一个n_x维列向量）</li>
<li>y：训练集标签，y∈0,1</li>
<li>weights——w：权重（一个n_x维列向量）</li>
<li>threshold——b：偏置量（一个实数）</li>
</ul>
<blockquote>
<p>Sigmoid 函数：</p>
<script type="math/tex; mode=display">
\sigma(z)=\frac{1}{1+e^{-z}}</script><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://s2.ax1x.com/2019/08/11/exSog1.md.png" alt="Sigmoid函数图像" title>
                </div>
                <div class="image-caption">Sigmoid函数图像</div>
            </figure>
<p>该函数的特点：</p>
<ul>
<li>当z趋于+∞，函数值趋于1</li>
<li>当z趋于-∞，函数值趋于0</li>
<li>当z=0，函数值为0.5</li>
</ul>
<p>作用：将输出值限制在[0,1]，使其可以表示一个概率值</p>
</blockquote>
<h3 id="代价函数（Cost-function-）"><a href="#代价函数（Cost-function-）" class="headerlink" title="代价函数（Cost function ）"></a>代价函数（Cost function ）</h3><h4 id="损失函数-vs-代价函数"><a href="#损失函数-vs-代价函数" class="headerlink" title="损失函数 vs 代价函数"></a>损失函数 vs 代价函数</h4><blockquote>
<p>损失函数（Loss function）是定义在<strong>单个样本</strong>上的，算的是一个样本的误差。</p>
<p>代价函数（Cost function）是定义在<strong>整个训练集</strong>上的，是所有样本误差的平均，也就是损失函数的平均。</p>
</blockquote>
<p><strong>代价函数用来衡量参数在整个模型中的作用效果。</strong></p>
<h4 id="逻辑回归的损失函数"><a href="#逻辑回归的损失函数" class="headerlink" title="逻辑回归的损失函数"></a>逻辑回归的损失函数</h4><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://s2.ax1x.com/2019/08/11/exSh4J.md.png" alt="损失函数" title>
                </div>
                <div class="image-caption">损失函数</div>
            </figure>
<p><strong>推导过程：</strong></p>
<p>我们希望算法输出ŷ 表示当给定输入特征x的时候y=1的概率。换句话说，如果y等于1，那么p(y|x)就等于ŷ ；相反地，当y=0时，p(y|x)就等于1-ŷ 。因此，如果ŷ表示当y=1的概率，那么1-ŷ就表示y=0的概率。</p>
<p>可以定义p(y|x)如下：</p>
<p>由于y只有0和1两种取值，因此上面的两个方程可以归纳为如下一个方程：</p>
<script type="math/tex; mode=display">
p(y|x)=ŷ^y*(1-ŷ)^{1-y}</script><p>因为对数函数是一个绝对的单调递增函数，最大化log(p(y|x))会得出和最大化p(y|x)相似的结果，因此可以取对数，简化公式。</p>
<script type="math/tex; mode=display">
log(ŷ^y*(1-ŷ)^{1-y}) =ylog(ŷ)+(1-y)log(1-ŷ)</script><p>又因为通常在训练一个学习算法的时候，我们想要让概率变大。而在逻辑回归中，我们想要最小化L(ŷ,y)这个损失函数。最小化损失函数相当于最大化概率的对数，所以需要加一个负号。</p>
<script type="math/tex; mode=display">
L(ŷ,y)=-(ylog(ŷ)+(1-y)log(1-ŷ))</script><h4 id="逻辑回归的代价函数"><a href="#逻辑回归的代价函数" class="headerlink" title="逻辑回归的代价函数"></a>逻辑回归的代价函数</h4><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://s2.ax1x.com/2019/08/11/exSI3R.md.png" alt="代价函数" title>
                </div>
                <div class="image-caption">代价函数</div>
            </figure>
<p><strong>推导过程：</strong></p>
<p>假设取出的训练样本相互独立，或者说服从独立同分布 (I.I.D: Independent and Identically Distributed) ，这些样本的概率就是各项概率的乘积。</p>
<p>给定X(i)从i=1到m时p(y(i))的乘积：</p>
<script type="math/tex; mode=display">
p(y^{(1)})*p(y^{(2)})*\cdots*p(y^{(m)})</script><p>最大化这个式子本身和最大化它的对数效果相同，所以取对数：</p>
<script type="math/tex; mode=display">
log(p(y^{(1)}))+log(p(y^{(2)}))+\cdots+log(p(y^{(m)}))=\sum_{i=1}^{m}log(p(y^{(i)}))=-\sum_{i=1}^{m}L(ŷ^{(i)},y^{(i)})</script><p>根据最大似然估计原理，选择能使该式最大化的参数。因为要最小化代价函数，而不是最大化似然值，所以要去掉这个负号。最后为了方便起见，使这些数值处于更好的尺度上，在前面添加一个缩放系数1/m。</p>
<p>综上，代价函数为：</p>
<script type="math/tex; mode=display">
J(w,b)=\frac{1}{m}\sum_{i=1}^{m}L(ŷ^{(i)},y^{(i)})=-\frac{1}{m}\sum_{i=1}^{m}[y^{(i)}log(ŷ^{(i)})+(1-y^{(i)})log(1-ŷ^{(i)})]</script><p><strong>优化逻辑回归模型时，我们试着去找参数w和b，以此来缩小代价函数J， 逻辑回归可被视为一个非常小的神经网络 。</strong></p>
<h2 id="训练过程——梯度下降（Gradient-Descent）算法"><a href="#训练过程——梯度下降（Gradient-Descent）算法" class="headerlink" title="训练过程——梯度下降（Gradient Descent）算法"></a>训练过程——<strong>梯度下降</strong>（Gradient Descent）算法</h2><h3 id="算法思想"><a href="#算法思想" class="headerlink" title="算法思想"></a>算法思想</h3><p>在逻辑回归问题中，代价函数J是一个凸函数(convex function) 。为了去找到优的参数值，首先用一些初始值（0或随机）来初始化w和b，因为函数是凸函数，无论在哪里初始化，应该达到同一点或大致相同的点。</p>
<p>梯度下降法以初始点开始，然后朝最陡的下坡方向走一步，这是梯度下降的一次迭代。经过多次迭代收敛到全局最优值或接近全局最优值。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://s2.ax1x.com/2019/08/11/exS5C9.md.png" alt="梯度下降示意图" title>
                </div>
                <div class="image-caption">梯度下降示意图</div>
            </figure>
<p>参数的迭代公式：</p>
<script type="math/tex; mode=display">
w:=w-\alpha\frac{\partial{J(w,b)}}{\partial{w}}</script><script type="math/tex; mode=display">
b:=b-\alpha\frac{\partial{J(w,b)}}{\partial{b}}</script><h3 id="数学基础：计算图（Computation-Graph）"><a href="#数学基础：计算图（Computation-Graph）" class="headerlink" title="数学基础：计算图（Computation Graph）"></a>数学基础：计算图（Computation Graph）</h3><p>以下式为例</p>
<script type="math/tex; mode=display">
J(a,b,c)=3(a+bc)</script><p>其计算图为：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://s2.ax1x.com/2019/08/11/exSHu6.md.png" alt="计算图" title>
                </div>
                <div class="image-caption">计算图</div>
            </figure>
<h4 id="计算图计算（前向传播过程）"><a href="#计算图计算（前向传播过程）" class="headerlink" title="计算图计算（前向传播过程）"></a>计算图计算（前向传播过程）</h4><p>前向传播即为计算一个样本下J的值，计算过程如下：</p>
<script type="math/tex; mode=display">
u=b*c=3*2=6 \\
v=a+u=5+6=11 \\
J=3V=33</script><h4 id="计算图求导（反向传播过程）"><a href="#计算图求导（反向传播过程）" class="headerlink" title="计算图求导（反向传播过程）"></a>计算图求导（反向传播过程）</h4><p>反向传播即根据<strong>求导的链式法则</strong>，求解最终输出变量J对各个变量的导数。</p>
<p>计算过程如下：</p>
<script type="math/tex; mode=display">
\frac{dJ}{dv}=\frac{d(3v)}{dv}=3 \\
\frac{dJ}{da}=\frac{dJ}{dv}*\frac{dv}{da}=3*\frac{d(a+u)}{da}=3 \\
\frac{dJ}{du}=\frac{dJ}{dv}*\frac{dv}{du}=3*\frac{d(a+u)}{du}=3 \\
\frac{dJ}{db}=\frac{dJ}{du}*\frac{du}{db}=3*\frac{d(bc)}{db}=3c=3*2=6 \\
\frac{dJ}{dc}=\frac{dJ}{du}*\frac{du}{dc}=3*\frac{d(bc)}{db}=3b=3*3=9</script><p>为简化表示，将dJ/dval简记为dval，则在本例中：</p>
<script type="math/tex; mode=display">
da=3,db=6,dc=9</script><h3 id="逻辑回归模型的前向传播和反向传播过程（单个样本）"><a href="#逻辑回归模型的前向传播和反向传播过程（单个样本）" class="headerlink" title="逻辑回归模型的前向传播和反向传播过程（单个样本）"></a>逻辑回归模型的前向传播和反向传播过程（单个样本）</h3><h4 id="逻辑回归模型的计算图"><a href="#逻辑回归模型的计算图" class="headerlink" title="逻辑回归模型的计算图"></a>逻辑回归模型的计算图</h4><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://s2.ax1x.com/2019/08/11/exSbDK.md.png" alt="逻辑回归模型的计算图" title>
                </div>
                <div class="image-caption">逻辑回归模型的计算图</div>
            </figure>
<h4 id="逻辑回归模型的前向传播"><a href="#逻辑回归模型的前向传播" class="headerlink" title="逻辑回归模型的前向传播"></a>逻辑回归模型的前向传播</h4><p>顺序计算即可。</p>
<h4 id="逻辑回归模型的反向传播"><a href="#逻辑回归模型的反向传播" class="headerlink" title="逻辑回归模型的反向传播"></a>逻辑回归模型的反向传播</h4><p>计算过程如下：</p>
<script type="math/tex; mode=display">
“da”=\frac{dL}{da}=-\frac{y}{a}+\frac{1-y}{1-a} \\
“dz”=\frac{dL}{dz}=\frac{dL}{da}*\frac{da}{dz}=(-\frac{y}{a}+\frac{1-y}{1-a})*\frac{d\sigma(z)}{dz} = a(1-a) \\
“dw_1”=\frac{dL}{dw_1}=\frac{dL}{dz}*\frac{dz}{dw_1}=x_1*“dz” \\
“dw_2”=\frac{dL}{dw_2}=\frac{dL}{dz}*\frac{dz}{dw_2}=x_2*“dz” \\
“db”=\frac{dL}{db}=\frac{dL}{dz}*\frac{dz}{db}=“dz”</script><h3 id="非向量化的逻辑回归训练过程"><a href="#非向量化的逻辑回归训练过程" class="headerlink" title="非向量化的逻辑回归训练过程"></a>非向量化的逻辑回归训练过程</h3><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://s2.ax1x.com/2019/08/11/exSqHO.md.png" alt="非向量化的逻辑回归训练过程" title>
                </div>
                <div class="image-caption">非向量化的逻辑回归训练过程</div>
            </figure>
<ul>
<li>缺点：显式地使用了两层for循环，时间效率低</li>
<li>解决方法：采用向量化的方法，可以极大地提高时间效率</li>
</ul>
<h3 id="向量化（Vectorizing）的逻辑回归训练过程"><a href="#向量化（Vectorizing）的逻辑回归训练过程" class="headerlink" title="向量化（Vectorizing）的逻辑回归训练过程"></a>向量化（Vectorizing）的逻辑回归训练过程</h3><script type="math/tex; mode=display">
Z = W^{T}*X+b = np.dot(W.T,X)+b //此处存在python的广播机制 \\
A = σ(Z) \\
dZ = A - Y \\
dW = \frac{1}{m}XdZ^{T}\\
db = \frac{1}{m}*np.sum(dZ)\\
W := W - \alpha dW\\
b := b - \alpha db</script><p>注意：此为一次迭代的过程，多次迭代使用显示循环不可避免。</p>

        </div>

        <blockquote class="post-copyright">
    
    <div class="content">
        
<span class="post-time">
    Last updated: <time datetime="2019-08-11T15:02:08.738Z" itemprop="dateUpdated">2019-08-11 23:02:08</time>
</span><br>


        
        版权声明:本文为博主原创文章,未经博主允许不得转载。<a href="/2019/08/11/deeplearning-ai学习笔记（二）神经网络基础——以二值分类问题的逻辑回归模型为例/" target="_blank" rel="external">http://yoursite.com/2019/08/11/deeplearning-ai学习笔记（二）神经网络基础——以二值分类问题的逻辑回归模型为例/</a>
        
    </div>
    
    <footer>
        <a href="http://yoursite.com">
            <img src="/img/avatar.jpg" alt="刘明辉">
            刘明辉
        </a>
    </footer>
</blockquote>

        
<div class="page-reward">
    <a id="rewardBtn" href="javascript:;" class="page-reward-btn waves-effect waves-circle waves-light">赏</a>
</div>



        <div class="post-footer">
            
	<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/deeplearning-ai/">deeplearning.ai</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/学习笔记/">学习笔记</a></li></ul>


            
<div class="page-share-wrap">
    

<div class="page-share" id="pageShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://yoursite.com/2019/08/11/deeplearning-ai学习笔记（二）神经网络基础——以二值分类问题的逻辑回归模型为例/&title=《deeplearning-ai学习笔记（二）神经网络基础——以二值分类问题的逻辑回归模型为例》 — 一只程序喵&pic=http://yoursite.com/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://yoursite.com/2019/08/11/deeplearning-ai学习笔记（二）神经网络基础——以二值分类问题的逻辑回归模型为例/&title=《deeplearning-ai学习笔记（二）神经网络基础——以二值分类问题的逻辑回归模型为例》 — 一只程序喵&source=" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://yoursite.com/2019/08/11/deeplearning-ai学习笔记（二）神经网络基础——以二值分类问题的逻辑回归模型为例/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《deeplearning-ai学习笔记（二）神经网络基础——以二值分类问题的逻辑回归模型为例》 — 一只程序喵&url=http://yoursite.com/2019/08/11/deeplearning-ai学习笔记（二）神经网络基础——以二值分类问题的逻辑回归模型为例/&via=http://yoursite.com" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://yoursite.com/2019/08/11/deeplearning-ai学习笔记（二）神经网络基础——以二值分类问题的逻辑回归模型为例/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>



    <a href="javascript:;" id="shareFab" class="page-share-fab waves-effect waves-circle">
        <i class="icon icon-share-alt icon-lg"></i>
    </a>
</div>



        </div>
    </div>

    
<nav class="post-nav flex-row flex-justify-between flex-row-reverse">
  

  
    <div class="waves-block waves-effect next">
      <a href="/2019/08/09/那些既熟悉又陌生的C-C-知识点（长期更新）/" id="post-next" class="post-nav-link">
        <div class="tips">Next <i class="icon icon-angle-right icon-lg icon-pl"></i></div>
        <h4 class="title">那些既熟悉又陌生的C/C++知识点（长期更新）</h4>
      </a>
    </div>
  
</nav>



    




















</article>

<div id="reward" class="page-modal reward-lay">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <h3 class="reward-title">
        <i class="icon icon-quote-left"></i>
        谢谢打赏ο(=•ω＜=)ρ⌒☆
        <i class="icon icon-quote-right"></i>
    </h3>
    <div class="reward-content">
        
        <div class="reward-code">
            <img id="rewardCode" src="/img/wechat.jpg" alt="打赏二维码">
        </div>
        
        <label class="reward-toggle">
            <input id="rewardToggle" type="checkbox" class="reward-toggle-check"
                data-wechat="/img/wechat.jpg" data-alipay="/img/alipay.jpg">
            <div class="reward-toggle-ctrol">
                <span class="reward-toggle-item wechat">微信</span>
                <span class="reward-toggle-label"></span>
                <span class="reward-toggle-item alipay">支付宝</span>
            </div>
        </label>
        
    </div>
</div>



</div>

        <footer class="footer">
    <div class="top">
        
<p>
    <span id="busuanzi_container_site_uv" style='display:none'>
        站点总访客数：<span id="busuanzi_value_site_uv"></span>
    </span>
    <span id="busuanzi_container_site_pv" style='display:none'>
        站点总访问量：<span id="busuanzi_value_site_pv"></span>
    </span>
</p>


        <p>
            
                <span><a href="/atom.xml" target="_blank" class="rss" title="rss"><i class="icon icon-lg icon-rss"></i></a></span>
            
            <span>This blog is licensed under a <a rel="license" href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>.</span>
        </p>
    </div>
    <div class="bottom">
        <p><span>刘明辉 &copy; 2015 - 2019</span>
            <span>
                
                Power by <a href="http://hexo.io/" target="_blank">Hexo</a> Theme <a href="https://github.com/yscoder/hexo-theme-indigo" target="_blank">indigo</a>
            </span>
        </p>
    </div>
</footer>

    </main>
    <div class="mask" id="mask"></div>
<a href="javascript:;" id="gotop" class="waves-effect waves-circle waves-light"><span class="icon icon-lg icon-chevron-up"></span></a>



<div class="global-share" id="globalShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://yoursite.com/2019/08/11/deeplearning-ai学习笔记（二）神经网络基础——以二值分类问题的逻辑回归模型为例/&title=《deeplearning-ai学习笔记（二）神经网络基础——以二值分类问题的逻辑回归模型为例》 — 一只程序喵&pic=http://yoursite.com/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://yoursite.com/2019/08/11/deeplearning-ai学习笔记（二）神经网络基础——以二值分类问题的逻辑回归模型为例/&title=《deeplearning-ai学习笔记（二）神经网络基础——以二值分类问题的逻辑回归模型为例》 — 一只程序喵&source=" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://yoursite.com/2019/08/11/deeplearning-ai学习笔记（二）神经网络基础——以二值分类问题的逻辑回归模型为例/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《deeplearning-ai学习笔记（二）神经网络基础——以二值分类问题的逻辑回归模型为例》 — 一只程序喵&url=http://yoursite.com/2019/08/11/deeplearning-ai学习笔记（二）神经网络基础——以二值分类问题的逻辑回归模型为例/&via=http://yoursite.com" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://yoursite.com/2019/08/11/deeplearning-ai学习笔记（二）神经网络基础——以二值分类问题的逻辑回归模型为例/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>


<div class="page-modal wx-share" id="wxShare">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <p>扫一扫，分享到微信</p>
    <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAASYAAAEmCAAAAADqr2IGAAAEg0lEQVR42u3aS24bMRAFwNz/0s7aQCS9180YolyzCqwRh1MK0B/2nz/x9fXgyr/1fJ3knudrJnt+fv+BCxMmTJgwvSVTu/XNyvlGc7hH39qs/I91MGHChAnT5UzJcs+/laC0KUWynxY0TyYwYcKECdNvZmrDcEK82UMOigkTJkyYMOUBuC04zyYEefhvi3BMmDBhwvR5TPly+VbawLw5vNy0mw/3wjFhwoQJ05sxnW2w3vXvwxcmTJgwYXozpq/yapOATWBuj1EfJRCzN/22DiZMmDBhupZp06J99OnmEHGWlOyhn78vJkyYMGG6lykPn/nwTf6t9ul5+zX/2ZKGLyZMmDBh+gymJOS3JegmwLf3zAJ/dD8mTJgwYbqc6VSAP7ChxVHrrEldlL6YMGHChOlCpk3QbcdSZ8eN+TptKzkpjw/kJpgwYcKE6c2Y8hDbJg15SG4R29I6T0denPpiwoQJE6a3Z2oD6qYFnCccyVNmP087fhSdwWLChAkTprdnioZX4u3mYTgpTdsiNt9D0bzGhAkTJkzXMrXt2nzQM6dpf5LNT7VKGjBhwoQJ07VM7Wu3R5ttqXm2PN4PAGHChAkTpnuZ6uGVxYZyvjyBaPlm5TEmTJgwYbqXaTMosxm7acdx8lK8HT+qi15MmDBhwnQV0ybkzxDzEjS/Z9Z6LlITTJgwYcJ0LVMbtpPxnbbgTEroWeu5fa9orAcTJkyYMF3FFPkFm57dOUsaZmtunoUJEyZMmG5n2pSR+RHj5v5Nszh/VpQrYcKECROmq5jysZWk9M2HdWbPbVvSmzVfnPpiwoQJE6YLmWZB+ucPR3P6fIXojTBhwoQJ04VMp9qsycNmgzI50CaZeHEPJkyYMGG6lim5tR3lmRWrm2DfrlCPpWLChAkTpsuZ2i+05eWpT88ef+b3Y8KECROm25na4vNU2dweOrbd6zyJOYCFCRMmTJjekul/DJWeGmk99fO0h7XRZBMmTJgwYbqEqW2M5oXupiTOw/xsJ3nZjAkTJkyYPolpVrK2wzSbcZy2sbv/mb/9BRMmTJgwXcjUhsbhNFAcVdsjzLaTPTwixYQJEyZM1zK1Tdh2iGeWHJz6dHPPw/9TmDBhwoTpKqZ2UCYpd2fDN2fD/CY5wIQJEyZMn830vAxui9g2scj3kNDk979YBxMmTJgwfRzT7C/toWZ+DNk+dzMYtOo9Y8KECROmS5hmr7Q5vNxgnU0UjpW+mDBhwoTpzZhmhWvSkJ2lCJs727Tm+d8xYcKECdO9TDOgBC7fXH61zz37REyYMGHCdCPTV3ltkom2xJ2tvBnoeXGciQkTJkyYLmRqQ+PPDPS0reF2n20xjwkTJkyYbmeajb/k4TwPxnlbOdnPrDgvGruYMGHChOkqpjZwJgSzwaBZqZxzrHIiTJgwYcL0C5jOjsJsmsttQb4aFcKECRMmTL+Gqd1EflTZlribI888fcGECRMmTJ/B1H4tCdWzIL0pZb/W14uEABMmTJgwXcjUloKbUrlNCGajNnlT+FgLGBMmTJgwvSPTX7S53N5mRp2pAAAAAElFTkSuQmCC" alt="微信分享二维码">
</div>




    <script src="//cdn.bootcss.com/node-waves/0.7.4/waves.min.js"></script>
<script>
var BLOG = { ROOT: '/', SHARE: true, REWARD: true };


</script>

<script src="//unpkg.com/hexo-theme-material-indigo@latest/js/main.min.js"></script>


<div class="search-panel" id="search-panel">
    <ul class="search-result" id="search-result"></ul>
</div>
<template id="search-tpl">
<li class="item">
    <a href="{path}" class="waves-block waves-effect">
        <div class="title ellipsis" title="{title}">{title}</div>
        <div class="flex-row flex-middle">
            <div class="tags ellipsis">
                {tags}
            </div>
            <time class="flex-col time">{date}</time>
        </div>
    </a>
</li>
</template>

<script src="//unpkg.com/hexo-theme-material-indigo@latest/js/search.min.js" async></script>






<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>



<script>
(function() {
    var OriginTitile = document.title, titleTime;
    document.addEventListener('visibilitychange', function() {
        if (document.hidden) {
            document.title = '一只程序喵 | 刘明辉的个人博客';
            clearTimeout(titleTime);
        } else {
            document.title = '一只程序喵 | 刘明辉的个人博客';
            titleTime = setTimeout(function() {
                document.title = OriginTitile;
            },2000);
        }
    });
})();
</script>



<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

</body>
</html>
